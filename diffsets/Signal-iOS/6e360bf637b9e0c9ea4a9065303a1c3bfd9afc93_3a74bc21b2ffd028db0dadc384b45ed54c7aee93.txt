diff --git a/Signal.xcodeproj/project.pbxproj b/Signal.xcodeproj/project.pbxproj
index 4d489b83250..cdc8f677379 100644
--- a/Signal.xcodeproj/project.pbxproj
+++ b/Signal.xcodeproj/project.pbxproj
@@ -786,6 +786,7 @@
 		88928A71264099EC009C9B30 /* ConversationViewController+VoiceMessage.swift in Sources */ = {isa = PBXBuildFile; fileRef = 88928A70264099EC009C9B30 /* ConversationViewController+VoiceMessage.swift */; };
 		88928A732640DC9B009C9B30 /* VoiceMessageModel.swift in Sources */ = {isa = PBXBuildFile; fileRef = 88928A722640DC9B009C9B30 /* VoiceMessageModel.swift */; };
 		88928A7526418904009C9B30 /* VoiceMessageTooltip.swift in Sources */ = {isa = PBXBuildFile; fileRef = 88928A7426418904009C9B30 /* VoiceMessageTooltip.swift */; };
+		88928A7726419D6B009C9B30 /* VoiceMessageDraftView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 88928A7626419D6B009C9B30 /* VoiceMessageDraftView.swift */; };
 		889C8A8D248831AC00C3D3D8 /* BlurTooltip.swift in Sources */ = {isa = PBXBuildFile; fileRef = 889C8A8C248831AC00C3D3D8 /* BlurTooltip.swift */; };
 		88A357B923639384009D6B9A /* MemberActionSheet.swift in Sources */ = {isa = PBXBuildFile; fileRef = 88A357B823639384009D6B9A /* MemberActionSheet.swift */; };
 		88A4CC09246CB0960082211F /* OnboardingTransferChoiceViewController.swift in Sources */ = {isa = PBXBuildFile; fileRef = 88A4CC08246CB0960082211F /* OnboardingTransferChoiceViewController.swift */; };
@@ -1930,6 +1931,7 @@
 		88928A70264099EC009C9B30 /* ConversationViewController+VoiceMessage.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = "ConversationViewController+VoiceMessage.swift"; sourceTree = "<group>"; };
 		88928A722640DC9B009C9B30 /* VoiceMessageModel.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = VoiceMessageModel.swift; sourceTree = "<group>"; };
 		88928A7426418904009C9B30 /* VoiceMessageTooltip.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = VoiceMessageTooltip.swift; sourceTree = "<group>"; };
+		88928A7626419D6B009C9B30 /* VoiceMessageDraftView.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = VoiceMessageDraftView.swift; sourceTree = "<group>"; };
 		889C8A8C248831AC00C3D3D8 /* BlurTooltip.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = BlurTooltip.swift; sourceTree = "<group>"; };
 		88A357B823639384009D6B9A /* MemberActionSheet.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = MemberActionSheet.swift; sourceTree = "<group>"; };
 		88A358252363FF63009D6B9A /* mr */ = {isa = PBXFileReference; lastKnownFileType = text.plist.strings; name = mr; path = translations/mr.lproj/Localizable.strings; sourceTree = "<group>"; };
@@ -3445,6 +3447,7 @@
 				32AC5CE6255B51E900829BD8 /* JoinGroupCallPill.swift */,
 				4C043929220A9EC800BAEA63 /* VoiceNoteLock.swift */,
 				88928A7426418904009C9B30 /* VoiceMessageTooltip.swift */,
+				88928A7626419D6B009C9B30 /* VoiceMessageDraftView.swift */,
 			);
 			name = Views;
 			path = views;
@@ -5222,6 +5225,7 @@
 				34F308A21ECB469700BB7697 /* OWSBezierPathView.m in Sources */,
 				88A9729422FB4D02004B4FBF /* LocationPicker.swift in Sources */,
 				347342F72548587900D440CD /* ConversationViewController+BottomBar.swift in Sources */,
+				88928A7726419D6B009C9B30 /* VoiceMessageDraftView.swift in Sources */,
 				45B27B862037FFB400A539DF /* DebugUIFileBrowser.swift in Sources */,
 				3498AC892513896400B1F315 /* Dependencies+MainApp.swift in Sources */,
 				347C3834252CE69400F3D941 /* CVItemModel.swift in Sources */,
diff --git a/Signal/src/ViewControllers/ConversationView/Cells/AudioWaveformProgressView.swift b/Signal/src/ViewControllers/ConversationView/Cells/AudioWaveformProgressView.swift
index 1e778da4fc8..7d010ae1a3a 100644
--- a/Signal/src/ViewControllers/ConversationView/Cells/AudioWaveformProgressView.swift
+++ b/Signal/src/ViewControllers/ConversationView/Cells/AudioWaveformProgressView.swift
@@ -157,8 +157,8 @@ class AudioWaveformProgressView: UIView {
             sampleSpacing = 0
         }
 
-        playedShapeLayer.frame = layer.frame
-        unplayedShapeLayer.frame = layer.frame
+        playedShapeLayer.frame = bounds
+        unplayedShapeLayer.frame = bounds
 
         let progress = self.value
         var thumbXPos = width * progress
@@ -182,10 +182,10 @@ class AudioWaveformProgressView: UIView {
             // from 0 (silence) to 1 (loudest possible value). Calculate the
             // height of the sample view so that the loudest value is the
             // full height of this view.
-            let height = max(minSampleHeight, frame.size.height * CGFloat(sample))
+            let sampleHeight = max(minSampleHeight, height * CGFloat(sample))
 
             // Center the sample vertically.
-            let yPos = frame.center.y - height / 2
+            let yPos = bounds.center.y - sampleHeight / 2
 
             var xPos = CGFloat(x) * (sampleWidth + sampleSpacing)
             if CurrentAppContext().isRTL { xPos = width - xPos }
@@ -194,7 +194,7 @@ class AudioWaveformProgressView: UIView {
                 x: xPos,
                 y: yPos,
                 width: sampleWidth,
-                height: height
+                height: sampleHeight
             )
 
             path.append(UIBezierPath(roundedRect: sampleFrame, cornerRadius: sampleWidth / 2))
diff --git a/Signal/src/ViewControllers/ConversationView/ConversationInputToolbar.h b/Signal/src/ViewControllers/ConversationView/ConversationInputToolbar.h
index 83085ad5746..104461f924d 100644
--- a/Signal/src/ViewControllers/ConversationView/ConversationInputToolbar.h
+++ b/Signal/src/ViewControllers/ConversationView/ConversationInputToolbar.h
@@ -14,6 +14,7 @@ NS_ASSUME_NONNULL_BEGIN
 @class PhotoCapture;
 @class SignalAttachment;
 @class StickerInfo;
+@class VoiceMessageModel;
 
 @protocol MentionTextViewDelegate;
 
@@ -39,6 +40,8 @@ NS_ASSUME_NONNULL_BEGIN
 
 - (void)voiceMemoGestureDidUpdateCancelWithRatioComplete:(CGFloat)cancelAlpha;
 
+- (void)sendVoiceMemoDraft:(VoiceMessageModel *)voiceMemoDraft;
+
 #pragma mark - Attachments
 
 - (void)cameraButtonPressed;
@@ -111,6 +114,8 @@ NS_ASSUME_NONNULL_BEGIN
 
 - (void)showVoiceMemoUI;
 
+- (void)showVoiceMemoDraft:(VoiceMessageModel *)voiceMemoDraft;
+
 - (void)hideVoiceMemoUI:(BOOL)animated;
 
 - (void)setVoiceMemoUICancelAlpha:(CGFloat)cancelAlpha;
@@ -121,6 +126,8 @@ NS_ASSUME_NONNULL_BEGIN
 
 - (void)removeVoiceMemoTooltip;
 
+@property (nonatomic, nullable, readonly) VoiceMessageModel *voiceMemoDraft;
+
 #pragma mark -
 
 @property (nonatomic, nullable) OWSQuotedReplyModel *quotedReply;
diff --git a/Signal/src/ViewControllers/ConversationView/ConversationInputToolbar.m b/Signal/src/ViewControllers/ConversationView/ConversationInputToolbar.m
index cee8a20c064..104660c9025 100644
--- a/Signal/src/ViewControllers/ConversationView/ConversationInputToolbar.m
+++ b/Signal/src/ViewControllers/ConversationView/ConversationInputToolbar.m
@@ -22,7 +22,8 @@
 typedef NS_CLOSED_ENUM(NSUInteger, VoiceMemoRecordingState) {
     VoiceMemoRecordingState_Idle,
     VoiceMemoRecordingState_RecordingHeld,
-    VoiceMemoRecordingState_RecordingLocked
+    VoiceMemoRecordingState_RecordingLocked,
+    VoiceMemoRecordingState_Draft
 };
 
 typedef NS_CLOSED_ENUM(NSUInteger, KeyboardType) { KeyboardType_System, KeyboardType_Sticker, KeyboardType_Attachment };
@@ -92,6 +93,7 @@ @interface ConversationInputToolbar () <ConversationTextViewToolbarDelegate,
 @property (nonatomic) VoiceMemoRecordingState voiceMemoRecordingState;
 @property (nonatomic) CGPoint voiceMemoGestureStartLocation;
 @property (nonatomic, nullable, weak) UIView *voiceMemoTooltip;
+@property (nonatomic, nullable) VoiceMessageModel *voiceMemoDraft;
 @property (nonatomic, nullable) NSArray<NSLayoutConstraint *> *layoutContraints;
 @property (nonatomic) UIEdgeInsets receivedSafeAreaInsets;
 @property (nonatomic, nullable) InputLinkPreview *inputLinkPreview;
@@ -756,6 +758,7 @@ - (void)handleLongPress:(UIGestureRecognizer *)sender
                     [self.inputToolbarDelegate voiceMemoGestureDidCancel];
                     break;
                 case VoiceMemoRecordingState_RecordingLocked:
+                case VoiceMemoRecordingState_Draft:
                     OWSFailDebug(@"once locked, shouldn't be possible to interact with gesture.");
                     [self.inputToolbarDelegate voiceMemoGestureDidCancel];
                     break;
@@ -791,6 +794,7 @@ - (void)handleLongPress:(UIGestureRecognizer *)sender
                             [self.inputToolbarDelegate voiceMemoGestureDidUpdateCancelWithRatioComplete:0];
                             break;
                         case VoiceMemoRecordingState_RecordingLocked:
+                        case VoiceMemoRecordingState_Draft:
                             // already locked
                             break;
                         case VoiceMemoRecordingState_Idle:
@@ -826,6 +830,7 @@ - (void)handleLongPress:(UIGestureRecognizer *)sender
                     [self.inputToolbarDelegate voiceMemoGestureDidComplete];
                     break;
                 case VoiceMemoRecordingState_RecordingLocked:
+                case VoiceMemoRecordingState_Draft:
                     // Continue recording.
                     break;
             }
@@ -850,6 +855,7 @@ - (BOOL)isRecordingVoiceMemo
             return NO;
         case VoiceMemoRecordingState_RecordingHeld:
         case VoiceMemoRecordingState_RecordingLocked:
+        case VoiceMemoRecordingState_Draft:
             return YES;
     }
 }
@@ -970,11 +976,38 @@ - (void)showVoiceMemoUI
                                                                     repeats:YES];
 }
 
+- (void)showVoiceMemoDraft:(VoiceMessageModel *)voiceMemoDraft
+{
+    OWSAssertIsOnMainThread();
+
+    self.voiceMemoDraft = voiceMemoDraft;
+    self.voiceMemoRecordingState = VoiceMemoRecordingState_Draft;
+
+    [self removeVoiceMemoTooltip];
+
+    [self.voiceMemoRedRecordingCircle removeFromSuperview];
+    [self.voiceMemoLockView removeFromSuperview];
+
+    [self.voiceMemoContentView removeAllSubviews];
+
+    [self.voiceMemoUpdateTimer invalidate];
+    self.voiceMemoUpdateTimer = nil;
+
+    __weak __typeof(self) weakSelf = self;
+    UIView *draftView = [[VoiceMessageDraftView alloc] initWithVoiceMessageModel:voiceMemoDraft
+                                                               didDeleteCallback:^{ [weakSelf hideVoiceMemoUI:YES]; }];
+    [self.voiceMemoContentView addSubview:draftView];
+    [draftView autoPinEdgesToSuperviewEdges];
+}
+
 - (void)hideVoiceMemoUI:(BOOL)animated
 {
     OWSAssertIsOnMainThread();
 
+    [self.voiceMemoContentView removeAllSubviews];
+
     self.voiceMemoRecordingState = VoiceMemoRecordingState_Idle;
+    self.voiceMemoDraft = nil;
 
     UIView *oldVoiceMemoRedRecordingCircle = self.voiceMemoRedRecordingCircle;
     UIView *oldVoiceMemoLockView = self.voiceMemoLockView;
@@ -987,6 +1020,8 @@ - (void)hideVoiceMemoUI:(BOOL)animated
     [self.voiceMemoUpdateTimer invalidate];
     self.voiceMemoUpdateTimer = nil;
 
+    self.voiceMemoDraft = nil;
+
     if (animated) {
         [UIView animateWithDuration:0.2f
             animations:^{
@@ -1109,7 +1144,11 @@ - (void)sendButtonPressed
 {
     OWSAssertDebug(self.inputToolbarDelegate);
 
-    if (self.isRecordingVoiceMemo) {
+    if (self.voiceMemoDraft) {
+        self.voiceMemoRecordingState = VoiceMemoRecordingState_Idle;
+        [self.inputToolbarDelegate sendVoiceMemoDraft:self.voiceMemoDraft];
+    } else if (self.isRecordingVoiceMemo) {
+        self.voiceMemoRecordingState = VoiceMemoRecordingState_Idle;
         [self.inputToolbarDelegate voiceMemoGestureDidComplete];
     } else {
         [self.inputToolbarDelegate sendButtonPressed];
diff --git a/Signal/src/ViewControllers/ConversationView/ConversationViewController+BottomBar.swift b/Signal/src/ViewControllers/ConversationView/ConversationViewController+BottomBar.swift
index 1b09c541bc1..c6285e239c2 100644
--- a/Signal/src/ViewControllers/ConversationView/ConversationViewController+BottomBar.swift
+++ b/Signal/src/ViewControllers/ConversationView/ConversationViewController+BottomBar.swift
@@ -149,18 +149,24 @@ public extension ConversationViewController {
             return
         }
 
-        let messageDraft: MessageBody?
+        var messageDraft: MessageBody?
+        var voiceMemoDraft: VoiceMessageModel?
         if let oldInputToolbar = self.inputToolbar {
             // Maintain draft continuity.
             messageDraft = oldInputToolbar.messageBody()
+            voiceMemoDraft = oldInputToolbar.voiceMemoDraft
         } else {
-            messageDraft = Self.databaseStorage.uiRead { transaction in
-                self.thread.currentDraft(with: transaction)
+            Self.databaseStorage.uiRead { transaction in
+                messageDraft = self.thread.currentDraft(with: transaction)
+                if VoiceMessageModel.hasDraft(for: self.thread, transaction: transaction) {
+                    voiceMemoDraft = VoiceMessageModel(thread: self.thread)
+                }
             }
         }
 
         let newInputToolbar = buildInputToolbar(conversationStyle: conversationStyle,
-                                                messageDraft: messageDraft)
+                                                messageDraft: messageDraft,
+                                                voiceMemoDraft: voiceMemoDraft)
 
         self.inputToolbar = newInputToolbar
 
diff --git a/Signal/src/ViewControllers/ConversationView/ConversationViewController+GestureRecognizers.swift b/Signal/src/ViewControllers/ConversationView/ConversationViewController+GestureRecognizers.swift
index 85862069412..3e9b1ed4df1 100644
--- a/Signal/src/ViewControllers/ConversationView/ConversationViewController+GestureRecognizers.swift
+++ b/Signal/src/ViewControllers/ConversationView/ConversationViewController+GestureRecognizers.swift
@@ -94,6 +94,10 @@ extension ConversationViewController: UIGestureRecognizerDelegate {
         guard sender.state == .recognized else {
             return
         }
+
+        // Stop any recording voice memos.
+        finishRecordingVoiceMessage(sendImmediately: false)
+
         guard let cell = findCell(forGesture: sender) else {
             return
         }
diff --git a/Signal/src/ViewControllers/ConversationView/ConversationViewController+VoiceMessage.swift b/Signal/src/ViewControllers/ConversationView/ConversationViewController+VoiceMessage.swift
index 5d71ededb83..dd911542428 100644
--- a/Signal/src/ViewControllers/ConversationView/ConversationViewController+VoiceMessage.swift
+++ b/Signal/src/ViewControllers/ConversationView/ConversationViewController+VoiceMessage.swift
@@ -68,12 +68,14 @@ extension ConversationViewController {
 
     @objc(finishRecordingVoiceMessageAndSendImmediately:)
     func finishRecordingVoiceMessage(sendImmediately: Bool = false) {
+        AssertIsOnMainThread()
+
         defer { viewState.currentVoiceMessageModel = nil }
         guard let voiceMessageModel = viewState.currentVoiceMessageModel else { return }
 
-        let duration = voiceMessageModel.stopRecording()
+        voiceMessageModel.stopRecording()
 
-        guard duration >= Self.minimumVoiceMessageDuration else {
+        guard let duration = voiceMessageModel.duration, duration >= Self.minimumVoiceMessageDuration else {
             inputToolbar?.showVoiceMemoTooltip()
             cancelRecordingVoiceMessage()
             return
@@ -82,14 +84,18 @@ extension ConversationViewController {
         ImpactHapticFeedback.impactOccured(style: .medium)
 
         if sendImmediately {
-            inputToolbar?.hideVoiceMemoUI(true)
             sendVoiceMessageModel(voiceMessageModel)
         } else {
-            databaseStorage.asyncWrite { voiceMessageModel.saveDraft(transaction: $0) }
+            databaseStorage.asyncWrite { voiceMessageModel.saveDraft(transaction: $0) } completion: {
+                self.inputToolbar?.showVoiceMemoDraft(voiceMessageModel)
+            }
         }
     }
 
+    @objc
     func sendVoiceMessageModel(_ voiceMessageModel: VoiceMessageModel) {
+        inputToolbar?.hideVoiceMemoUI(true)
+
         var attachment: SignalAttachment?
         databaseStorage.asyncWrite { transaction in
             do {
diff --git a/Signal/src/ViewControllers/ConversationView/ConversationViewController.h b/Signal/src/ViewControllers/ConversationView/ConversationViewController.h
index 1a7c896f49d..3857f02065b 100644
--- a/Signal/src/ViewControllers/ConversationView/ConversationViewController.h
+++ b/Signal/src/ViewControllers/ConversationView/ConversationViewController.h
@@ -53,7 +53,8 @@ typedef NS_ENUM(NSUInteger, ConversationViewAction) {
 
 - (ConversationInputToolbar *)buildInputToolbar:(ConversationStyle *)conversationStyle
                                    messageDraft:(nullable MessageBody *)messageDraft
-    NS_SWIFT_NAME(buildInputToolbar(conversationStyle:messageDraft:));
+                                 voiceMemoDraft:(nullable VoiceMessageModel *)voiceMemoDraft
+    NS_SWIFT_NAME(buildInputToolbar(conversationStyle:messageDraft:voiceMemoDraft:));
 
 #pragma mark 3D Touch/UIContextMenu Methods
 
diff --git a/Signal/src/ViewControllers/ConversationView/ConversationViewController.m b/Signal/src/ViewControllers/ConversationView/ConversationViewController.m
index 9f329f7ee75..6c778884aee 100644
--- a/Signal/src/ViewControllers/ConversationView/ConversationViewController.m
+++ b/Signal/src/ViewControllers/ConversationView/ConversationViewController.m
@@ -3397,6 +3397,13 @@ - (void)voiceMemoGestureDidUpdateCancelWithRatioComplete:(CGFloat)cancelAlpha
     [self.inputToolbar setVoiceMemoUICancelAlpha:cancelAlpha];
 }
 
+- (void)sendVoiceMemoDraft:(VoiceMessageModel *)voiceMemoDraft
+{
+    OWSAssertIsOnMainThread();
+
+    [self sendVoiceMessageModel:voiceMemoDraft];
+}
+
 #pragma mark - Database Observation
 
 - (BOOL)isViewVisible
@@ -3865,6 +3872,7 @@ - (void)openGifSearch
 
 - (ConversationInputToolbar *)buildInputToolbar:(ConversationStyle *)conversationStyle
                                    messageDraft:(nullable MessageBody *)messageDraft
+                                 voiceMemoDraft:(nullable VoiceMessageModel *)voiceMemoDraft
 {
     OWSAssertIsOnMainThread();
     OWSAssertDebug(self.hasViewWillAppearEverBegun);
@@ -3876,6 +3884,11 @@ - (ConversationInputToolbar *)buildInputToolbar:(ConversationStyle *)conversatio
                                               inputTextViewDelegate:self
                                                     mentionDelegate:self];
     SET_SUBVIEW_ACCESSIBILITY_IDENTIFIER(self, inputToolbar);
+
+    if (voiceMemoDraft) {
+        [inputToolbar showVoiceMemoDraft:voiceMemoDraft];
+    }
+
     return inputToolbar;
 }
 
diff --git a/Signal/src/views/LottieToggleButton.swift b/Signal/src/views/LottieToggleButton.swift
index 3d1e84bfcc3..acd7e7f1b31 100644
--- a/Signal/src/views/LottieToggleButton.swift
+++ b/Signal/src/views/LottieToggleButton.swift
@@ -1,5 +1,5 @@
 //
-//  Copyright (c) 2020 Open Whisper Systems. All rights reserved.
+//  Copyright (c) 2021 Open Whisper Systems. All rights reserved.
 //
 
 import Foundation
@@ -21,12 +21,26 @@ class LottieToggleButton: UIButton {
         }
     }
 
+    @objc
+    var animationSpeed: CGFloat {
+        get {
+            animationView?.animationSpeed ?? 0
+        }
+        set {
+            animationView?.animationSpeed = newValue
+        }
+    }
+
     override var isSelected: Bool {
         didSet {
             animationView?.currentProgress = isSelected ? 1 : 0
         }
     }
 
+    func setValueProvider(_ valueProvider: AnyValueProvider, keypath: AnimationKeypath) {
+        animationView?.setValueProvider(valueProvider, keypath: keypath)
+    }
+
     @objc
     func setSelected(_ isSelected: Bool, animated: Bool) {
         AssertIsOnMainThread()
diff --git a/Signal/src/views/VoiceMessageDraftView.swift b/Signal/src/views/VoiceMessageDraftView.swift
new file mode 100644
index 00000000000..eee674db6dc
--- /dev/null
+++ b/Signal/src/views/VoiceMessageDraftView.swift
@@ -0,0 +1,137 @@
+//
+//  Copyright (c) 2021 Open Whisper Systems. All rights reserved.
+//
+
+import Foundation
+import Lottie
+
+@objc
+class VoiceMessageDraftView: UIStackView, OWSAudioPlayerDelegate {
+    private let playbackTimeLabel = UILabel()
+    private let waveformView = AudioWaveformProgressView()
+    private let voiceMessageModel: VoiceMessageModel
+    private let playPauseButton = LottieToggleButton()
+
+    var audioPlaybackState: AudioPlaybackState = .stopped
+
+    @objc
+    init(voiceMessageModel: VoiceMessageModel, didDeleteCallback: @escaping () -> Void) {
+        self.voiceMessageModel = voiceMessageModel
+        super.init(frame: .zero)
+
+        axis = .horizontal
+        spacing = 8
+        alignment = .center
+        isLayoutMarginsRelativeArrangement = true
+        layoutMargins = UIEdgeInsets(hMargin: 16, vMargin: 0)
+
+        let trashButton = OWSButton {
+            voiceMessageModel.audioPlayer.stop()
+            Self.databaseStorage.asyncWrite {
+                voiceMessageModel.clearDraft(transaction: $0)
+            } completion: {
+                didDeleteCallback()
+            }
+        }
+        trashButton.setTemplateImageName("trash-solid-24", tintColor: .ows_accentRed)
+        trashButton.autoSetDimensions(to: CGSize(square: 24))
+        addArrangedSubview(trashButton)
+
+        playPauseButton.animationName = "playPauseButton"
+        playPauseButton.animationSize = CGSize(square: 24)
+        playPauseButton.animationSpeed = 3
+        playPauseButton.addTarget(self, action: #selector(didTogglePlayPause), for: .touchUpInside)
+
+        let playedColor: UIColor = Theme.isDarkThemeEnabled ? .ows_gray15 : .ows_gray60
+
+        let fillColorKeypath = AnimationKeypath(keypath: "**.Fill 1.Color")
+        playPauseButton.setValueProvider(
+            ColorValueProvider(playedColor.lottieColorValue),
+            keypath: fillColorKeypath
+        )
+
+        addArrangedSubview(playPauseButton)
+
+        waveformView.thumbColor = playedColor
+        waveformView.playedColor = playedColor
+        waveformView.unplayedColor = Theme.isDarkThemeEnabled ? .ows_gray60 : .ows_gray25
+        waveformView.audioWaveform = voiceMessageModel.audioWaveform
+        waveformView.autoSetDimension(.height, toSize: 22)
+
+        addArrangedSubview(waveformView)
+
+        let panGestureRecognizer = UIPanGestureRecognizer(target: self, action: #selector(handlePan))
+        addGestureRecognizer(panGestureRecognizer)
+
+        playbackTimeLabel.font = UIFont.ows_dynamicTypeSubheadlineClamped.ows_monospaced
+        playbackTimeLabel.textColor = Theme.ternaryTextColor
+        updateAudioProgress(currentTime: 0)
+        addArrangedSubview(playbackTimeLabel)
+
+        voiceMessageModel.audioPlayer.delegate = self
+    }
+
+    deinit {
+        voiceMessageModel.audioPlayer.stop()
+    }
+
+    required init(coder: NSCoder) {
+        fatalError("init(coder:) has not been implemented")
+    }
+
+    private var isScrubbing = false
+    @objc
+    public func handlePan(_ sender: UIPanGestureRecognizer) {
+        let location = sender.location(in: waveformView)
+        var progress = CGFloatClamp01(location.x / waveformView.width)
+
+        // When in RTL mode, the slider moves in the opposite direction so inverse the ratio.
+        if CurrentAppContext().isRTL {
+            progress = 1 - progress
+        }
+
+        guard let duration = voiceMessageModel.duration else {
+            return owsFailDebug("Missing duration")
+        }
+
+        let currentTime = duration * Double(progress)
+
+        switch sender.state {
+        case .began:
+            isScrubbing = true
+            updateAudioProgress(currentTime: currentTime)
+        case .changed:
+            updateAudioProgress(currentTime: currentTime)
+        case .ended:
+            isScrubbing = false
+            voiceMessageModel.audioPlayer.setCurrentTime(currentTime)
+        case .cancelled, .failed, .possible:
+            isScrubbing = false
+        @unknown default:
+            isScrubbing = false
+        }
+    }
+
+    func updateAudioProgress(currentTime: TimeInterval) {
+        guard let duration = voiceMessageModel.duration else { return }
+        waveformView.value = CGFloatClamp01(CGFloat(currentTime / duration))
+        playbackTimeLabel.text = OWSFormat.formatDurationSeconds(Int(duration - currentTime))
+    }
+
+    @objc
+    func didTogglePlayPause() {
+        cvAudioPlayer.stopAll()
+        playPauseButton.setSelected(!playPauseButton.isSelected, animated: true)
+        voiceMessageModel.audioPlayer.togglePlayState()
+    }
+
+    func setAudioProgress(_ progress: TimeInterval, duration: TimeInterval) {
+        guard !isScrubbing else { return }
+        updateAudioProgress(currentTime: progress)
+    }
+
+    func audioPlayerDidFinish() {
+        playPauseButton.setSelected(false, animated: true)
+        updateAudioProgress(currentTime: 0)
+    }
+}
diff --git a/SignalMessaging/ViewModels/VoiceMessageModel.swift b/SignalMessaging/ViewModels/VoiceMessageModel.swift
index 30f247fe042..344e4a4e3a7 100644
--- a/SignalMessaging/ViewModels/VoiceMessageModel.swift
+++ b/SignalMessaging/ViewModels/VoiceMessageModel.swift
@@ -114,6 +114,12 @@ public class VoiceMessageModel: NSObject {
         return directory
     }
 
+    public lazy var audioWaveform: AudioWaveform? =
+        AudioWaveformManager.audioWaveform(forAudioPath: audioFile.path, waveformPath: waveformFile.path)
+
+    public lazy var audioPlayer: OWSAudioPlayer =
+        .init(mediaUrl: audioFile, audioBehavior: .audioMessagePlayback)
+
     private var audioFile: URL { URL(fileURLWithPath: "voice-memo.\(Self.audioExtension)", relativeTo: directory) }
     private var waveformFile: URL { URL(fileURLWithPath: "waveform.dat", relativeTo: directory) }
     private func outputFileName(at date: Date) -> String {
@@ -130,6 +136,12 @@ public class VoiceMessageModel: NSObject {
     @objc
     public var isRecording: Bool { audioRecorder?.isRecording ?? false }
 
+    public var duration: TimeInterval? {
+        guard OWSFileSystem.fileOrFolderExists(url: audioFile) else { return nil }
+        audioPlayer.setupAudioPlayer()
+        return audioPlayer.duration
+    }
+
     private var audioRecorder: AVAudioRecorder? {
         didSet {
             guard oldValue != audioRecorder else { return }
@@ -184,15 +196,10 @@ public class VoiceMessageModel: NSObject {
         }
     }
 
-    @discardableResult
-    public func stopRecording() -> TimeInterval {
+    public func stopRecording() {
         AssertIsOnMainThread()
 
-        let duration = audioRecorder?.currentTime ?? 0
-
         audioRecorder?.stop()
         audioRecorder = nil
-
-        return duration
     }
 }
diff --git a/SignalMessaging/utils/OWSAudioPlayer.h b/SignalMessaging/utils/OWSAudioPlayer.h
index 92e377f74df..a1c6aa28e31 100644
--- a/SignalMessaging/utils/OWSAudioPlayer.h
+++ b/SignalMessaging/utils/OWSAudioPlayer.h
@@ -1,5 +1,5 @@
 //
-//  Copyright (c) 2020 Open Whisper Systems. All rights reserved.
+//  Copyright (c) 2021 Open Whisper Systems. All rights reserved.
 //
 
 NS_ASSUME_NONNULL_BEGIN
@@ -36,6 +36,7 @@ typedef NS_ENUM(NSUInteger, OWSAudioBehavior) {
 @property (nonatomic, weak) id<OWSAudioPlayerDelegate> delegate;
 
 @property (nonatomic) BOOL isLooping;
+@property (nonatomic, readonly) NSTimeInterval duration;
 
 - (instancetype)initWithMediaUrl:(NSURL *)mediaUrl audioBehavior:(OWSAudioBehavior)audioBehavior;
 
diff --git a/SignalMessaging/utils/OWSAudioPlayer.m b/SignalMessaging/utils/OWSAudioPlayer.m
index 6262332f367..f58b6e8788b 100644
--- a/SignalMessaging/utils/OWSAudioPlayer.m
+++ b/SignalMessaging/utils/OWSAudioPlayer.m
@@ -78,6 +78,11 @@ - (void)dealloc
     [self stop];
 }
 
+- (NSTimeInterval)duration
+{
+    return self.audioPlayer.duration;
+}
+
 #pragma mark -
 
 - (void)applicationDidEnterBackground:(NSNotification *)notification
@@ -241,8 +246,6 @@ - (void)setupAudioPlayer
 
 - (void)stop
 {
-    OWSAssertIsOnMainThread();
-
     self.delegate.audioPlaybackState = AudioPlaybackState_Stopped;
     [self.audioPlayer pause];
     [self.audioPlayerPoller invalidate];
diff --git a/SignalServiceKit/src/Util/AudioWaveform.swift b/SignalServiceKit/src/Util/AudioWaveform.swift
index 132ae2b0732..2e2936918af 100644
--- a/SignalServiceKit/src/Util/AudioWaveform.swift
+++ b/SignalServiceKit/src/Util/AudioWaveform.swift
@@ -22,7 +22,7 @@ public class AudioWaveformManager: NSObject {
 
     private static var cache = [AttachmentId: Weak<AudioWaveform>]()
 
-    private static var observerMap = [AttachmentId: SamplingObserver]()
+    private static var observerMap = [String: SamplingObserver]()
 
     @available(*, unavailable, message: "Do not instantiate this class.")
     private override init() {}
@@ -32,12 +32,31 @@ public class AudioWaveformManager: NSObject {
         unfairLock.withLock {
             let attachmentId = attachment.uniqueId
 
+            guard attachment.isAudio else {
+                owsFailDebug("Not audio.")
+                return nil
+            }
+
+            guard let audioWaveformPath = attachment.audioWaveformPath else {
+                owsFailDebug("Missing audioWaveformPath.")
+                return nil
+            }
+
+            guard let originalFilePath = attachment.originalFilePath else {
+                owsFailDebug("Missing originalFilePath.")
+                return nil
+            }
+
             if let cacheBox = cache[attachmentId],
                let cachedValue = cacheBox.value {
                 return cachedValue
             }
 
-            guard let value = buildAudioWaveForm(forAttachment: attachment) else {
+            guard let value = buildAudioWaveForm(
+                forAudioPath: originalFilePath,
+                waveformPath: audioWaveformPath,
+                identifier: attachmentId
+            ) else {
                 return nil
             }
 
@@ -47,38 +66,36 @@ public class AudioWaveformManager: NSObject {
         }
     }
 
-    // This method should only be called with unfairLock acquired.
-    private static func buildAudioWaveForm(forAttachment attachment: TSAttachmentStream) -> AudioWaveform? {
-
-        let attachmentId = attachment.uniqueId
-
-        guard attachment.isAudio else {
-            owsFailDebug("Not audio.")
-            return nil
-        }
-        guard let audioWaveformPath = attachment.audioWaveformPath else {
-            owsFailDebug("Missing audioWaveformPath.")
-            return nil
+    @objc
+    public static func audioWaveform(forAudioPath audioPath: String, waveformPath: String) -> AudioWaveform? {
+        unfairLock.withLock {
+            guard let value = buildAudioWaveForm(
+                    forAudioPath: audioPath,
+                    waveformPath: waveformPath,
+                    identifier: UUID().uuidString
+            ) else {
+                return nil
+            }
+            return value
         }
+    }
+
+    // This method should only be called with unfairLock acquired.
+    private static func buildAudioWaveForm(forAudioPath audioPath: String, waveformPath: String, identifier: String) -> AudioWaveform? {
 
-        if FileManager.default.fileExists(atPath: audioWaveformPath) {
+        if FileManager.default.fileExists(atPath: waveformPath) {
             // We have a cached waveform on disk, read it into memory.
             do {
-                return try AudioWaveform(contentsOfFile: audioWaveformPath)
+                return try AudioWaveform(contentsOfFile: waveformPath)
             } catch {
                 owsFailDebug("Error: \(error)")
 
                 // Remove the file from disk and create a new one.
-                OWSFileSystem.deleteFileIfExists(audioWaveformPath)
+                OWSFileSystem.deleteFileIfExists(waveformPath)
             }
         }
 
-        guard let originalFilePath = attachment.originalFilePath else {
-            owsFailDebug("Missing originalFilePath.")
-            return nil
-        }
-
-        var asset = AVURLAsset(url: URL(fileURLWithPath: originalFilePath))
+        var asset = AVURLAsset(url: URL(fileURLWithPath: audioPath))
 
         // If the asset isn't readable, we may not be able to generate a waveform for this file.
         //
@@ -87,14 +104,12 @@ public class AudioWaveformManager: NSObject {
         // the file extension, we can. This is pretty brittle and hopefully android will
         // be able to fix the issue in the future in which case `isReadable` will become
         // true and this path will no longer be hit.
-        if !asset.isReadable,
-           attachment.isVoiceMessage,
-           originalFilePath.hasSuffix("m4a") {
+        if !asset.isReadable, audioPath.hasSuffix("m4a") {
 
             let symlinkPath = OWSFileSystem.temporaryFilePath(fileExtension: "aac")
             do {
                 try FileManager.default.createSymbolicLink(atPath: symlinkPath,
-                                                           withDestinationPath: originalFilePath)
+                                                           withDestinationPath: audioPath)
             } catch {
                 owsFailDebug("Failed to create voice memo symlink: \(error)")
                 return nil
@@ -111,15 +126,15 @@ public class AudioWaveformManager: NSObject {
             return nil
         }
 
-        Logger.verbose("Sampling waveform: \(attachmentId)")
+        Logger.verbose("Sampling waveform: \(identifier)")
 
         let waveform = AudioWaveform()
 
         // Listen for sampling completion so we can cache the final waveform to disk.
         let observer = SamplingObserver(waveform: waveform,
-                                        attachmentId: attachmentId,
-                                        audioWaveformPath: audioWaveformPath)
-        observerMap[attachmentId] = observer
+                                        identifier: identifier,
+                                        audioWaveformPath: waveformPath)
+        observerMap[identifier] = observer
         waveform.addSamplingObserver(observer)
 
         waveform.beginSampling(for: asset)
@@ -130,22 +145,22 @@ public class AudioWaveformManager: NSObject {
     private class SamplingObserver: AudioWaveformSamplingObserver {
         // Retain waveform until sampling is complete.
         let waveform: AudioWaveform
-        let attachmentId: AttachmentId
+        let identifier: String
         let audioWaveformPath: String
 
         init(waveform: AudioWaveform,
-             attachmentId: AttachmentId,
+             identifier: String,
              audioWaveformPath: String) {
             self.waveform = waveform
-            self.attachmentId = attachmentId
+            self.identifier = identifier
             self.audioWaveformPath = audioWaveformPath
         }
 
         func audioWaveformDidFinishSampling(_ audioWaveform: AudioWaveform) {
-            let attachmentId = self.attachmentId
+            let identifier = self.identifier
             let audioWaveformPath = self.audioWaveformPath
 
-            Logger.verbose("Sampling waveform complete: \(attachmentId)")
+            Logger.verbose("Sampling waveform complete: \(identifier)")
 
             DispatchQueue.global().async {
                 AudioWaveformManager.unfairLock.withLock {
@@ -162,8 +177,8 @@ public class AudioWaveformManager: NSObject {
                     }
 
                     // Discard observer.
-                    owsAssertDebug(observerMap[attachmentId] != nil)
-                    observerMap[attachmentId] = nil
+                    owsAssertDebug(observerMap[identifier] != nil)
+                    observerMap[identifier] = nil
                 }
             }
         }
