diff --git a/mobile/lib/services/machine_learning/ml_isolate.dart b/mobile/lib/services/machine_learning/ml_isolate.dart
index 0c7dc66cc3d..01a36a768fc 100644
--- a/mobile/lib/services/machine_learning/ml_isolate.dart
+++ b/mobile/lib/services/machine_learning/ml_isolate.dart
@@ -15,7 +15,7 @@ import "package:photos/services/remote_assets_service.dart";
 import "package:photos/utils/ml_util.dart";
 import "package:synchronized/synchronized.dart";
 
-enum MLOperation { analyzeImage, loadModels, runClipText }
+enum MLOperation { analyzeImage, loadModels }
 
 class MLIsolate {
   static final _logger = Logger("MLIsolate");
@@ -95,10 +95,6 @@ class MLIsolate {
             await ClipImageEncoder.instance.loadModel(useEntePlugin: true);
             sendPort.send(true);
             break;
-          case MLOperation.runClipText:
-            final textEmbedding = await ClipTextEncoder.predict(args);
-            sendPort.send(List.from(textEmbedding, growable: false));
-            break;
         }
       } catch (e, s) {
         _logger.severe("Error in FaceML isolate", e, s);
@@ -229,30 +225,6 @@ class MLIsolate {
     return result;
   }
 
-  Future<List<double>> runClipText(String query) async {
-    try {
-      final int clipAddress = ClipTextEncoder.instance.sessionAddress;
-      final String remotePath = ClipTextEncoder.instance.vocabRemotePath;
-      final String tokenizerVocabPath =
-          await RemoteAssetsService.instance.getAssetPath(remotePath);
-      final textEmbedding = await _runInIsolate(
-        (
-          MLOperation.runClipText,
-          {
-            "text": query,
-            "address": clipAddress,
-            "vocabPath": tokenizerVocabPath,
-            "useEntePlugin": Platform.isAndroid,
-          }
-        ),
-      ) as List<double>;
-      return textEmbedding;
-    } catch (e, s) {
-      _logger.severe("Could not run clip text in isolate", e, s);
-      rethrow;
-    }
-  }
-
   Future<void> loadModels() async {
     try {
       await _runInIsolate(
diff --git a/mobile/lib/services/machine_learning/semantic_search/semantic_search_service.dart b/mobile/lib/services/machine_learning/semantic_search/semantic_search_service.dart
index ffe6a5904a5..5460a975ef7 100644
--- a/mobile/lib/services/machine_learning/semantic_search/semantic_search_service.dart
+++ b/mobile/lib/services/machine_learning/semantic_search/semantic_search_service.dart
@@ -16,12 +16,12 @@ import "package:photos/models/file/file.dart";
 import "package:photos/models/ml/ml_versions.dart";
 import "package:photos/services/collections_service.dart";
 import "package:photos/services/machine_learning/face_ml/face_clustering/cosine_distance.dart";
-import "package:photos/services/machine_learning/ml_isolate.dart";
 import "package:photos/services/machine_learning/ml_result.dart";
 import "package:photos/services/machine_learning/semantic_search/clip/clip_image_encoder.dart";
 import "package:photos/services/machine_learning/semantic_search/clip/clip_text_encoder.dart";
 import 'package:photos/services/machine_learning/semantic_search/embedding_store.dart';
 import "package:photos/utils/debouncer.dart";
+import "package:photos/utils/image_isolate.dart";
 import "package:photos/utils/local_settings.dart";
 import "package:photos/utils/ml_util.dart";
 
@@ -295,7 +295,7 @@ class SemanticSearchService {
     if (cachedResult != null) {
       return cachedResult;
     }
-    final textEmbedding = await MLIsolate.instance.runClipText(query);
+    final textEmbedding = await ImageIsolate.instance.runClipText(query);
     _queryCache.put(query, textEmbedding);
     return textEmbedding;
   }
diff --git a/mobile/lib/utils/image_isolate.dart b/mobile/lib/utils/image_isolate.dart
index 9ebad2bb944..e22a5ff2cea 100644
--- a/mobile/lib/utils/image_isolate.dart
+++ b/mobile/lib/utils/image_isolate.dart
@@ -1,16 +1,19 @@
 import 'dart:async';
-import "dart:io" show File;
+import "dart:io" show File, Platform;
 import 'dart:isolate';
 import 'dart:typed_data' show Uint8List;
 
 import "package:dart_ui_isolate/dart_ui_isolate.dart";
 import "package:logging/logging.dart";
 import "package:photos/face/model/box.dart";
+import "package:photos/services/machine_learning/semantic_search/clip/clip_text_encoder.dart";
+import "package:photos/services/remote_assets_service.dart";
 import "package:photos/utils/image_ml_util.dart";
 import "package:synchronized/synchronized.dart";
 
 enum ImageOperation {
   generateFaceThumbnails,
+  runClipText,
 }
 
 class ImageIsolate {
@@ -88,6 +91,10 @@ class ImageIsolate {
               faceBoxes,
             );
             sendPort.send(List.from(results));
+          case ImageOperation.runClipText:
+            final textEmbedding = await ClipTextEncoder.predict(args);
+            sendPort.send(List.from(textEmbedding, growable: false));
+            break;
         }
       } catch (e, stackTrace) {
         sendPort
@@ -175,4 +182,28 @@ class ImageIsolate {
       ),
     ).then((value) => value.cast<Uint8List>());
   }
+
+  Future<List<double>> runClipText(String query) async {
+    try {
+      final int clipAddress = ClipTextEncoder.instance.ffiSessionAddress;
+      final String remotePath = ClipTextEncoder.instance.vocabRemotePath;
+      final String tokenizerVocabPath =
+          await RemoteAssetsService.instance.getAssetPath(remotePath);
+      final textEmbedding = await _runInIsolate(
+        (
+          ImageOperation.runClipText,
+          {
+            "text": query,
+            "address": clipAddress,
+            "vocabPath": tokenizerVocabPath,
+            "useEntePlugin": Platform.isAndroid,
+          }
+        ),
+      ) as List<double>;
+      return textEmbedding;
+    } catch (e, s) {
+      _logger.severe("Could not run clip text in isolate", e, s);
+      rethrow;
+    }
+  }
 }
