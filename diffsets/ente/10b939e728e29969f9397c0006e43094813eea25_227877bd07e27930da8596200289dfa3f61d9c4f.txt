diff --git a/mobile/lib/services/machine_learning/ml_service.dart b/mobile/lib/services/machine_learning/ml_service.dart
index 1d54325cbf5..fb2d9c12516 100644
--- a/mobile/lib/services/machine_learning/ml_service.dart
+++ b/mobile/lib/services/machine_learning/ml_service.dart
@@ -623,6 +623,7 @@ class MLService {
 
   Future<void> _ensureReadyForInference() async {
     await _initIsolate();
+    await _initModelsUsingFfiBasedPlugin();
     if (Platform.isAndroid) {
       await _initModelUsingEntePlugin();
     } else {
diff --git a/mobile/lib/services/machine_learning/semantic_search/clip/clip_text_encoder.dart b/mobile/lib/services/machine_learning/semantic_search/clip/clip_text_encoder.dart
index 2840aa52c36..58a042c5343 100644
--- a/mobile/lib/services/machine_learning/semantic_search/clip/clip_text_encoder.dart
+++ b/mobile/lib/services/machine_learning/semantic_search/clip/clip_text_encoder.dart
@@ -2,13 +2,18 @@ import "dart:math";
 
 import "package:flutter/foundation.dart";
 import "package:logging/logging.dart";
+import "package:onnx_dart/onnx_dart.dart";
 import "package:onnxruntime/onnxruntime.dart";
+import "package:photos/extensions/stop_watch.dart";
 import "package:photos/services/machine_learning/ml_model.dart";
 import 'package:photos/services/machine_learning/semantic_search/clip/clip_text_tokenizer.dart';
+import "package:photos/utils/ml_util.dart";
 
 class ClipTextEncoder extends MlModel {
   static const kRemoteBucketModelPath = "clip-text-vit-32-float32-int32.onnx";
+
   // static const kRemoteBucketModelPath = "clip-text-vit-32-uint8.onnx";
+  static const _modelName = "ClipTextEncoder";
 
   @override
   String get modelRemotePath => kModelBucketEndpoint + kRemoteBucketModelPath;
@@ -18,7 +23,7 @@ class ClipTextEncoder extends MlModel {
   static final _logger = Logger('ClipTextEncoder');
 
   @override
-  String get modelName => "ClipTextEncoder";
+  String get modelName => _modelName;
 
   // Singleton pattern
   ClipTextEncoder._privateConstructor();
@@ -28,9 +33,17 @@ class ClipTextEncoder extends MlModel {
   static Future<List<double>> infer(Map args) async {
     final text = args["text"];
     final address = args["address"] as int;
-    final runOptions = OrtRunOptions();
     final List<int> tokenize = await ClipTextTokenizer.instance.tokenize(text);
-    final data = List.filled(1, Int32List.fromList(tokenize));
+    final int32list = Int32List.fromList(tokenize);
+    return _runFFIBasedPredict(int32list, address);
+  }
+
+  static List<double> _runFFIBasedPredict(
+    Int32List int32list,
+    int address,
+  ) {
+    final runOptions = OrtRunOptions();
+    final data = List.filled(1, int32list);
     final inputOrt = OrtValueTensor.createTensorWithDataList(data, [1, 77]);
     final inputs = {'input': inputOrt};
     final session = OrtSession.fromAddress(address);
@@ -40,12 +53,23 @@ class ClipTextEncoder extends MlModel {
     for (int i = 0; i < 512; i++) {
       textNormalization += embedding[i] * embedding[i];
     }
-
     final double sqrtTextNormalization = sqrt(textNormalization);
     for (int i = 0; i < 512; i++) {
       embedding[i] = embedding[i] / sqrtTextNormalization;
     }
+    return embedding;
+  }
 
-    return (embedding);
+  static Future<List<double>> _runEntePlugin(Int32List int32list) async {
+    final w = EnteWatch("ClipTextEncoder._runEntePlugin")..start();
+    final OnnxDart plugin = OnnxDart();
+    final result = await plugin.predictInt(
+      int32list,
+      _modelName,
+    );
+    final List<double> embedding = result!.sublist(0, 512);
+    normalizeEmbedding(embedding);
+    w.stopWithLog("done");
+    return embedding;
   }
 }
diff --git a/mobile/lib/services/machine_learning/semantic_search/semantic_search_service.dart b/mobile/lib/services/machine_learning/semantic_search/semantic_search_service.dart
index a03fe19a5a5..5d6475bb55b 100644
--- a/mobile/lib/services/machine_learning/semantic_search/semantic_search_service.dart
+++ b/mobile/lib/services/machine_learning/semantic_search/semantic_search_service.dart
@@ -69,9 +69,13 @@ class SemanticSearchService {
 
     // ignore: unawaited_futures
     _loadTextModel().then((_) async {
-      _logger.info("Getting text embedding");
-      await _getTextEmbedding("warm up text encoder");
-      _logger.info("Got text embedding");
+      try {
+        _logger.info("Getting text embedding");
+        await _getTextEmbedding("warm up text encoder");
+        _logger.info("Got text embedding");
+      } catch (e) {
+        _logger.severe("Failed to get text embedding", e);
+      }
     });
   }
 
@@ -257,6 +261,7 @@ class SemanticSearchService {
     _logger.info("Initializing ML framework");
     try {
       await ClipTextEncoder.instance.loadModel();
+      await ClipTextEncoder.instance.loadModel(useEntePlugin: true);
       _textModelIsLoaded = true;
     } catch (e, s) {
       _logger.severe("Clip text loading failed", e, s);
@@ -291,13 +296,16 @@ class SemanticSearchService {
     }
     try {
       final int clipAddress = ClipTextEncoder.instance.sessionAddress;
-      final textEmbedding = await _computer.compute(
-        ClipTextEncoder.infer,
-        param: {
-          "text": query,
-          "address": clipAddress,
-        },
-      ) as List<double>;
+      // final textEmbedding = await _computer.compute(
+      //   ClipTextEncoder.infer,
+      //   param: {
+      //     "text": query,
+      //     "address": clipAddress,
+      //   },
+      // ) as List<double>;
+      final textEmbedding = await ClipTextEncoder.infer(
+        {"text": query, "address": clipAddress},
+      );
       _queryCache.put(query, textEmbedding);
       return textEmbedding;
     } catch (e) {
@@ -343,6 +351,7 @@ class SemanticSearchService {
       clipImageAddress,
       useEntePlugin: useEntePlugin,
     );
+
     final clipResult = ClipResult(fileID: enteFileID, embedding: embedding);
 
     return clipResult;
