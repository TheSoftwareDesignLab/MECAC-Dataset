diff --git a/frontend/appflowy_flutter/lib/ai/service/ai_model_state_notifier.dart b/frontend/appflowy_flutter/lib/ai/service/ai_model_state_notifier.dart
index c175c06d07cd5..c36035624093d 100644
--- a/frontend/appflowy_flutter/lib/ai/service/ai_model_state_notifier.dart
+++ b/frontend/appflowy_flutter/lib/ai/service/ai_model_state_notifier.dart
@@ -79,7 +79,7 @@ class AIModelStateNotifier {
     _aiModelSwitchListener.start(
       onUpdateSelectedModel: (model) async {
         final updatedModels = _availableModels?.deepCopy()
-          ?..selectedModel = model;
+          ?..globalModel = model;
         _availableModels = updatedModels;
         _notifyAvailableModelsChanged();
 
@@ -161,7 +161,7 @@ class AIModelStateNotifier {
       );
     }
 
-    if (!availableModels.selectedModel.isLocal) {
+    if (!availableModels.globalModel.isLocal) {
       return AIModelState(
         type: AiType.cloud,
         hintText: LocaleKeys.chat_inputMessageHint.tr(),
@@ -199,7 +199,7 @@ class AIModelStateNotifier {
     if (availableModels == null) {
       return ([], null);
     }
-    return (availableModels.models, availableModels.selectedModel);
+    return (availableModels.models, availableModels.globalModel);
   }
 
   void _notifyAvailableModelsChanged() {
diff --git a/frontend/appflowy_flutter/lib/ai/widgets/prompt_input/select_model_menu.dart b/frontend/appflowy_flutter/lib/ai/widgets/prompt_input/select_model_menu.dart
index 36b389f8c4c54..317f90ac21f23 100644
--- a/frontend/appflowy_flutter/lib/ai/widgets/prompt_input/select_model_menu.dart
+++ b/frontend/appflowy_flutter/lib/ai/widgets/prompt_input/select_model_menu.dart
@@ -218,9 +218,10 @@ class _CurrentModelButton extends StatelessWidget {
         child: SizedBox(
           height: DesktopAIPromptSizes.actionBarButtonSize,
           child: AnimatedSize(
-            duration: const Duration(milliseconds: 50),
-            curve: Curves.easeInOut,
+            duration: const Duration(milliseconds: 200),
+            curve: Curves.easeOutCubic,
             alignment: AlignmentDirectional.centerStart,
+            clipBehavior: Clip.none,
             child: FlowyHover(
               style: const HoverStyle(
                 borderRadius: BorderRadius.all(Radius.circular(8)),
@@ -228,6 +229,7 @@ class _CurrentModelButton extends StatelessWidget {
               child: Padding(
                 padding: const EdgeInsetsDirectional.all(4.0),
                 child: Row(
+                  mainAxisSize: MainAxisSize.min,
                   children: [
                     Padding(
                       // TODO: remove this after change icon to 20px
@@ -239,14 +241,18 @@ class _CurrentModelButton extends StatelessWidget {
                       ),
                     ),
                     if (model != null && !model!.isDefault)
-                      Padding(
-                        padding: EdgeInsetsDirectional.only(end: 2.0),
-                        child: FlowyText(
-                          model!.i18n,
-                          fontSize: 12,
-                          figmaLineHeight: 16,
-                          color: Theme.of(context).hintColor,
-                          overflow: TextOverflow.ellipsis,
+                      AnimatedSize(
+                        duration: const Duration(milliseconds: 150),
+                        curve: Curves.easeOutCubic,
+                        child: Padding(
+                          padding: EdgeInsetsDirectional.only(end: 2.0),
+                          child: FlowyText(
+                            model!.i18n,
+                            fontSize: 12,
+                            figmaLineHeight: 16,
+                            color: Theme.of(context).hintColor,
+                            overflow: TextOverflow.ellipsis,
+                          ),
                         ),
                       ),
                     FlowySvg(
diff --git a/frontend/appflowy_flutter/lib/mobile/presentation/setting/ai/ai_settings_group.dart b/frontend/appflowy_flutter/lib/mobile/presentation/setting/ai/ai_settings_group.dart
index fe0e0a71601bb..6bed7ed035bff 100644
--- a/frontend/appflowy_flutter/lib/mobile/presentation/setting/ai/ai_settings_group.dart
+++ b/frontend/appflowy_flutter/lib/mobile/presentation/setting/ai/ai_settings_group.dart
@@ -36,7 +36,7 @@ class AiSettingsGroup extends StatelessWidget {
               MobileSettingItem(
                 name: LocaleKeys.settings_aiPage_keys_llmModelType.tr(),
                 trailing: MobileSettingTrailing(
-                  text: state.availableModels?.selectedModel.name ?? "",
+                  text: state.availableModels?.globalModel.name ?? "",
                 ),
                 onTap: () => _onLLMModelTypeTap(context, state),
               ),
@@ -73,7 +73,7 @@ class AiSettingsGroup extends StatelessWidget {
                   text: entry.value.name,
                   showTopBorder: entry.key == 0,
                   isSelected:
-                      availableModels?.selectedModel.name == entry.value.name,
+                      availableModels?.globalModel.name == entry.value.name,
                   onTap: () {
                     context
                         .read<SettingsAIBloc>()
diff --git a/frontend/appflowy_flutter/lib/workspace/application/settings/ai/ollama_setting_bloc.dart b/frontend/appflowy_flutter/lib/workspace/application/settings/ai/ollama_setting_bloc.dart
index 2659292b11abc..3b9060bb08f73 100644
--- a/frontend/appflowy_flutter/lib/workspace/application/settings/ai/ollama_setting_bloc.dart
+++ b/frontend/appflowy_flutter/lib/workspace/application/settings/ai/ollama_setting_bloc.dart
@@ -6,177 +6,181 @@ import 'package:appflowy_backend/protobuf/flowy-ai/entities.pb.dart';
 import 'package:appflowy_result/appflowy_result.dart';
 import 'package:bloc/bloc.dart';
 import 'package:collection/collection.dart';
-import 'package:freezed_annotation/freezed_annotation.dart';
 import 'package:equatable/equatable.dart';
+import 'package:freezed_annotation/freezed_annotation.dart';
 
 part 'ollama_setting_bloc.freezed.dart';
 
 const kDefaultChatModel = 'llama3.1:latest';
 const kDefaultEmbeddingModel = 'nomic-embed-text:latest';
 
+/// Extension methods to map between PB and UI models
 class OllamaSettingBloc extends Bloc<OllamaSettingEvent, OllamaSettingState> {
   OllamaSettingBloc() : super(const OllamaSettingState()) {
-    on<OllamaSettingEvent>(_handleEvent);
+    on<_Started>(_handleStarted);
+    on<_DidLoadLocalModels>(_onLoadLocalModels);
+    on<_DidLoadSetting>(_onLoadSetting);
+    on<_UpdateSetting>(_onLoadSetting);
+    on<_OnEdit>(_onEdit);
+    on<_OnSubmit>(_onSubmit);
+    on<_SetDefaultModel>(_onSetDefaultModel);
   }
 
-  Future<void> _handleEvent(
-    OllamaSettingEvent event,
+  Future<void> _handleStarted(
+    _Started event,
     Emitter<OllamaSettingState> emit,
   ) async {
-    event.when(
-      started: () {
-        AIEventGetLocalAISetting().send().fold(
-          (setting) {
-            if (!isClosed) {
-              add(OllamaSettingEvent.didLoadSetting(setting));
-            }
-          },
-          Log.error,
-        );
-      },
-      didLoadSetting: (setting) => _updateSetting(setting, emit),
-      updateSetting: (setting) => _updateSetting(setting, emit),
-      onEdit: (content, settingType) {
-        final updatedSubmittedItems = state.submittedItems
-            .map(
-              (item) => item.settingType == settingType
-                  ? SubmittedItem(
-                      content: content,
-                      settingType: item.settingType,
-                    )
-                  : item,
-            )
-            .toList();
-
-        // Convert both lists to maps: {settingType: content}
-        final updatedMap = {
-          for (final item in updatedSubmittedItems)
-            item.settingType: item.content,
-        };
-
-        final inputMap = {
-          for (final item in state.inputItems) item.settingType: item.content,
-        };
-
-        // Compare maps instead of lists
-        final isEdited = !const MapEquality<SettingType, String>()
-            .equals(updatedMap, inputMap);
-
-        emit(
-          state.copyWith(
-            submittedItems: updatedSubmittedItems,
-            isEdited: isEdited,
-          ),
-        );
-      },
-      submit: () {
-        final setting = LocalAISettingPB();
-        final settingUpdaters = <SettingType, void Function(String)>{
-          SettingType.serverUrl: (value) => setting.serverUrl = value,
-          SettingType.chatModel: (value) => setting.defaultModel = value,
-          SettingType.embeddingModel: (value) =>
-              setting.embeddingModelName = value,
-        };
-
-        for (final item in state.submittedItems) {
-          settingUpdaters[item.settingType]?.call(item.content);
-        }
-        add(OllamaSettingEvent.updateSetting(setting));
-        AIEventUpdateLocalAISetting(setting).send().fold(
-              (_) => Log.info('AI setting updated successfully'),
-              (err) => Log.error("update ai setting failed: $err"),
-            );
-      },
-    );
+    try {
+      final results = await Future.wait([
+        AIEventGetLocalAIModels().send().then((r) => r.getOrThrow()),
+        AIEventGetLocalAISetting().send().then((r) => r.getOrThrow()),
+      ]);
+
+      final models = results[0] as AvailableModelsPB;
+      final setting = results[1] as LocalAISettingPB;
+
+      if (!isClosed) {
+        add(OllamaSettingEvent.didLoadLocalModels(models));
+        add(OllamaSettingEvent.didLoadSetting(setting));
+      }
+    } catch (e, st) {
+      Log.error('Failed to load initial AI data: $e\n$st');
+    }
   }
 
-  void _updateSetting(
-    LocalAISettingPB setting,
+  void _onLoadLocalModels(
+    _DidLoadLocalModels event,
+    Emitter<OllamaSettingState> emit,
+  ) {
+    emit(state.copyWith(localModels: event.models));
+  }
+
+  void _onLoadSetting(
+    dynamic event,
     Emitter<OllamaSettingState> emit,
   ) {
+    final setting = (event as dynamic).setting as LocalAISettingPB;
+    final submitted = setting.toSubmittedItems();
     emit(
       state.copyWith(
         setting: setting,
-        inputItems: _createInputItems(setting),
-        submittedItems: _createSubmittedItems(setting),
-        isEdited: false, // Reset to false when the setting is loaded/updated.
+        inputItems: setting.toInputItems(),
+        submittedItems: submitted,
+        originalMap: {
+          for (final item in submitted) item.settingType: item.content,
+        },
+        isEdited: false,
       ),
     );
   }
 
-  List<SettingItem> _createInputItems(LocalAISettingPB setting) => [
-        SettingItem(
-          content: setting.serverUrl,
-          hintText: 'http://localhost:11434',
-          settingType: SettingType.serverUrl,
-        ),
-        SettingItem(
-          content: setting.defaultModel,
-          hintText: kDefaultChatModel,
-          settingType: SettingType.chatModel,
-        ),
-        SettingItem(
-          content: setting.embeddingModelName,
-          hintText: kDefaultEmbeddingModel,
-          settingType: SettingType.embeddingModel,
-        ),
-      ];
+  void _onEdit(
+    _OnEdit event,
+    Emitter<OllamaSettingState> emit,
+  ) {
+    final updated = state.submittedItems
+        .map(
+          (item) => item.settingType == event.settingType
+              ? item.copyWith(content: event.content)
+              : item,
+        )
+        .toList();
+
+    final currentMap = {for (final i in updated) i.settingType: i.content};
+    final isEdited = !const MapEquality<SettingType, String>()
+        .equals(state.originalMap, currentMap);
+
+    emit(state.copyWith(submittedItems: updated, isEdited: isEdited));
+  }
 
-  List<SubmittedItem> _createSubmittedItems(LocalAISettingPB setting) => [
-        SubmittedItem(
-          content: setting.serverUrl,
-          settingType: SettingType.serverUrl,
-        ),
-        SubmittedItem(
-          content: setting.defaultModel,
-          settingType: SettingType.chatModel,
-        ),
-        SubmittedItem(
-          content: setting.embeddingModelName,
-          settingType: SettingType.embeddingModel,
-        ),
-      ];
+  void _onSubmit(
+    _OnSubmit event,
+    Emitter<OllamaSettingState> emit,
+  ) {
+    final pb = LocalAISettingPB();
+    for (final item in state.submittedItems) {
+      switch (item.settingType) {
+        case SettingType.serverUrl:
+          pb.serverUrl = item.content;
+          break;
+        case SettingType.chatModel:
+          pb.globalChatModel = state.selectedModel?.name ?? item.content;
+          break;
+        case SettingType.embeddingModel:
+          pb.embeddingModelName = item.content;
+          break;
+      }
+    }
+    add(OllamaSettingEvent.updateSetting(pb));
+    AIEventUpdateLocalAISetting(pb).send().fold(
+          (_) => Log.info('AI setting updated successfully'),
+          (err) => Log.error('Update AI setting failed: $err'),
+        );
+  }
+
+  void _onSetDefaultModel(
+    _SetDefaultModel event,
+    Emitter<OllamaSettingState> emit,
+  ) {
+    emit(state.copyWith(selectedModel: event.model, isEdited: true));
+  }
 }
 
-// Create an enum for setting type.
+/// Setting types for mapping
 enum SettingType {
   serverUrl,
   chatModel,
-  embeddingModel; // semicolon needed after the enum values
+  embeddingModel;
 
   String get title {
     switch (this) {
       case SettingType.serverUrl:
         return 'Ollama server url';
       case SettingType.chatModel:
-        return 'Chat model name';
+        return 'Default model name';
       case SettingType.embeddingModel:
         return 'Embedding model name';
     }
   }
 }
 
+/// Input field representation
 class SettingItem extends Equatable {
   const SettingItem({
     required this.content,
     required this.hintText,
     required this.settingType,
   });
+
   final String content;
   final String hintText;
   final SettingType settingType;
+
   @override
   List<Object?> get props => [content, settingType];
 }
 
+/// Items pending submission
 class SubmittedItem extends Equatable {
   const SubmittedItem({
     required this.content,
     required this.settingType,
   });
+
   final String content;
   final SettingType settingType;
 
+  /// Returns a copy of this SubmittedItem with given fields updated.
+  SubmittedItem copyWith({
+    String? content,
+    SettingType? settingType,
+  }) {
+    return SubmittedItem(
+      content: content ?? this.content,
+      settingType: settingType ?? this.settingType,
+    );
+  }
+
   @override
   List<Object?> get props => [content, settingType];
 }
@@ -184,10 +188,18 @@ class SubmittedItem extends Equatable {
 @freezed
 class OllamaSettingEvent with _$OllamaSettingEvent {
   const factory OllamaSettingEvent.started() = _Started;
-  const factory OllamaSettingEvent.didLoadSetting(LocalAISettingPB setting) =
-      _DidLoadSetting;
-  const factory OllamaSettingEvent.updateSetting(LocalAISettingPB setting) =
-      _UpdateSetting;
+  const factory OllamaSettingEvent.didLoadLocalModels(
+    AvailableModelsPB models,
+  ) = _DidLoadLocalModels;
+  const factory OllamaSettingEvent.didLoadSetting(
+    LocalAISettingPB setting,
+  ) = _DidLoadSetting;
+  const factory OllamaSettingEvent.updateSetting(
+    LocalAISettingPB setting,
+  ) = _UpdateSetting;
+  const factory OllamaSettingEvent.setDefaultModel(
+    AIModelPB model,
+  ) = _SetDefaultModel;
   const factory OllamaSettingEvent.onEdit(
     String content,
     SettingType settingType,
@@ -199,25 +211,42 @@ class OllamaSettingEvent with _$OllamaSettingEvent {
 class OllamaSettingState with _$OllamaSettingState {
   const factory OllamaSettingState({
     LocalAISettingPB? setting,
-    @Default([
-      SettingItem(
-        content: 'http://localhost:11434',
-        hintText: 'http://localhost:11434',
-        settingType: SettingType.serverUrl,
-      ),
-      SettingItem(
-        content: kDefaultChatModel,
-        hintText: kDefaultChatModel,
-        settingType: SettingType.chatModel,
-      ),
-      SettingItem(
-        content: kDefaultEmbeddingModel,
-        hintText: kDefaultEmbeddingModel,
-        settingType: SettingType.embeddingModel,
-      ),
-    ])
-    List<SettingItem> inputItems,
+    @Default([]) List<SettingItem> inputItems,
+    AIModelPB? selectedModel,
+    AvailableModelsPB? localModels,
+    AIModelPB? defaultModel,
     @Default([]) List<SubmittedItem> submittedItems,
     @Default(false) bool isEdited,
-  }) = _PluginStateState;
+    @Default({}) Map<SettingType, String> originalMap,
+  }) = _OllamaSettingState;
+}
+
+extension on LocalAISettingPB {
+  List<SettingItem> toInputItems() => [
+        SettingItem(
+          content: serverUrl,
+          hintText: 'http://localhost:11434',
+          settingType: SettingType.serverUrl,
+        ),
+        SettingItem(
+          content: embeddingModelName,
+          hintText: kDefaultEmbeddingModel,
+          settingType: SettingType.embeddingModel,
+        ),
+      ];
+
+  List<SubmittedItem> toSubmittedItems() => [
+        SubmittedItem(
+          content: serverUrl,
+          settingType: SettingType.serverUrl,
+        ),
+        SubmittedItem(
+          content: globalChatModel,
+          settingType: SettingType.chatModel,
+        ),
+        SubmittedItem(
+          content: embeddingModelName,
+          settingType: SettingType.embeddingModel,
+        ),
+      ];
 }
diff --git a/frontend/appflowy_flutter/lib/workspace/presentation/settings/pages/setting_ai_view/model_selection.dart b/frontend/appflowy_flutter/lib/workspace/presentation/settings/pages/setting_ai_view/model_selection.dart
index 7357c2951cd19..83f4ff603e418 100644
--- a/frontend/appflowy_flutter/lib/workspace/presentation/settings/pages/setting_ai_view/model_selection.dart
+++ b/frontend/appflowy_flutter/lib/workspace/presentation/settings/pages/setting_ai_view/model_selection.dart
@@ -30,7 +30,7 @@ class AIModelSelection extends StatelessWidget {
 
         final localModels = models.where((model) => model.isLocal).toList();
         final cloudModels = models.where((model) => !model.isLocal).toList();
-        final selectedModel = state.availableModels!.selectedModel;
+        final selectedModel = state.availableModels!.globalModel;
 
         return Padding(
           padding: const EdgeInsets.symmetric(vertical: 6),
diff --git a/frontend/appflowy_flutter/lib/workspace/presentation/settings/pages/setting_ai_view/ollama_setting.dart b/frontend/appflowy_flutter/lib/workspace/presentation/settings/pages/setting_ai_view/ollama_setting.dart
index 6f38043927add..fc56fc61e771c 100644
--- a/frontend/appflowy_flutter/lib/workspace/presentation/settings/pages/setting_ai_view/ollama_setting.dart
+++ b/frontend/appflowy_flutter/lib/workspace/presentation/settings/pages/setting_ai_view/ollama_setting.dart
@@ -7,6 +7,14 @@ import 'package:flowy_infra_ui/widget/spacing.dart';
 import 'package:flutter/material.dart';
 import 'package:flutter_bloc/flutter_bloc.dart';
 
+import 'package:appflowy/ai/ai.dart';
+import 'package:appflowy_backend/protobuf/flowy-ai/entities.pb.dart';
+
+import 'package:appflowy/generated/locale_keys.g.dart';
+import 'package:appflowy/workspace/presentation/settings/shared/af_dropdown_menu_entry.dart';
+import 'package:appflowy/workspace/presentation/settings/shared/settings_dropdown.dart';
+import 'package:easy_localization/easy_localization.dart';
+
 class OllamaSettingPage extends StatelessWidget {
   const OllamaSettingPage({super.key});
 
@@ -32,6 +40,7 @@ class OllamaSettingPage extends StatelessWidget {
               children: [
                 for (final item in state.inputItems)
                   _SettingItemWidget(item: item),
+                const LocalAIModelSelection(),
                 _SaveButton(isEdited: state.isEdited),
               ],
             ),
@@ -113,3 +122,59 @@ class _SaveButton extends StatelessWidget {
     );
   }
 }
+
+class LocalAIModelSelection extends StatelessWidget {
+  const LocalAIModelSelection({super.key});
+  static const double height = 49;
+
+  @override
+  Widget build(BuildContext context) {
+    return BlocBuilder<OllamaSettingBloc, OllamaSettingState>(
+      buildWhen: (previous, current) =>
+          previous.localModels != current.localModels,
+      builder: (context, state) {
+        final models = state.localModels;
+        if (models == null) {
+          return const SizedBox(
+            // Using same height as SettingsDropdown to avoid layout shift
+            height: height,
+          );
+        }
+
+        return Column(
+          crossAxisAlignment: CrossAxisAlignment.start,
+          children: [
+            FlowyText.medium(
+              LocaleKeys.settings_aiPage_keys_globalLLMModel.tr(),
+              fontSize: 12,
+              figmaLineHeight: 16,
+            ),
+            const VSpace(4),
+            SizedBox(
+              height: 40,
+              child: SettingsDropdown<AIModelPB>(
+                key: const Key('_AIModelSelection'),
+                onChanged: (model) => context
+                    .read<OllamaSettingBloc>()
+                    .add(OllamaSettingEvent.setDefaultModel(model)),
+                selectedOption: models.globalModel,
+                selectOptionCompare: (left, right) => left?.name == right?.name,
+                options: models.models
+                    .map(
+                      (model) => buildDropdownMenuEntry<AIModelPB>(
+                        context,
+                        value: model,
+                        label: model.i18n,
+                        subLabel: model.desc,
+                        maximumHeight: height,
+                      ),
+                    )
+                    .toList(),
+              ),
+            ),
+          ],
+        );
+      },
+    );
+  }
+}
diff --git a/frontend/resources/translations/en.json b/frontend/resources/translations/en.json
index 91abe94cc5806..27940824a90d0 100644
--- a/frontend/resources/translations/en.json
+++ b/frontend/resources/translations/en.json
@@ -866,6 +866,7 @@
         "aiSettingsDescription": "Choose your preferred model to power AppFlowy AI. Now includes GPT-4o, GPT-o3-mini, DeepSeek R1, Claude 3.5 Sonnet, and models available in Ollama",
         "loginToEnableAIFeature": "AI features are only enabled after logging in with @:appName Cloud. If you don't have an @:appName account, go to 'My Account' to sign up",
         "llmModel": "Language Model",
+        "globalLLMModel": "Global Language Model",
         "llmModelType": "Language Model Type",
         "downloadLLMPrompt": "Download {}",
         "downloadAppFlowyOfflineAI": "Downloading AI offline package will enable AI to run on your device. Do you want to continue?",
@@ -3342,4 +3343,4 @@
     "rewrite": "Rewrite",
     "insertBelow": "Insert below"
   }
-}
+}
\ No newline at end of file
diff --git a/frontend/rust-lib/flowy-ai/src/ai_manager.rs b/frontend/rust-lib/flowy-ai/src/ai_manager.rs
index d18cb9135500c..bfddebcd8e85d 100644
--- a/frontend/rust-lib/flowy-ai/src/ai_manager.rs
+++ b/frontend/rust-lib/flowy-ai/src/ai_manager.rs
@@ -341,14 +341,14 @@ impl AIManager {
   }
 
   pub async fn update_local_ai_setting(&self, setting: LocalAISetting) -> FlowyResult<()> {
-    let previous_model = self.local_ai.get_local_ai_setting().chat_model_name;
+    let old_settings = self.local_ai.get_local_ai_setting();
+    let need_restart = old_settings.ollama_server_url != setting.ollama_server_url;
     self.local_ai.update_local_ai_setting(setting).await?;
     let current_model = self.local_ai.get_local_ai_setting().chat_model_name;
-
-    if previous_model != current_model {
+    if old_settings.chat_model_name != current_model {
       info!(
         "[AI Plugin] update global active model, previous: {}, current: {}",
-        previous_model, current_model
+        old_settings.chat_model_name, current_model
       );
       let model = AIModel::local(current_model, "".to_string());
       self
@@ -356,6 +356,9 @@ impl AIManager {
         .await?;
     }
 
+    if need_restart {
+      self.local_ai.restart_plugin().await;
+    }
     Ok(())
   }
 
@@ -446,7 +449,7 @@ impl AIManager {
       .store_preferences
       .set_object::<AIModel>(&source_key, &model)?;
 
-    chat_notification_builder(&source, ChatNotification::DidUpdateSelectedModel)
+    chat_notification_builder(&source_key, ChatNotification::DidUpdateSelectedModel)
       .payload(AIModelPB::from(model))
       .send();
     Ok(())
@@ -501,99 +504,109 @@ impl AIManager {
     }
   }
 
+  pub async fn get_local_available_models(&self) -> FlowyResult<AvailableModelsPB> {
+    let setting = self.local_ai.get_local_ai_setting();
+    let models = self.local_ai.get_all_chat_local_models().await;
+    let selected_model = AIModel::local(setting.chat_model_name, "".to_string());
+
+    Ok(AvailableModelsPB {
+      models: models.into_iter().map(AIModelPB::from).collect(),
+      global_model: AIModelPB::from(selected_model),
+    })
+  }
+
   pub async fn get_available_models(&self, source: String) -> FlowyResult<AvailableModelsPB> {
     let is_local_mode = self.user_service.is_local_model().await?;
     if is_local_mode {
+      return self.get_local_available_models().await;
+    }
+
+    // Fetch server models
+    let mut all_models: Vec<AIModel> = self
+      .get_server_available_models()
+      .await?
+      .into_iter()
+      .map(AIModel::from)
+      .collect();
+
+    trace!("[Model Selection]: Available models: {:?}", all_models);
+
+    // Add local models if enabled
+    if self.local_ai.is_enabled() {
       let setting = self.local_ai.get_local_ai_setting();
-      let models = self.local_ai.get_all_chat_local_models().await;
-      let selected_model = AIModel::local(setting.chat_model_name, "".to_string());
+      all_models.push(AIModel::local(setting.chat_model_name, "".to_string()).into());
+    }
 
-      Ok(AvailableModelsPB {
-        models: models.into_iter().map(|m| m.into()).collect(),
-        selected_model: AIModelPB::from(selected_model),
-      })
-    } else {
-      // Build the models list from server models and mark them as non-local.
-      let mut all_models: Vec<AIModel> = self
-        .get_server_available_models()
-        .await?
-        .into_iter()
-        .map(AIModel::from)
-        .collect();
-
-      trace!("[Model Selection]: Available models: {:?}", all_models);
-
-      // If user enable local ai, then add local ai model to the list.
-      if self.local_ai.is_enabled() {
-        let local_models = self.local_ai.get_all_chat_local_models().await;
-        all_models.extend(local_models.into_iter().map(|m| m));
-      }
+    // Return early if no models available
+    if all_models.is_empty() {
+      return Ok(AvailableModelsPB {
+        models: Vec::new(),
+        global_model: AIModelPB::default(),
+      });
+    }
 
-      if all_models.is_empty() {
-        return Ok(AvailableModelsPB {
-          models: all_models.into_iter().map(|m| m.into()).collect(),
-          selected_model: AIModelPB::default(),
-        });
-      }
+    // Get server active model (only once)
+    let server_active_model = self
+      .get_workspace_select_model()
+      .await
+      .map(|m| AIModel::server(m, "".to_string()))
+      .unwrap_or_else(|_| AIModel::default());
 
-      // Global active model is the model selected by the user in the workspace settings.
-      let mut server_active_model = self
-        .get_workspace_select_model()
-        .await
-        .map(|m| AIModel::server(m, "".to_string()))
-        .unwrap_or_else(|_| AIModel::default());
+    trace!(
+      "[Model Selection] server active model: {:?}",
+      server_active_model
+    );
 
-      trace!(
-        "[Model Selection] server active model: {:?}",
-        server_active_model
-      );
+    // Use server model as default if it exists in available models
+    let default_model = if all_models
+      .iter()
+      .any(|m| m.name == server_active_model.name)
+    {
+      server_active_model.clone()
+    } else {
+      AIModel::default()
+    };
 
-      let mut user_selected_model = server_active_model.clone();
-      // when current select model is deprecated, reset the model to default
-      if !all_models
-        .iter()
-        .any(|m| m.name == server_active_model.name)
-      {
-        server_active_model = AIModel::default();
-      }
+    // Get user's previously selected model
+    let user_selected_model = match self.get_active_model(&source).await {
+      Some(model) => {
+        trace!("[Model Selection] user previous select model: {:?}", model);
+        model
+      },
+      None => {
+        // When no selected model and local AI is active, use local AI model
+        all_models
+          .iter()
+          .find(|m| m.is_local)
+          .cloned()
+          .unwrap_or_else(|| default_model.clone())
+      },
+    };
 
-      // We use source to identify user selected model. source can be document id or chat id.
-      match self.get_active_model(&source).await {
-        None => {
-          // when there is selected model and current local ai is active, then use local ai
-          if let Some(local_ai_model) = all_models.iter().find(|m| m.is_local) {
-            user_selected_model = local_ai_model.clone();
-          }
-        },
-        Some(model) => {
-          trace!("[Model Selection] user previous select model: {:?}", model);
-          user_selected_model = model;
-        },
+    // Determine final active model - use user's selection if available, otherwise default
+    let active_model = all_models
+      .iter()
+      .find(|m| m.name == user_selected_model.name)
+      .cloned()
+      .unwrap_or(default_model.clone());
+
+    // Update stored preference if changed
+    if active_model.name != user_selected_model.name {
+      if let Err(err) = self
+        .update_selected_model(source, active_model.clone())
+        .await
+      {
+        error!("[Model Selection] failed to update selected model: {}", err);
       }
+    }
 
-      // If user selected model is not available in the list, use the global active model.
-      let active_model = all_models
-        .iter()
-        .find(|m| m.name == user_selected_model.name)
-        .cloned()
-        .or(Some(server_active_model.clone()));
-
-      // Update the stored preference if a different model is used.
-      if let Some(ref active_model) = active_model {
-        if active_model.name != user_selected_model.name {
-          self
-            .update_selected_model(source, active_model.clone())
-            .await?;
-        }
-      }
+    trace!("[Model Selection] final active model: {:?}", active_model);
 
-      trace!("[Model Selection] final active model: {:?}", active_model);
-      let selected_model = AIModelPB::from(active_model.unwrap_or_default());
-      Ok(AvailableModelsPB {
-        models: all_models.into_iter().map(|m| m.into()).collect(),
-        selected_model,
-      })
-    }
+    // Create response with one transformation pass
+    Ok(AvailableModelsPB {
+      models: all_models.into_iter().map(AIModelPB::from).collect(),
+      global_model: AIModelPB::from(active_model),
+    })
   }
 
   pub async fn get_or_create_chat_instance(&self, chat_id: &Uuid) -> Result<Arc<Chat>, FlowyError> {
diff --git a/frontend/rust-lib/flowy-ai/src/entities.rs b/frontend/rust-lib/flowy-ai/src/entities.rs
index 796664a18f168..5e03fadf56ec0 100644
--- a/frontend/rust-lib/flowy-ai/src/entities.rs
+++ b/frontend/rust-lib/flowy-ai/src/entities.rs
@@ -222,7 +222,13 @@ pub struct AvailableModelsPB {
   pub models: Vec<AIModelPB>,
 
   #[pb(index = 2)]
-  pub selected_model: AIModelPB,
+  pub global_model: AIModelPB,
+}
+
+#[derive(Default, ProtoBuf, Clone, Debug)]
+pub struct RepeatedAIModelPB {
+  #[pb(index = 1)]
+  pub items: Vec<AIModelPB>,
 }
 
 #[derive(Default, ProtoBuf, Clone, Debug)]
@@ -686,7 +692,7 @@ pub struct LocalAISettingPB {
 
   #[pb(index = 2)]
   #[validate(custom(function = "required_not_empty_str"))]
-  pub default_model: String,
+  pub global_chat_model: String,
 
   #[pb(index = 3)]
   #[validate(custom(function = "required_not_empty_str"))]
@@ -697,7 +703,7 @@ impl From<LocalAISetting> for LocalAISettingPB {
   fn from(value: LocalAISetting) -> Self {
     LocalAISettingPB {
       server_url: value.ollama_server_url,
-      default_model: value.chat_model_name,
+      global_chat_model: value.chat_model_name,
       embedding_model_name: value.embedding_model_name,
     }
   }
@@ -707,7 +713,7 @@ impl From<LocalAISettingPB> for LocalAISetting {
   fn from(value: LocalAISettingPB) -> Self {
     LocalAISetting {
       ollama_server_url: value.server_url,
-      chat_model_name: value.default_model,
+      chat_model_name: value.global_chat_model,
       embedding_model_name: value.embedding_model_name,
     }
   }
diff --git a/frontend/rust-lib/flowy-ai/src/event_handler.rs b/frontend/rust-lib/flowy-ai/src/event_handler.rs
index f778063309ac2..160fbe6928bbd 100644
--- a/frontend/rust-lib/flowy-ai/src/event_handler.rs
+++ b/frontend/rust-lib/flowy-ai/src/event_handler.rs
@@ -340,6 +340,15 @@ pub(crate) async fn get_local_ai_setting_handler(
   data_result_ok(pb)
 }
 
+#[tracing::instrument(level = "debug", skip_all)]
+pub(crate) async fn get_local_ai_models_handler(
+  ai_manager: AFPluginState<Weak<AIManager>>,
+) -> DataResult<AvailableModelsPB, FlowyError> {
+  let ai_manager = upgrade_ai_manager(ai_manager)?;
+  let data = ai_manager.get_local_available_models().await?;
+  data_result_ok(data)
+}
+
 #[tracing::instrument(level = "debug", skip_all, err)]
 pub(crate) async fn update_local_ai_setting_handler(
   ai_manager: AFPluginState<Weak<AIManager>>,
diff --git a/frontend/rust-lib/flowy-ai/src/event_map.rs b/frontend/rust-lib/flowy-ai/src/event_map.rs
index 5020836a30756..ee77f454a543b 100644
--- a/frontend/rust-lib/flowy-ai/src/event_map.rs
+++ b/frontend/rust-lib/flowy-ai/src/event_map.rs
@@ -31,6 +31,7 @@ pub fn init(ai_manager: Weak<AIManager>) -> AFPlugin {
     .event(AIEvent::ToggleLocalAI, toggle_local_ai_handler)
     .event(AIEvent::GetLocalAIState, get_local_ai_state_handler)
     .event(AIEvent::GetLocalAISetting, get_local_ai_setting_handler)
+    .event(AIEvent::GetLocalAIModels, get_local_ai_models_handler)
     .event(
       AIEvent::UpdateLocalAISetting,
       update_local_ai_setting_handler,
@@ -121,4 +122,7 @@ pub enum AIEvent {
 
   #[event(input = "UpdateSelectedModelPB")]
   UpdateSelectedModel = 32,
+
+  #[event(output = "AvailableModelsPB")]
+  GetLocalAIModels = 33,
 }
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/controller.rs b/frontend/rust-lib/flowy-ai/src/local_ai/controller.rs
index d384ddfb7529a..6b147b9c0c901 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/controller.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/controller.rs
@@ -384,11 +384,7 @@ impl LocalAIController {
       setting,
       std::thread::current().id()
     );
-
-    if self.resource.set_llm_setting(setting).await.is_ok() {
-      let is_enabled = self.is_enabled();
-      self.toggle_plugin(is_enabled).await?;
-    }
+    self.resource.set_llm_setting(setting).await?;
     Ok(())
   }
 
