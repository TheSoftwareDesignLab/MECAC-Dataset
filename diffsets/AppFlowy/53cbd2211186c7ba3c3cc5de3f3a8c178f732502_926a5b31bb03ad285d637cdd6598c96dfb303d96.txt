diff --git a/.github/workflows/flutter_ci.yaml b/.github/workflows/flutter_ci.yaml
index 07cfd9933def0..dc54aa089f8e3 100644
--- a/.github/workflows/flutter_ci.yaml
+++ b/.github/workflows/flutter_ci.yaml
@@ -28,7 +28,7 @@ env:
   FLUTTER_VERSION: "3.27.4"
   RUST_TOOLCHAIN: "1.84.0"
   CARGO_MAKE_VERSION: "0.37.18"
-  CLOUD_VERSION: 0.9.37-amd64
+  CLOUD_VERSION: 0.9.49-amd64
 
 concurrency:
   group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
diff --git a/.github/workflows/rust_ci.yaml b/.github/workflows/rust_ci.yaml
index 3ab5f91ba418e..6939981bd8500 100644
--- a/.github/workflows/rust_ci.yaml
+++ b/.github/workflows/rust_ci.yaml
@@ -18,7 +18,7 @@ on:
 
 env:
   CARGO_TERM_COLOR: always
-  CLOUD_VERSION: 0.8.3-amd64
+  CLOUD_VERSION: 0.9.49-amd64
   RUST_TOOLCHAIN: "1.84.0"
 
 jobs:
diff --git a/frontend/appflowy_flutter/lib/ai/widgets/prompt_input/desktop_prompt_input.dart b/frontend/appflowy_flutter/lib/ai/widgets/prompt_input/desktop_prompt_input.dart
index a45d6568c6894..288110c5f2dc4 100644
--- a/frontend/appflowy_flutter/lib/ai/widgets/prompt_input/desktop_prompt_input.dart
+++ b/frontend/appflowy_flutter/lib/ai/widgets/prompt_input/desktop_prompt_input.dart
@@ -634,7 +634,7 @@ class _PromptBottomActions extends StatelessWidget {
 
               const Spacer(),
 
-              if (state.modelState.type == AiType.cloud) _selectSourcesButton(),
+              _selectSourcesButton(),
               if (extraBottomActionButton != null) extraBottomActionButton!,
               // _mentionButton(context),
               if (state.supportChatWithFile) _attachmentButton(context),
diff --git a/frontend/appflowy_flutter/lib/workspace/presentation/home/menu/view/view_item.dart b/frontend/appflowy_flutter/lib/workspace/presentation/home/menu/view/view_item.dart
index 22182f742919c..cc70c922418bf 100644
--- a/frontend/appflowy_flutter/lib/workspace/presentation/home/menu/view/view_item.dart
+++ b/frontend/appflowy_flutter/lib/workspace/presentation/home/menu/view/view_item.dart
@@ -720,7 +720,8 @@ class _SingleInnerViewItemState extends State<SingleInnerViewItem> {
     final viewBloc = context.read<ViewBloc>();
 
     // the name of new document should be empty
-    final viewName = pluginBuilder.layoutType != ViewLayoutPB.Document
+    final viewName = ![ViewLayoutPB.Document, ViewLayoutPB.Chat]
+            .contains(pluginBuilder.layoutType)
         ? LocaleKeys.menuAppHeader_defaultNewPageName.tr()
         : '';
     viewBloc.add(
diff --git a/frontend/rust-lib/Cargo.lock b/frontend/rust-lib/Cargo.lock
index 1ec2d90640d46..11876b9076229 100644
--- a/frontend/rust-lib/Cargo.lock
+++ b/frontend/rust-lib/Cargo.lock
@@ -493,7 +493,7 @@ checksum = "dcfed56ad506cb2c684a14971b8861fdc3baaaae314b9e5f9bb532cbe3ba7a4f"
 [[package]]
 name = "app-error"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "anyhow",
  "bincode",
@@ -513,7 +513,7 @@ dependencies = [
 [[package]]
 name = "appflowy-ai-client"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "anyhow",
  "bytes",
@@ -1233,7 +1233,7 @@ dependencies = [
 [[package]]
 name = "client-api"
 version = "0.2.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "again",
  "anyhow",
@@ -1288,7 +1288,7 @@ dependencies = [
 [[package]]
 name = "client-api-entity"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "collab-entity",
  "collab-rt-entity",
@@ -1301,7 +1301,7 @@ dependencies = [
 [[package]]
 name = "client-websocket"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "futures-channel",
  "futures-util",
@@ -1572,7 +1572,7 @@ dependencies = [
 [[package]]
 name = "collab-rt-entity"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "anyhow",
  "bincode",
@@ -1594,7 +1594,7 @@ dependencies = [
 [[package]]
 name = "collab-rt-protocol"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "anyhow",
  "async-trait",
@@ -2073,7 +2073,7 @@ checksum = "c2e66c9d817f1720209181c316d28635c050fa304f9c79e47a520882661b7308"
 [[package]]
 name = "database-entity"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "bincode",
  "bytes",
@@ -2722,6 +2722,7 @@ dependencies = [
  "lib-infra",
  "serde",
  "serde_json",
+ "tracing",
  "twox-hash",
  "uuid",
 ]
@@ -3643,7 +3644,7 @@ dependencies = [
 [[package]]
 name = "gotrue"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "anyhow",
  "getrandom 0.2.10",
@@ -3658,7 +3659,7 @@ dependencies = [
 [[package]]
 name = "gotrue-entity"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "app-error",
  "jsonwebtoken",
@@ -4371,7 +4372,7 @@ dependencies = [
 [[package]]
 name = "infra"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "anyhow",
  "bytes",
@@ -7514,7 +7515,7 @@ dependencies = [
 [[package]]
 name = "shared-entity"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "anyhow",
  "app-error",
@@ -9660,7 +9661,7 @@ dependencies = [
 [[package]]
 name = "workspace-template"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=667bb51f57f63ec02137053be71e77236f56faa7#667bb51f57f63ec02137053be71e77236f56faa7"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
 dependencies = [
  "anyhow",
  "async-trait",
diff --git a/frontend/rust-lib/Cargo.toml b/frontend/rust-lib/Cargo.toml
index 23f6c7c8bcbb6..88270beab7027 100644
--- a/frontend/rust-lib/Cargo.toml
+++ b/frontend/rust-lib/Cargo.toml
@@ -111,9 +111,9 @@ flowy-sqlite-vec = { path = "flowy-sqlite-vec" }
 # Run the script.add_workspace_members:
 # scripts/tool/update_client_api_rev.sh  new_rev_id
 # ⚠️⚠️⚠️️
-client-api = { git = "https://github.com/AppFlowy-IO/AppFlowy-Cloud", rev = "667bb51f57f63ec02137053be71e77236f56faa7" }
-client-api-entity = { git = "https://github.com/AppFlowy-IO/AppFlowy-Cloud", rev = "667bb51f57f63ec02137053be71e77236f56faa7" }
-workspace-template = { git = "https://github.com/AppFlowy-IO/AppFlowy-Cloud", rev = "667bb51f57f63ec02137053be71e77236f56faa7" }
+client-api = { git = "https://github.com/AppFlowy-IO/AppFlowy-Cloud", rev = "9e16ff714903823ed68a001e78607da5c7f5c93a" }
+client-api-entity = { git = "https://github.com/AppFlowy-IO/AppFlowy-Cloud", rev = "9e16ff714903823ed68a001e78607da5c7f5c93a" }
+workspace-template = { git = "https://github.com/AppFlowy-IO/AppFlowy-Cloud", rev = "9e16ff714903823ed68a001e78607da5c7f5c93a" }
 
 [profile.dev]
 opt-level = 0
diff --git a/frontend/rust-lib/collab-integrate/src/collab_builder.rs b/frontend/rust-lib/collab-integrate/src/collab_builder.rs
index dda510a17b06a..3472a70f9d870 100644
--- a/frontend/rust-lib/collab-integrate/src/collab_builder.rs
+++ b/frontend/rust-lib/collab-integrate/src/collab_builder.rs
@@ -33,7 +33,7 @@ use collab_plugins::local_storage::kv::KVTransactionDB;
 use collab_plugins::local_storage::CollabPersistenceConfig;
 use collab_user::core::{UserAwareness, UserAwarenessNotifier};
 
-use crate::instant_indexed_data_provider::InstantIndexedDataProvider;
+use crate::instant_indexed_data_provider::InstantIndexedDataWriter;
 use flowy_error::FlowyError;
 use lib_infra::util::get_operating_system;
 use lib_infra::{if_native, if_wasm};
@@ -81,14 +81,14 @@ pub struct AppFlowyCollabBuilder {
   #[cfg(not(target_arch = "wasm32"))]
   rocksdb_backup: ArcSwapOption<Arc<dyn RocksdbBackup>>,
   workspace_integrate: Arc<dyn WorkspaceCollabIntegrate>,
-  embeddings_writer: Option<Weak<InstantIndexedDataProvider>>,
+  embeddings_writer: Option<Weak<InstantIndexedDataWriter>>,
 }
 
 impl AppFlowyCollabBuilder {
   pub fn new(
     storage_provider: impl CollabCloudPluginProvider + 'static,
     workspace_integrate: impl WorkspaceCollabIntegrate + 'static,
-    embeddings_writer: Option<Weak<InstantIndexedDataProvider>>,
+    embeddings_writer: Option<Weak<InstantIndexedDataWriter>>,
   ) -> Self {
     Self {
       embeddings_writer,
diff --git a/frontend/rust-lib/collab-integrate/src/instant_indexed_data_provider.rs b/frontend/rust-lib/collab-integrate/src/instant_indexed_data_provider.rs
index 9147058fa438e..f2c1f669361d1 100644
--- a/frontend/rust-lib/collab-integrate/src/instant_indexed_data_provider.rs
+++ b/frontend/rust-lib/collab-integrate/src/instant_indexed_data_provider.rs
@@ -1,8 +1,11 @@
+use collab::core::collab::DataSource;
+use collab::core::origin::CollabOrigin;
+use collab::entity::EncodedCollab;
 use collab::lock::RwLock;
 use collab::preclude::{Collab, Transact};
 use collab_document::document::DocumentBody;
 use collab_entity::{CollabObject, CollabType};
-use flowy_ai_pub::entities::UnindexedData;
+use flowy_ai_pub::entities::{UnindexedCollab, UnindexedCollabMetadata, UnindexedData};
 use flowy_error::{FlowyError, FlowyResult};
 use lib_infra::async_trait::async_trait;
 use lib_infra::util::get_operating_system;
@@ -20,25 +23,25 @@ pub struct WriteObject {
   pub collab: Weak<dyn CollabIndexedData>,
 }
 
-pub struct InstantIndexedDataProvider {
+pub struct InstantIndexedDataWriter {
   collab_by_object: Arc<RwLock<HashMap<String, WriteObject>>>,
   consumers: Arc<RwLock<Vec<Box<dyn InstantIndexedDataConsumer>>>>,
 }
 
-impl Default for InstantIndexedDataProvider {
+impl Default for InstantIndexedDataWriter {
   fn default() -> Self {
     Self::new()
   }
 }
 
-impl InstantIndexedDataProvider {
-  pub fn new() -> InstantIndexedDataProvider {
+impl InstantIndexedDataWriter {
+  pub fn new() -> InstantIndexedDataWriter {
     let collab_by_object = Arc::new(RwLock::new(HashMap::<String, WriteObject>::new()));
     let consumers = Arc::new(RwLock::new(
       Vec::<Box<dyn InstantIndexedDataConsumer>>::new(),
     ));
 
-    InstantIndexedDataProvider {
+    InstantIndexedDataWriter {
       collab_by_object,
       consumers,
     }
@@ -109,8 +112,30 @@ impl InstantIndexedDataProvider {
                   // Snapshot consumers
                   let consumers_guard = consumers.read().await;
                   for consumer in consumers_guard.iter() {
+                    let workspace_id = match Uuid::parse_str(&wo.collab_object.workspace_id) {
+                      Ok(id) => id,
+                      Err(err) => {
+                        error!(
+                          "Invalid workspace_id {}: {}",
+                          wo.collab_object.workspace_id, err
+                        );
+                        continue;
+                      },
+                    };
+                    let object_id = match Uuid::parse_str(&wo.collab_object.object_id) {
+                      Ok(id) => id,
+                      Err(err) => {
+                        error!("Invalid object_id {}: {}", wo.collab_object.object_id, err);
+                        continue;
+                      },
+                    };
                     match consumer
-                      .consume_collab(&wo.collab_object, data.clone())
+                      .consume_collab(
+                        &workspace_id,
+                        data.clone(),
+                        &object_id,
+                        wo.collab_object.collab_type,
+                      )
                       .await
                     {
                       Ok(is_indexed) => {
@@ -155,6 +180,56 @@ impl InstantIndexedDataProvider {
     matches!(t, CollabType::Document)
   }
 
+  pub async fn index_encoded_collab(
+    &self,
+    workspace_id: Uuid,
+    object_id: Uuid,
+    data: EncodedCollab,
+    collab_type: CollabType,
+  ) -> FlowyResult<()> {
+    match unindexed_collab_from_encoded_collab(workspace_id, object_id, data, collab_type) {
+      None => Err(FlowyError::internal().with_context("Failed to create unindexed collab")),
+      Some(data) => {
+        self.index_unindexed_collab(data).await?;
+        Ok(())
+      },
+    }
+  }
+
+  pub async fn index_unindexed_collab(&self, data: UnindexedCollab) -> FlowyResult<()> {
+    let consumers_guard = self.consumers.read().await;
+    for consumer in consumers_guard.iter() {
+      match consumer
+        .consume_collab(
+          &data.workspace_id,
+          data.data.clone(),
+          &data.object_id,
+          data.collab_type,
+        )
+        .await
+      {
+        Ok(is_indexed) => {
+          if is_indexed {
+            trace!(
+              "[Indexing] {} consumed {}",
+              consumer.consumer_id(),
+              data.object_id
+            );
+          }
+        },
+        Err(err) => {
+          error!(
+            "Consumer {} failed on {}: {}",
+            consumer.consumer_id(),
+            data.object_id,
+            err
+          );
+        },
+      }
+    }
+    Ok(())
+  }
+
   pub async fn queue_collab_embed(
     &self,
     collab_object: CollabObject,
@@ -179,7 +254,10 @@ impl InstantIndexedDataProvider {
   }
 }
 
-fn index_data_for_collab(collab: &Collab, collab_type: &CollabType) -> Option<UnindexedData> {
+pub fn unindexed_data_form_collab(
+  collab: &Collab,
+  collab_type: &CollabType,
+) -> Option<UnindexedData> {
   match collab_type {
     CollabType::Document => {
       let txn = collab.doc().try_transact().ok()?;
@@ -191,6 +269,35 @@ fn index_data_for_collab(collab: &Collab, collab_type: &CollabType) -> Option<Un
   }
 }
 
+pub fn unindexed_collab_from_encoded_collab(
+  workspace_id: Uuid,
+  object_id: Uuid,
+  encoded_collab: EncodedCollab,
+  collab_type: CollabType,
+) -> Option<UnindexedCollab> {
+  match collab_type {
+    CollabType::Document => {
+      let collab = Collab::new_with_source(
+        CollabOrigin::Empty,
+        &object_id.to_string(),
+        DataSource::DocStateV1(encoded_collab.doc_state.to_vec()),
+        vec![],
+        false,
+      )
+      .ok()?;
+      let data = unindexed_data_form_collab(&collab, &collab_type)?;
+      Some(UnindexedCollab {
+        workspace_id,
+        object_id,
+        collab_type,
+        data,
+        metadata: UnindexedCollabMetadata::default(), // default means do not update metadata
+      })
+    },
+    _ => None,
+  }
+}
+
 #[async_trait]
 pub trait CollabIndexedData: Send + Sync + 'static {
   async fn get_unindexed_data(&self, collab_type: &CollabType) -> Option<UnindexedData>;
@@ -203,7 +310,7 @@ where
 {
   async fn get_unindexed_data(&self, collab_type: &CollabType) -> Option<UnindexedData> {
     let collab = self.try_read().ok()?;
-    index_data_for_collab(collab.borrow(), collab_type)
+    unindexed_data_form_collab(collab.borrow(), collab_type)
   }
 }
 
@@ -214,8 +321,10 @@ pub trait InstantIndexedDataConsumer: Send + Sync + 'static {
 
   async fn consume_collab(
     &self,
-    collab_object: &CollabObject,
+    workspace_id: &Uuid,
     data: UnindexedData,
+    object_id: &Uuid,
+    collab_type: CollabType,
   ) -> Result<bool, FlowyError>;
 
   async fn did_delete_collab(
diff --git a/frontend/rust-lib/event-integration-test/src/chat_event.rs b/frontend/rust-lib/event-integration-test/src/chat_event.rs
index c8c638bc858a3..8336dfd476455 100644
--- a/frontend/rust-lib/event-integration-test/src/chat_event.rs
+++ b/frontend/rust-lib/event-integration-test/src/chat_event.rs
@@ -87,4 +87,11 @@ impl EventIntegrationTest {
       .await
       .parse::<ChatMessageListPB>()
   }
+
+  pub async fn toggle_local_ai(&self) {
+    EventBuilder::new(self.clone())
+      .event(AIEvent::ToggleLocalAI)
+      .async_send()
+      .await;
+  }
 }
diff --git a/frontend/rust-lib/event-integration-test/tests/chat/local_chat_test.rs b/frontend/rust-lib/event-integration-test/tests/chat/local_chat_test.rs
new file mode 100644
index 0000000000000..8b137891791fe
--- /dev/null
+++ b/frontend/rust-lib/event-integration-test/tests/chat/local_chat_test.rs
@@ -0,0 +1 @@
+
diff --git a/frontend/rust-lib/event-integration-test/tests/chat/mod.rs b/frontend/rust-lib/event-integration-test/tests/chat/mod.rs
index 773bdab81fef0..7203432c70265 100644
--- a/frontend/rust-lib/event-integration-test/tests/chat/mod.rs
+++ b/frontend/rust-lib/event-integration-test/tests/chat/mod.rs
@@ -1 +1,2 @@
 mod chat_message_test;
+mod local_chat_test;
diff --git a/frontend/rust-lib/flowy-ai-pub/Cargo.toml b/frontend/rust-lib/flowy-ai-pub/Cargo.toml
index f463e76fd0956..016a7910b663f 100644
--- a/frontend/rust-lib/flowy-ai-pub/Cargo.toml
+++ b/frontend/rust-lib/flowy-ai-pub/Cargo.toml
@@ -16,4 +16,4 @@ uuid.workspace = true
 flowy-sqlite = { workspace = true }
 chrono.workspace = true
 twox-hash = { version = "2.1.0", features = ["xxhash64"] }
-
+tracing.workspace = true
diff --git a/frontend/rust-lib/flowy-ai-pub/src/cloud.rs b/frontend/rust-lib/flowy-ai-pub/src/cloud.rs
index 63d2fd2b5471e..19b185c9b04b3 100644
--- a/frontend/rust-lib/flowy-ai-pub/src/cloud.rs
+++ b/frontend/rust-lib/flowy-ai-pub/src/cloud.rs
@@ -123,7 +123,7 @@ pub trait ChatCloudService: Send + Sync + 'static {
     chat_id: &Uuid,
     question_id: i64,
     format: ResponseFormat,
-    ai_model: Option<AIModel>,
+    ai_model: AIModel,
   ) -> Result<StreamAnswer, FlowyError>;
 
   async fn get_answer(
diff --git a/frontend/rust-lib/flowy-ai-pub/src/entities.rs b/frontend/rust-lib/flowy-ai-pub/src/entities.rs
index 842cb684229cd..0cd20bdd58571 100644
--- a/frontend/rust-lib/flowy-ai-pub/src/entities.rs
+++ b/frontend/rust-lib/flowy-ai-pub/src/entities.rs
@@ -2,18 +2,19 @@ use crate::cloud::workspace_dto::ViewIcon;
 use crate::cloud::CollabType;
 use twox_hash::xxhash64::Hasher;
 use uuid::Uuid;
+pub const RAG_IDS: &str = "rag_ids";
 pub const SOURCE_ID: &str = "id";
-pub const SOURCE: &str = "appflowy";
-pub const SOURCE_NAME: &str = "document";
+pub const SOURCE: &str = "source";
+pub const SOURCE_NAME: &str = "name";
 pub struct EmbeddingRecord {
   pub workspace_id: Uuid,
   pub object_id: Uuid,
   pub chunks: Vec<EmbeddedChunk>,
 }
 
-#[derive(Debug, Clone)]
+#[derive(Debug, Clone, Default)]
 pub struct UnindexedCollabMetadata {
-  pub name: String,
+  pub name: Option<String>,
   pub icon: Option<ViewIcon>,
 }
 
diff --git a/frontend/rust-lib/flowy-ai-pub/src/persistence/chat_message_sql.rs b/frontend/rust-lib/flowy-ai-pub/src/persistence/chat_message_sql.rs
index 110659fa001a5..0abac81a797f5 100644
--- a/frontend/rust-lib/flowy-ai-pub/src/persistence/chat_message_sql.rs
+++ b/frontend/rust-lib/flowy-ai-pub/src/persistence/chat_message_sql.rs
@@ -11,7 +11,7 @@ use flowy_sqlite::{
   Queryable,
 };
 
-#[derive(Queryable, Insertable, Identifiable)]
+#[derive(Queryable, Insertable, Identifiable, Debug)]
 #[diesel(table_name = chat_message_table)]
 #[diesel(primary_key(message_id))]
 pub struct ChatMessageTable {
@@ -73,6 +73,7 @@ pub fn upsert_chat_messages(
   mut conn: DBConnection,
   new_messages: &[ChatMessageTable],
 ) -> FlowyResult<()> {
+  //trace!("Upserting chat messages: {:?}", new_messages);
   conn.immediate_transaction(|conn| {
     for message in new_messages {
       let _ = insert_into(chat_message_table::table)
diff --git a/frontend/rust-lib/flowy-ai/src/ai_manager.rs b/frontend/rust-lib/flowy-ai/src/ai_manager.rs
index ae6a552f7147a..6c391ff5edf24 100644
--- a/frontend/rust-lib/flowy-ai/src/ai_manager.rs
+++ b/frontend/rust-lib/flowy-ai/src/ai_manager.rs
@@ -207,13 +207,20 @@ impl AIManager {
       ))
     });
 
-    if self.local_ai.is_ready().await {
+    if self.local_ai.is_enabled() {
       let workspace_id = self.user_service.workspace_id()?;
       let model = self.get_active_model(&chat_id.to_string()).await;
       trace!("[AI Plugin] notify open chat: {}", chat_id);
+      let mut rag_ids = vec![];
+      if let Some(settings) = self
+        .store_preferences
+        .get_object::<ChatSettings>(&setting_store_key(chat_id))
+      {
+        rag_ids = settings.rag_ids;
+      }
       self
         .local_ai
-        .open_chat(&workspace_id, chat_id, &model.name)
+        .open_chat(&workspace_id, chat_id, &model.name, rag_ids)
         .await?;
     }
 
@@ -221,6 +228,7 @@ impl AIManager {
     let cloud_service_wm = self.cloud_service_wm.clone();
     let store_preferences = self.store_preferences.clone();
     let external_service = self.external_service.clone();
+    let local_ai = self.local_ai.clone();
     let chat_id = *chat_id;
     tokio::spawn(async move {
       match refresh_chat_setting(
@@ -232,6 +240,7 @@ impl AIManager {
       .await
       {
         Ok(settings) => {
+          local_ai.set_rag_ids(&chat_id, &settings.rag_ids).await;
           let rag_ids = settings
             .rag_ids
             .into_iter()
@@ -316,7 +325,7 @@ impl AIManager {
   ) -> Result<ChatMessagePB, FlowyError> {
     let chat = self.get_or_create_chat_instance(&params.chat_id).await?;
     let ai_model = self.get_active_model(&params.chat_id.to_string()).await;
-    let question = chat.stream_chat_message(&params, Some(ai_model)).await?;
+    let question = chat.stream_chat_message(&params, ai_model).await?;
     let _ = self
       .external_service
       .notify_did_send_message(&params.chat_id, &params.message)
@@ -342,7 +351,7 @@ impl AIManager {
       Some(model) => model.into(),
     };
     chat
-      .stream_regenerate_response(question_message_id, answer_stream_port, format, Some(model))
+      .stream_regenerate_response(question_message_id, answer_stream_port, format, model)
       .await?;
     Ok(())
   }
@@ -481,8 +490,10 @@ impl AIManager {
     }
   }
 
-  pub async fn get_local_available_models(&self) -> FlowyResult<ModelSelectionPB> {
-    let setting = self.local_ai.get_local_ai_setting();
+  pub async fn get_local_available_models(
+    &self,
+    source: Option<String>,
+  ) -> FlowyResult<ModelSelectionPB> {
     let workspace_id = self.user_service.workspace_id()?;
     let mut models = self
       .model_control
@@ -491,10 +502,25 @@ impl AIManager {
       .get_local_models(&workspace_id)
       .await;
 
-    let selected_model = AIModel::local(setting.chat_model_name, "".to_string());
-    if models.is_empty() {
-      models.push(selected_model.clone());
-    }
+    let selected_model = match source {
+      None => {
+        let setting = self.local_ai.get_local_ai_setting();
+        let selected_model = AIModel::local(setting.chat_model_name, "".to_string());
+        if models.is_empty() {
+          models.push(selected_model.clone());
+        }
+        selected_model
+      },
+      Some(source) => {
+        let source_key = SourceKey::new(source);
+        self
+          .model_control
+          .lock()
+          .await
+          .get_active_model(&workspace_id, &source_key)
+          .await
+      },
+    };
 
     Ok(ModelSelectionPB {
       models: models.into_iter().map(AIModelPB::from).collect(),
@@ -509,7 +535,7 @@ impl AIManager {
   ) -> FlowyResult<ModelSelectionPB> {
     let is_local_mode = self.user_service.is_local_model().await?;
     if is_local_mode {
-      return self.get_local_available_models().await;
+      return self.get_local_available_models(Some(source)).await;
     }
 
     let workspace_id = self.user_service.workspace_id()?;
@@ -524,6 +550,14 @@ impl AIManager {
     let active_model = model_control
       .get_active_model(&workspace_id, &source_key)
       .await;
+
+    trace!(
+      "[Model Selection] {} active model: {:?}, global model:{:?}",
+      source_key.storage_id(),
+      active_model,
+      local_model_name
+    );
+
     let all_models = model_control
       .get_models_with_specific_local_model(&workspace_id, local_model_name)
       .await;
@@ -674,6 +708,8 @@ impl AIManager {
 
     let user_service = self.user_service.clone();
     let external_service = self.external_service.clone();
+    self.local_ai.set_rag_ids(chat_id, &rag_ids).await;
+
     let rag_ids = rag_ids
       .into_iter()
       .flat_map(|r| Uuid::from_str(&r).ok())
@@ -705,7 +741,6 @@ async fn sync_chat_documents(
       {
         if let Ok(uid) = user_service.user_id() {
           if let Ok(conn) = user_service.sqlite_connection(uid) {
-            info!("sync rag documents success: {}", metadatas.len());
             batch_insert_collab_metadata(conn, &metadatas).unwrap();
           }
         }
diff --git a/frontend/rust-lib/flowy-ai/src/chat.rs b/frontend/rust-lib/flowy-ai/src/chat.rs
index e594c66750551..c81cdf09f515c 100644
--- a/frontend/rust-lib/flowy-ai/src/chat.rs
+++ b/frontend/rust-lib/flowy-ai/src/chat.rs
@@ -73,7 +73,7 @@ impl Chat {
   pub async fn stream_chat_message(
     &self,
     params: &StreamMessageParams,
-    preferred_ai_model: Option<AIModel>,
+    preferred_ai_model: AIModel,
   ) -> Result<ChatMessagePB, FlowyError> {
     trace!(
       "[Chat] stream chat message: chat_id={}, message={}, message_type={:?}, format={:?}",
@@ -138,7 +138,7 @@ impl Chat {
     question_id: i64,
     answer_stream_port: i64,
     format: Option<PredefinedFormatPB>,
-    ai_model: Option<AIModel>,
+    ai_model: AIModel,
   ) -> FlowyResult<()> {
     trace!(
       "[Chat] regenerate and stream chat message: chat_id={}",
@@ -178,7 +178,7 @@ impl Chat {
     workspace_id: Uuid,
     question_id: i64,
     format: ResponseFormat,
-    ai_model: Option<AIModel>,
+    ai_model: AIModel,
   ) {
     let stop_stream = self.stop_stream.clone();
     let chat_id = self.chat_id;
@@ -542,6 +542,12 @@ impl Chat {
     limit: u64,
     offset: MessageCursor,
   ) -> Result<Vec<ChatMessagePB>, FlowyError> {
+    trace!(
+      "[Chat] Loading messages from disk: chat_id={}, limit={}, offset={:?}",
+      self.chat_id,
+      limit,
+      offset
+    );
     let conn = self.user_service.sqlite_connection(self.uid)?;
     let rows = select_chat_messages(conn, &self.chat_id.to_string(), limit, offset)?.messages;
     let messages = rows
diff --git a/frontend/rust-lib/flowy-ai/src/embeddings/document_indexer.rs b/frontend/rust-lib/flowy-ai/src/embeddings/document_indexer.rs
index 8b122ce947155..10b8446f6abdf 100644
--- a/frontend/rust-lib/flowy-ai/src/embeddings/document_indexer.rs
+++ b/frontend/rust-lib/flowy-ai/src/embeddings/document_indexer.rs
@@ -111,7 +111,9 @@ pub fn split_text_into_chunks(
   let mut chunks = Vec::new();
 
   for (index, content) in split_contents.into_iter().enumerate() {
-    let consistent_hash = Hasher::oneshot(0, content.as_bytes());
+    let metadata_string = metadata.to_string();
+    let combined_data = format!("{}{}", content, metadata_string);
+    let consistent_hash = Hasher::oneshot(0, combined_data.as_bytes());
     let fragment_id = format!("{:x}", consistent_hash);
     if seen.insert(fragment_id.clone()) {
       chunks.push(EmbeddedChunk {
@@ -120,7 +122,7 @@ pub fn split_text_into_chunks(
         content_type: 0,
         content: Some(content),
         embeddings: None,
-        metadata: Some(metadata.to_string()),
+        metadata: Some(metadata_string),
         fragment_index: index as i32,
         embedder_type: 0,
       });
diff --git a/frontend/rust-lib/flowy-ai/src/embeddings/scheduler.rs b/frontend/rust-lib/flowy-ai/src/embeddings/scheduler.rs
index b350499b233cb..76f3df01f855c 100644
--- a/frontend/rust-lib/flowy-ai/src/embeddings/scheduler.rs
+++ b/frontend/rust-lib/flowy-ai/src/embeddings/scheduler.rs
@@ -110,7 +110,7 @@ impl EmbeddingScheduler {
       Some(query_embed) => {
         let result = self
           .vector_db
-          .search_with_score(&workspace_id.to_string(), vec![], query_embed, 10, 0.4)
+          .search_with_score(&workspace_id.to_string(), &[], query_embed, 10, 0.4)
           .await
           .map_err(|err| {
             error!("[Embedding] Failed to search: {}", err);
diff --git a/frontend/rust-lib/flowy-ai/src/embeddings/store.rs b/frontend/rust-lib/flowy-ai/src/embeddings/store.rs
index c01d5fac28186..f7afd249f5bf9 100644
--- a/frontend/rust-lib/flowy-ai/src/embeddings/store.rs
+++ b/frontend/rust-lib/flowy-ai/src/embeddings/store.rs
@@ -3,9 +3,10 @@ use crate::embeddings::embedder::{Embedder, OllamaEmbedder};
 use crate::embeddings::indexer::{EmbeddingModel, IndexerProvider};
 use async_trait::async_trait;
 use flowy_ai_pub::cloud::CollabType;
-use flowy_ai_pub::entities::SOURCE_ID;
-use flowy_error::FlowyError;
+use flowy_ai_pub::entities::{RAG_IDS, SOURCE_ID};
+use flowy_error::{FlowyError, FlowyResult};
 use flowy_sqlite_vec::db::VectorSqliteDB;
+use flowy_sqlite_vec::entities::SqliteEmbeddedDocument;
 use futures::stream::{self, StreamExt};
 use langchain_rust::llm::client::OllamaClient;
 use langchain_rust::{
@@ -17,7 +18,7 @@ use serde_json::Value;
 use std::collections::HashMap;
 use std::error::Error;
 use std::sync::{Arc, Weak};
-use tracing::{error, trace};
+use tracing::{error, trace, warn};
 use uuid::Uuid;
 
 #[derive(Clone)]
@@ -45,6 +46,25 @@ impl SqliteVectorStore {
     let embedder = Embedder::Ollama(OllamaEmbedder { ollama });
     Ok(embedder)
   }
+
+  pub(crate) async fn select_all_embedded_documents(
+    &self,
+    workspace_id: &str,
+    rag_ids: &[String],
+  ) -> FlowyResult<Vec<SqliteEmbeddedDocument>> {
+    // Get the vector database
+    let vector_db = match self.vector_db.upgrade() {
+      Some(db) => db,
+      None => return Err(FlowyError::internal().with_context("Vector database not initialized")),
+    };
+
+    vector_db
+      .select_all_embedded_documents(workspace_id, rag_ids)
+      .await
+      .map_err(|err| {
+        FlowyError::internal().with_context(format!("Failed to select embedded documents: {}", err))
+      })
+  }
 }
 
 #[async_trait]
@@ -156,7 +176,7 @@ impl VectorStore for SqliteVectorStore {
     let rag_ids = opt
       .filters
       .as_ref()
-      .and_then(|filters| filters.get("rag_ids"))
+      .and_then(|filters| filters.get(RAG_IDS))
       .and_then(|value| value.as_array())
       .map(|array| {
         array
@@ -177,7 +197,10 @@ impl VectorStore for SqliteVectorStore {
     // Return empty result if workspace_id is missing
     let workspace_id = match workspace_id {
       Some(id) => id.to_string(),
-      None => return Ok(Vec::new()),
+      None => {
+        warn!("[VectorStore] Missing workspace_id in filters. Returning empty result.");
+        return Ok(Vec::new());
+      },
     };
 
     // Get the vector database
@@ -203,13 +226,14 @@ impl VectorStore for SqliteVectorStore {
     }
 
     let score_threshold = opt.score_threshold.unwrap_or(0.4);
+    debug_assert!(embedding.len() == 1);
     let query_embedding = embedding.first().unwrap();
 
     // Perform similarity search in the database
     let results = vector_db
       .search_with_score(
         &workspace_id,
-        rag_ids,
+        &rag_ids,
         query_embedding,
         limit as i32,
         score_threshold,
@@ -217,9 +241,11 @@ impl VectorStore for SqliteVectorStore {
       .await?;
 
     trace!(
-      "[VectorStore] Found {} results for query: {}",
+      "[VectorStore] Found {} results for query:{}, rag_ids: {:?}, score_threshold: {}",
       results.len(),
-      query
+      query,
+      rag_ids,
+      score_threshold
     );
 
     // Convert results to Documents
diff --git a/frontend/rust-lib/flowy-ai/src/event_handler.rs b/frontend/rust-lib/flowy-ai/src/event_handler.rs
index 03a8830e8430a..1f4d35bc33211 100644
--- a/frontend/rust-lib/flowy-ai/src/event_handler.rs
+++ b/frontend/rust-lib/flowy-ai/src/event_handler.rs
@@ -346,7 +346,7 @@ pub(crate) async fn get_local_ai_models_handler(
   ai_manager: AFPluginState<Weak<AIManager>>,
 ) -> DataResult<ModelSelectionPB, FlowyError> {
   let ai_manager = upgrade_ai_manager(ai_manager)?;
-  let data = ai_manager.get_local_available_models().await?;
+  let data = ai_manager.get_local_available_models(None).await?;
   data_result_ok(data)
 }
 
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/chat/conversation_chain.rs b/frontend/rust-lib/flowy-ai/src/local_ai/chat/conversation_chain.rs
index aa7173a79cefd..95232aada8ee9 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/chat/conversation_chain.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/chat/conversation_chain.rs
@@ -1,7 +1,7 @@
 use crate::local_ai::chat::llm::LLMOllama;
 use async_stream::stream;
 use async_trait::async_trait;
-use flowy_ai_pub::entities::SOURCE_ID;
+use flowy_ai_pub::entities::{RAG_IDS, SOURCE_ID};
 use futures::Stream;
 use futures_util::{pin_mut, StreamExt};
 use langchain_rust::chain::{
@@ -14,10 +14,11 @@ use langchain_rust::prompt::{FormatPrompter, PromptArgs};
 use langchain_rust::schemas::{BaseMemory, Document, Message, Retriever, StreamData};
 use langchain_rust::vectorstore::{VecStoreOptions, VectorStore};
 use serde_json::{json, Value};
-use std::collections::HashSet;
 use std::error::Error;
 use std::{collections::HashMap, pin::Pin, sync::Arc};
 use tokio::sync::Mutex;
+use tokio_util::either::Either;
+use tracing::trace;
 
 pub(crate) const DEFAULT_OUTPUT_KEY: &str = "output";
 pub(crate) const DEFAULT_RESULT_KEY: &str = "generate_result";
@@ -28,7 +29,7 @@ const CONVERSATIONAL_RETRIEVAL_QA_DEFAULT_INPUT_KEY: &str = "question";
 
 pub struct ConversationalRetrieverChain {
   pub(crate) ollama: LLMOllama,
-  pub(crate) retriever: AFRetriever<Value>,
+  pub(crate) retriever: AFRetriever,
   pub memory: Arc<Mutex<dyn BaseMemory>>,
   pub(crate) combine_documents_chain: Box<dyn Chain>,
   pub(crate) condense_question_chain: Box<dyn Chain>,
@@ -38,65 +39,6 @@ pub struct ConversationalRetrieverChain {
   pub(crate) output_key: String,
 }
 impl ConversationalRetrieverChain {
-  pub fn set_rag_ids(&mut self, new_rag_ids: Vec<String>) {
-    let filters = self
-      .retriever
-      .options
-      .filters
-      .get_or_insert_with(|| json!({}));
-
-    filters["rag_ids"] = json!(new_rag_ids);
-  }
-
-  pub fn add_rag_ids<I, S>(&mut self, rag_ids: I)
-  where
-    I: IntoIterator<Item = S>,
-    S: AsRef<str>,
-  {
-    // Ensure a filter object exists
-    let filters = self
-      .retriever
-      .options
-      .filters
-      .get_or_insert_with(|| json!({}));
-
-    let arr = filters
-      .as_object_mut()
-      .expect("filters must be a JSON object")
-      .entry("rag_ids")
-      .or_insert_with(|| json!([]))
-      .as_array_mut()
-      .expect("`rag_ids` must be an array");
-
-    let mut existing: HashSet<String> = arr
-      .iter()
-      .filter_map(|v| v.as_str().map(|s| s.to_string()))
-      .collect();
-
-    // Insert new IDs if not already present
-    for id in rag_ids {
-      let id_str = id.as_ref();
-      if existing.insert(id_str.to_string()) {
-        arr.push(json!(id_str));
-      }
-    }
-  }
-
-  pub fn remove_rag_ids(&mut self, rag_ids: Vec<String>) {
-    if let Some(filters) = self.retriever.options.filters.as_mut() {
-      if let Some(current_rag_ids) = filters
-        .get_mut("rag_ids")
-        .and_then(|filter| filter.as_array_mut())
-      {
-        // Remove specified rag_ids from the array
-        current_rag_ids.retain(|id| {
-          id.as_str()
-            .map_or(true, |id_str| !rag_ids.contains(&id_str.to_string()))
-        });
-      }
-    }
-  }
-
   async fn get_question(
     &self,
     history: &[Message],
@@ -127,6 +69,35 @@ impl ConversationalRetrieverChain {
 
     Ok((question, token_usage))
   }
+
+  async fn get_documents_or_result(
+    &self,
+    question: &str,
+  ) -> Result<Either<Vec<Document>, GenerateResult>, ChainError> {
+    let rag_ids = self.retriever.get_rag_ids();
+    if rag_ids.is_empty() {
+      Ok(Either::Left(vec![]))
+    } else {
+      let documents = self
+        .retriever
+        .get_relevant_documents(question)
+        .await
+        .map_err(|e| ChainError::RetrieverError(e.to_string()))?;
+
+      if documents.is_empty() {
+        trace!(
+          "[Embedding] No relevant documents found, but we have RAG IDs:{:?}. return I don't know",
+          rag_ids
+        );
+        return Ok(Either::Right(GenerateResult {
+            tokens: None,
+            generation: "I couldn’t find any relevant information in the sources you selected. Please try asking a different question".to_string(),
+          }));
+      }
+
+      Ok(Either::Left(documents))
+    }
+  }
 }
 
 #[async_trait]
@@ -157,11 +128,19 @@ impl Chain for ConversationalRetrieverChain {
       token_usage = Some(token);
     }
 
-    let documents = self
-      .retriever
-      .get_relevant_documents(&question)
-      .await
-      .map_err(|e| ChainError::RetrieverError(e.to_string()))?;
+    let documents = match self.get_documents_or_result(&question).await? {
+      Either::Left(docs) => docs,
+      Either::Right(result) => {
+        let mut memory = self.memory.lock().await;
+        memory.add_message(human_message);
+        memory.add_message(Message::new_ai_message(&result.generation));
+
+        let mut output = HashMap::new();
+        output.insert(self.output_key.clone(), json!(result.generation));
+        output.insert(DEFAULT_RESULT_KEY.to_string(), json!(result));
+        return Ok(output);
+      },
+    };
 
     let mut output = self
       .combine_documents_chain
@@ -188,7 +167,6 @@ impl Chain for ConversationalRetrieverChain {
 
     let mut result = HashMap::new();
     result.insert(self.output_key.clone(), json!(output.generation));
-
     result.insert(DEFAULT_RESULT_KEY.to_string(), json!(output));
 
     if self.return_source_documents {
@@ -224,11 +202,22 @@ impl Chain for ConversationalRetrieverChain {
 
     let (question, _) = self.get_question(&history, &human_message.content).await?;
 
-    let documents = self
-      .retriever
-      .get_relevant_documents(&question)
-      .await
-      .map_err(|e| ChainError::RetrieverError(e.to_string()))?;
+    let documents = match self.get_documents_or_result(&question).await? {
+      Either::Left(docs) => docs,
+      Either::Right(result) => {
+        let mut memory = self.memory.lock().await;
+        memory.add_message(human_message);
+        memory.add_message(Message::new_ai_message(&result.generation));
+
+        return Ok(Box::pin(stream! {
+          yield Ok(StreamData::new(
+            json!(result),
+            result.tokens,
+            result.generation,
+          ));
+        }));
+      },
+    };
 
     let stream = self
       .combine_documents_chain
@@ -246,16 +235,6 @@ impl Chain for ConversationalRetrieverChain {
     let complete_ai_message_clone = complete_ai_message.clone();
     let output_stream = stream! {
         pin_mut!(stream);
-
-        for source in sources {
-          yield Ok(StreamData::new(
-              json!({"source": source}),
-              None,
-              "".to_string(),
-          ));
-        }
-
-
         while let Some(result) = stream.next().await {
             match result {
                 Ok(data) => {
@@ -271,6 +250,14 @@ impl Chain for ConversationalRetrieverChain {
             }
         }
 
+        for source in sources {
+          yield Ok(StreamData::new(
+              json!({"source": source}),
+              None,
+              "".to_string(),
+          ));
+        }
+
         let mut memory = memory.lock().await;
         memory.add_message(human_message);
         memory.add_message(Message::new_ai_message(&complete_ai_message.lock().await));
@@ -302,7 +289,7 @@ impl Chain for ConversationalRetrieverChain {
 
 pub struct ConversationalRetrieverChainBuilder {
   llm: Option<LLMOllama>,
-  retriever: Option<AFRetriever<Value>>,
+  retriever: Option<AFRetriever>,
   memory: Option<Arc<Mutex<dyn BaseMemory>>>,
   combine_documents_chain: Option<Box<dyn Chain>>,
   condense_question_chain: Option<Box<dyn Chain>>,
@@ -328,7 +315,7 @@ impl ConversationalRetrieverChainBuilder {
     }
   }
 
-  pub fn retriever(mut self, retriever: AFRetriever<Value>) -> Self {
+  pub fn retriever(mut self, retriever: AFRetriever) -> Self {
     self.retriever = Some(retriever);
     self
   }
@@ -424,34 +411,52 @@ impl ConversationalRetrieverChainBuilder {
 }
 
 // Retriever is a retriever for vector stores.
-pub struct AFRetriever<F> {
-  vstore: Box<dyn VectorStore<Options = VecStoreOptions<F>>>,
+pub type RetrieverOption = VecStoreOptions<Value>;
+pub struct AFRetriever {
+  vector_store: Box<dyn VectorStore<Options = RetrieverOption>>,
   num_docs: usize,
-  options: VecStoreOptions<F>,
+  options: RetrieverOption,
 }
-impl<F> AFRetriever<F> {
-  pub fn new<V: Into<Box<dyn VectorStore<Options = VecStoreOptions<F>>>>>(
-    vstore: V,
+impl AFRetriever {
+  pub fn new<V: Into<Box<dyn VectorStore<Options = RetrieverOption>>>>(
+    vector_store: V,
     num_docs: usize,
+    options: RetrieverOption,
   ) -> Self {
     AFRetriever {
-      vstore: vstore.into(),
+      vector_store: vector_store.into(),
       num_docs,
-      options: VecStoreOptions::<F>::new(),
+      options,
     }
   }
+  pub fn set_rag_ids(&mut self, new_rag_ids: Vec<String>) {
+    trace!("[VectorStore] retriever {:p}", self);
+    let filters = self.options.filters.get_or_insert_with(|| json!({}));
+    filters[RAG_IDS] = json!(new_rag_ids);
+  }
 
-  pub fn with_options(mut self, options: VecStoreOptions<F>) -> Self {
-    self.options = options;
+  pub fn get_rag_ids(&self) -> Vec<&str> {
+    trace!("[VectorStore] retriever {:p}", self);
     self
+      .options
+      .filters
+      .as_ref()
+      .and_then(|filters| filters.get(RAG_IDS).and_then(|rag_ids| rag_ids.as_array()))
+      .map(|rag_ids| rag_ids.iter().filter_map(|id| id.as_str()).collect())
+      .unwrap_or_default()
   }
 }
 
 #[async_trait]
-impl<O: Sync + Send> Retriever for AFRetriever<O> {
+impl Retriever for AFRetriever {
   async fn get_relevant_documents(&self, query: &str) -> Result<Vec<Document>, Box<dyn Error>> {
+    trace!(
+      "[VectorStore] filters: {:?}, retrieving documents for query: {}",
+      self.options.filters,
+      query,
+    );
     self
-      .vstore
+      .vector_store
       .similarity_search(query, self.num_docs, &self.options)
       .await
   }
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/chat/format_prompt.rs b/frontend/rust-lib/flowy-ai/src/local_ai/chat/format_prompt.rs
new file mode 100644
index 0000000000000..fb29ea7b7766c
--- /dev/null
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/chat/format_prompt.rs
@@ -0,0 +1,104 @@
+use crate::local_ai::prompt::format_prompt;
+use flowy_ai_pub::cloud::ResponseFormat;
+use flowy_error::{FlowyError, FlowyResult};
+use langchain_rust::prompt::{
+  FormatPrompter, HumanMessagePromptTemplate, MessageFormatter, PromptArgs, PromptError,
+};
+use langchain_rust::schemas::{Message, PromptValue};
+use langchain_rust::template_jinja2;
+use std::sync::{Arc, RwLock};
+
+const DEFAULT_QA_TEMPLATE: &str = r#"
+Only Use the context provided below to formulate your answer. Do not use any other information. If the context doesn't contain sufficient information to answer the question, respond with "I don't know".
+Do not reference external knowledge or information outside the context.
+
+##Context##
+{{context}}
+
+Question:{{question}}
+Answer:
+"#;
+
+struct FormatState {
+  format_msg: Arc<Message>,
+  format: ResponseFormat,
+}
+
+pub struct AFMessageFormatter {
+  system_msg: Arc<Message>,
+  state: Arc<RwLock<FormatState>>,
+  user_tmpl: Arc<HumanMessagePromptTemplate>,
+}
+
+impl AFMessageFormatter {
+  pub fn new(system_msg: Message, fmt: &ResponseFormat) -> Self {
+    // Compile the Jinja template exactly once
+    let user_tmpl =
+      HumanMessagePromptTemplate::new(template_jinja2!(DEFAULT_QA_TEMPLATE, "context", "question"));
+
+    let state = FormatState {
+      format_msg: Arc::new(format_prompt(fmt)),
+      format: fmt.clone(),
+    };
+
+    Self {
+      system_msg: Arc::new(system_msg),
+      state: Arc::new(RwLock::new(state)),
+      user_tmpl: Arc::new(user_tmpl),
+    }
+  }
+
+  /// Returns true if we actually swapped in a new instruction
+  pub fn update_format(&self, new_fmt: &ResponseFormat) -> FlowyResult<()> {
+    let mut st = self
+      .state
+      .write()
+      .map_err(|err| FlowyError::internal().with_context(err))?;
+
+    if st.format.output_layout != new_fmt.output_layout {
+      st.format = new_fmt.clone();
+      st.format_msg = Arc::new(format_prompt(new_fmt));
+    }
+
+    Ok(())
+  }
+}
+
+impl Clone for AFMessageFormatter {
+  fn clone(&self) -> Self {
+    Self {
+      system_msg: Arc::clone(&self.system_msg),
+      state: Arc::clone(&self.state),
+      user_tmpl: Arc::clone(&self.user_tmpl),
+    }
+  }
+}
+
+impl MessageFormatter for AFMessageFormatter {
+  fn format_messages(&self, args: PromptArgs) -> Result<Vec<Message>, PromptError> {
+    let mut out = Vec::with_capacity(3);
+    out.push((*self.system_msg).clone());
+
+    if let Ok(st) = self.state.try_read() {
+      out.push((*st.format_msg).clone());
+    }
+
+    out.extend(self.user_tmpl.format_messages(args)?);
+    Ok(out)
+  }
+
+  fn input_variables(&self) -> Vec<String> {
+    vec!["context".into(), "question".into()]
+  }
+}
+
+impl FormatPrompter for AFMessageFormatter {
+  fn format_prompt(&self, input_variables: PromptArgs) -> Result<PromptValue, PromptError> {
+    let messages = self.format_messages(input_variables)?;
+    Ok(PromptValue::from_messages(messages))
+  }
+
+  fn get_input_variables(&self) -> Vec<String> {
+    self.input_variables()
+  }
+}
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/chat/llm_chat.rs b/frontend/rust-lib/flowy-ai/src/local_ai/chat/llm_chat.rs
index 6748d76c709d4..a60efa651fd91 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/chat/llm_chat.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/chat/llm_chat.rs
@@ -1,33 +1,33 @@
 use crate::local_ai::chat::conversation_chain::{
-  AFRetriever, ConversationalRetrieverChain, ConversationalRetrieverChainBuilder,
+  AFRetriever, ConversationalRetrieverChain, ConversationalRetrieverChainBuilder, RetrieverOption,
 };
+use crate::local_ai::chat::format_prompt::AFMessageFormatter;
 use crate::local_ai::chat::llm::LLMOllama;
 use crate::local_ai::chat::OllamaClientRef;
-use crate::local_ai::prompt::format_prompt;
 use crate::SqliteVectorStore;
 use flowy_ai_pub::cloud::{QuestionStreamValue, ResponseFormat, StreamAnswer};
-use flowy_ai_pub::entities::SOURCE_ID;
+use flowy_ai_pub::entities::{RAG_IDS, SOURCE_ID};
 use flowy_error::{FlowyError, FlowyResult};
 use futures::StreamExt;
 use langchain_rust::chain::{Chain, ChainError};
 use langchain_rust::memory::SimpleMemory;
-use langchain_rust::prompt::{HumanMessagePromptTemplate, MessageFormatterStruct};
+use langchain_rust::prompt_args;
 use langchain_rust::schemas::{Document, Message};
 use langchain_rust::vectorstore::{VecStoreOptions, VectorStore};
-use langchain_rust::{fmt_message, fmt_template, message_formatter, prompt_args, template_jinja2};
-use serde_json::{json, Value};
+use serde_json::json;
 use std::collections::HashMap;
+use tracing::{info, trace};
 use uuid::Uuid;
 
 pub struct LLMChat {
   workspace_id: Uuid,
-  #[allow(dead_code)]
   chat_id: Uuid,
   store: Option<SqliteVectorStore>,
   chain: ConversationalRetrieverChain,
+  #[allow(dead_code)]
   client: OllamaClientRef,
-  current_format: ResponseFormat,
   rag_ids: Vec<String>,
+  dynamic_formatter: AFMessageFormatter,
 }
 
 impl LLMChat {
@@ -39,13 +39,14 @@ impl LLMChat {
     store: Option<SqliteVectorStore>,
     rag_ids: Vec<String>,
   ) -> FlowyResult<Self> {
-    let current_format = ResponseFormat::default();
-    let chain = create_chain(
+    let initial_format = ResponseFormat::default();
+    let formatter = create_dynamic_prompt_with_format(&initial_format);
+    let (chain, dynamic_formatter) = create_chain(
       &workspace_id,
       model,
       &client,
       rag_ids.clone(),
-      &current_format,
+      formatter,
       store.clone(),
     )
     .await?;
@@ -56,28 +57,55 @@ impl LLMChat {
       store,
       chain,
       client,
-      current_format,
       rag_ids,
+      dynamic_formatter,
     })
   }
 
-  pub async fn set_chat_model(&mut self, model: &str) -> FlowyResult<()> {
+  pub fn set_chat_model(&mut self, model: &str) {
     self.chain.ollama.set_model(model);
-    Ok(())
   }
 
-  pub async fn add_rag_id(&mut self, id: String) -> FlowyResult<()> {
-    self.chain.add_rag_ids(vec![id]);
-    Ok(())
+  pub async fn set_rag_ids(&mut self, rag_ids: Vec<String>) {
+    info!("[VectorStore]: {} set rag ids: {:?}", self.chat_id, rag_ids);
+    self.chain.retriever.set_rag_ids(rag_ids);
   }
 
-  pub async fn set_rag_ids(&mut self, rag_ids: Vec<String>) -> FlowyResult<()> {
-    self.chain.set_rag_ids(rag_ids);
-    Ok(())
+  pub async fn search(
+    &self,
+    query: &str,
+    limit: usize,
+    ids: Vec<String>,
+  ) -> FlowyResult<Vec<Document>> {
+    let store = self
+      .store
+      .as_ref()
+      .ok_or_else(|| FlowyError::local_ai().with_context("VectorStore is not initialized"))?;
+
+    let options =
+      RetrieverOption::new().with_filters(json!({RAG_IDS: ids, "workspace_id": self.workspace_id}));
+    let result = store
+      .similarity_search(query, limit, &options)
+      .await
+      .map_err(|err| FlowyError::local_ai().with_context(err))?;
+    Ok(result)
   }
 
-  pub fn remove_rag_id(&mut self, id: String) {
-    self.chain.remove_rag_ids(vec![id]);
+  #[cfg(any(target_os = "windows", target_os = "macos", target_os = "linux"))]
+  pub async fn get_all_embedded_documents(
+    &self,
+  ) -> FlowyResult<Vec<flowy_sqlite_vec::entities::SqliteEmbeddedDocument>> {
+    let store = self
+      .store
+      .as_ref()
+      .ok_or_else(|| FlowyError::local_ai().with_context("VectorStore is not initialized"))?;
+
+    store
+      .select_all_embedded_documents(&self.workspace_id.to_string(), &self.rag_ids)
+      .await
+      .map_err(|err| {
+        FlowyError::local_ai().with_context(format!("Failed to select embedded documents: {}", err))
+      })
   }
 
   pub async fn embed_paragraphs(
@@ -117,19 +145,7 @@ impl LLMChat {
     message: &str,
     format: ResponseFormat,
   ) -> Result<StreamAnswer, FlowyError> {
-    if self.current_format.output_layout != format.output_layout {
-      self.current_format = format.clone();
-      self.chain = create_chain(
-        &self.workspace_id,
-        self.chain.ollama.model_name.as_ref(),
-        &self.client,
-        self.rag_ids.clone(),
-        &self.current_format,
-        self.store.clone(),
-      )
-      .await?;
-    }
-
+    self.dynamic_formatter.update_format(&format)?;
     let input_variables = prompt_args! {
         "question" => message,
     };
@@ -140,6 +156,7 @@ impl LLMChat {
       result
         .map(|stream_data| {
           if let Some(source) = stream_data.value.as_object().and_then(|v| v.get("source")) {
+            trace!("[VectorStore]: reference sources: {:?}", source);
             QuestionStreamValue::Metadata {
               value: source.clone(),
             }
@@ -155,37 +172,28 @@ impl LLMChat {
   }
 }
 
-fn create_prompt_with_format(format: &ResponseFormat) -> MessageFormatterStruct {
-  let format_instruction = format_prompt(format);
-  message_formatter![
-    fmt_message!(Message::new_system_message(
-      "You are a helpful assistant", 
-    )),
-    fmt_message!(format_instruction),
-    fmt_template!(HumanMessagePromptTemplate::new(
-      template_jinja2!("
-        Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.
-        {{context}}
-
-        Question:{{question}}
-        Answer:
-        ",
-        "context",
-        "question"
-      )
-    ))
-  ]
+fn create_dynamic_prompt_with_format(format: &ResponseFormat) -> AFMessageFormatter {
+  let system_message =
+    Message::new_system_message("You are an assistant for question-answering tasks");
+
+  AFMessageFormatter::new(system_message, format)
 }
 
 fn create_retriever(
   workspace_id: &Uuid,
   rag_ids: Vec<String>,
   store: SqliteVectorStore,
-) -> AFRetriever<Value> {
+) -> AFRetriever {
+  trace!(
+    "[VectorStore]: {} create retriever with rag_ids: {:?}",
+    workspace_id,
+    rag_ids,
+  );
   let options = VecStoreOptions::default()
-    .with_score_threshold(0.4)
-    .with_filters(json!({"rag_ids": rag_ids, "workspace_id": workspace_id}));
-  AFRetriever::<Value>::new(store, 5).with_options(options)
+    .with_score_threshold(0.2)
+    .with_filters(json!({RAG_IDS: rag_ids, "workspace_id": workspace_id}));
+
+  AFRetriever::new(store, 5, options)
 }
 
 async fn create_chain(
@@ -193,15 +201,14 @@ async fn create_chain(
   model: &str,
   client: &OllamaClientRef,
   rag_ids: Vec<String>,
-  format: &ResponseFormat,
+  formatter: AFMessageFormatter,
   store: Option<SqliteVectorStore>,
-) -> FlowyResult<ConversationalRetrieverChain> {
+) -> FlowyResult<(ConversationalRetrieverChain, AFMessageFormatter)> {
   let llm = create_llm(client, model).await?;
-  let prompt = create_prompt_with_format(format);
 
   let mut builder = ConversationalRetrieverChainBuilder::new()
     .llm(llm)
-    .rephrase_question(true)
+    .rephrase_question(false)
     .memory(SimpleMemory::new().into());
 
   if let Some(store) = store {
@@ -210,11 +217,11 @@ async fn create_chain(
   }
 
   let chain = builder
-    .prompt(prompt)
+    .prompt(formatter.clone())
     .build()
     .map_err(|err| FlowyError::local_ai().with_context(err))?;
 
-  Ok(chain)
+  Ok((chain, formatter))
 }
 
 async fn create_llm(client: &OllamaClientRef, model: &str) -> FlowyResult<LLMOllama> {
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/chat/mod.rs b/frontend/rust-lib/flowy-ai/src/local_ai/chat/mod.rs
index f89b7addbe40b..5d233bcac635d 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/chat/mod.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/chat/mod.rs
@@ -1,4 +1,5 @@
 mod conversation_chain;
+mod format_prompt;
 pub mod llm;
 pub mod llm_chat;
 pub mod related_question_chain;
@@ -27,7 +28,7 @@ use std::collections::HashMap;
 use std::path::PathBuf;
 use std::sync::{Arc, Weak};
 use tokio::sync::RwLock;
-use tracing::trace;
+use tracing::{info, trace};
 use uuid::Uuid;
 
 type OllamaClientRef = Arc<RwLock<Option<Weak<Ollama>>>>;
@@ -63,20 +64,28 @@ impl LLMChatController {
     *self.store.write().await = Some(store);
   }
 
+  pub async fn set_rag_ids(&self, chat_id: &Uuid, rag_ids: &[String]) {
+    if let Some(mut chat) = self.chat_by_id.get_mut(chat_id) {
+      chat.set_rag_ids(rag_ids.to_vec()).await;
+    }
+  }
+
   pub async fn open_chat(
     &self,
     workspace_id: &Uuid,
     chat_id: &Uuid,
     model: &str,
+    rag_ids: Vec<String>,
   ) -> FlowyResult<()> {
     let store = self.store.read().await.clone();
+    info!("[VectorStore]: {} open chat: {:?}", chat_id, model);
     let chat = LLMChat::new(
       *workspace_id,
       *chat_id,
       model,
       self.client.clone(),
       store,
-      vec![],
+      rag_ids,
     )
     .await?;
     self.chat_by_id.insert(*chat_id, chat);
@@ -208,8 +217,11 @@ impl LLMChatController {
     chat_id: &Uuid,
     question: &str,
     format: ResponseFormat,
+    model_name: &str,
   ) -> FlowyResult<StreamAnswer> {
     if let Some(mut chat) = self.chat_by_id.get_mut(chat_id) {
+      chat.set_chat_model(model_name);
+
       let response = chat.stream_question(question, format).await;
       return response;
     }
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/controller.rs b/frontend/rust-lib/flowy-ai/src/local_ai/controller.rs
index 80e43678ad115..738207f2c78b5 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/controller.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/controller.rs
@@ -177,11 +177,20 @@ impl LocalAIController {
     Some(self.resource.get_llm_setting().chat_model_name)
   }
 
+  pub async fn set_chat_rag_ids(&self, chat_id: &Uuid, rag_ids: &[String]) {
+    if !self.is_enabled() {
+      return;
+    }
+
+    self.llm_controller.set_rag_ids(chat_id, rag_ids).await;
+  }
+
   pub async fn open_chat(
     &self,
     workspace_id: &Uuid,
     chat_id: &Uuid,
     model: &str,
+    rag_ids: Vec<String>,
   ) -> FlowyResult<()> {
     if !self.is_enabled() {
       return Ok(());
@@ -197,7 +206,7 @@ impl LocalAIController {
     self.current_chat_id.store(Some(Arc::new(*chat_id)));
     self
       .llm_controller
-      .open_chat(workspace_id, chat_id, model)
+      .open_chat(workspace_id, chat_id, model, rag_ids)
       .await?;
     Ok(())
   }
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/resource.rs b/frontend/rust-lib/flowy-ai/src/local_ai/resource.rs
index 3f091f69a713e..e5cba112702fe 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/resource.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/resource.rs
@@ -144,7 +144,6 @@ impl LocalAIResourceController {
           log::error!("[LLM Resource] Failed to parse /api/tags JSON response: {e:?}")
         })?;
         // Check if each of our required models exists in the list of available models
-        trace!("[LLM Resource] ollama available models: {:?}", tags.models);
         for required in &required_models {
           if !tags
             .models
diff --git a/frontend/rust-lib/flowy-ai/src/middleware/chat_service_mw.rs b/frontend/rust-lib/flowy-ai/src/middleware/chat_service_mw.rs
index 0fa92c6649fc9..af15e744468aa 100644
--- a/frontend/rust-lib/flowy-ai/src/middleware/chat_service_mw.rs
+++ b/frontend/rust-lib/flowy-ai/src/middleware/chat_service_mw.rs
@@ -101,20 +101,15 @@ impl ChatCloudService for ChatServiceMiddleware {
     chat_id: &Uuid,
     question_id: i64,
     format: ResponseFormat,
-    ai_model: Option<AIModel>,
+    ai_model: AIModel,
   ) -> Result<StreamAnswer, FlowyError> {
-    let use_local_ai = match &ai_model {
-      None => false,
-      Some(model) => model.is_local,
-    };
-
     info!("stream_answer use model: {:?}", ai_model);
-    if use_local_ai {
+    if ai_model.is_local {
       if self.local_ai.is_ready().await {
         let content = self.get_message_content(question_id)?;
         self
           .local_ai
-          .stream_question(chat_id, &content, format)
+          .stream_question(chat_id, &content, format, &ai_model.name)
           .await
       } else {
         Err(FlowyError::local_ai_not_ready())
diff --git a/frontend/rust-lib/flowy-ai/src/offline/offline_message_sync.rs b/frontend/rust-lib/flowy-ai/src/offline/offline_message_sync.rs
index 24148b9927fdc..71d3e37ce70f7 100644
--- a/frontend/rust-lib/flowy-ai/src/offline/offline_message_sync.rs
+++ b/frontend/rust-lib/flowy-ai/src/offline/offline_message_sync.rs
@@ -133,7 +133,7 @@ impl ChatCloudService for AutoSyncChatService {
     chat_id: &Uuid,
     question_id: i64,
     format: ResponseFormat,
-    ai_model: Option<AIModel>,
+    ai_model: AIModel,
   ) -> Result<StreamAnswer, FlowyError> {
     self
       .cloud_service
diff --git a/frontend/rust-lib/flowy-ai/src/search/summary.rs b/frontend/rust-lib/flowy-ai/src/search/summary.rs
index 1b38b9980fdb9..672517dbf80b7 100644
--- a/frontend/rust-lib/flowy-ai/src/search/summary.rs
+++ b/frontend/rust-lib/flowy-ai/src/search/summary.rs
@@ -31,7 +31,6 @@ struct SummarySearchSchema {
 
 const SYSTEM_PROMPT: &str = r#"
 You are a strict, context-bound question answering assistant. Answer solely based on the context provided below. If the context lacks sufficient information for a confident response, reply with an empty answer.
-Do not reference or use any information beyond what is provided in the context.
 
 Output must include:
 - `answer`: a detailed, on-point answer to the user’s question.
diff --git a/frontend/rust-lib/flowy-ai/tests/chat_test/mod.rs b/frontend/rust-lib/flowy-ai/tests/chat_test/mod.rs
index 829066d2f4f84..ad02b10033993 100644
--- a/frontend/rust-lib/flowy-ai/tests/chat_test/mod.rs
+++ b/frontend/rust-lib/flowy-ai/tests/chat_test/mod.rs
@@ -4,10 +4,11 @@ use flowy_ai::local_ai::chat::llm::LLMOllama;
 use flowy_ai::local_ai::chat::llm_chat::LLMChat;
 use flowy_ai::local_ai::chat::related_question_chain::RelatedQuestionChain;
 use flowy_ai_pub::cloud::{OutputLayout, QuestionStreamValue, ResponseFormat, StreamAnswer};
-use flowy_ai_pub::entities::SOURCE_ID;
+use flowy_ai_pub::entities::{SOURCE, SOURCE_ID, SOURCE_NAME};
 use flowy_sqlite_vec::db::VectorSqliteDB;
 use ollama_rs::Ollama;
 use reqwest::Url;
+use serde_json::Value;
 use std::sync::Arc;
 use tempfile::tempdir;
 use tokio::sync::RwLock;
@@ -25,14 +26,14 @@ pub struct TestContext {
 
 impl TestContext {
   pub fn new() -> anyhow::Result<Self> {
-    let ollama_url = "http://localhost:11434";
+    setup_log();
 
+    let ollama_url = "http://localhost:11434";
     let url = Url::parse(ollama_url)?;
     let ollama = Arc::new(Ollama::from_url(url.clone()));
 
     let temp_dir = tempdir()?;
     let db = Arc::new(VectorSqliteDB::new(temp_dir.into_path())?);
-
     let vector_store = SqliteVectorStore::new(Arc::downgrade(&ollama), Arc::downgrade(&db));
 
     Ok(Self {
@@ -75,7 +76,7 @@ async fn local_ollama_test_simple_question() {
 }
 
 #[tokio::test]
-async fn local_ollama_test_chat_context_retrieve() {
+async fn local_ollama_test_chat_with_multiple_docs_retrieve() {
   let context = TestContext::new().unwrap();
   let mut chat = context.create_chat(vec![]).await;
   let mut ids = vec![];
@@ -86,7 +87,29 @@ async fn local_ollama_test_chat_context_retrieve() {
         ids.push(id.to_string());
         chat.embed_paragraphs(&id.to_string(), vec![doc.to_string()]).await.unwrap();
     }
-  chat.set_rag_ids(ids.clone()).await.unwrap();
+  chat.set_rag_ids(ids.clone()).await;
+
+  let all_docs = chat.get_all_embedded_documents().await.unwrap();
+  assert_eq!(all_docs.len(), 3);
+  assert_eq!(all_docs[0].fragments.len(), 1);
+  assert_eq!(all_docs[1].fragments.len(), 1);
+  assert_eq!(all_docs[2].fragments.len(), 1);
+
+  let docs = chat
+    .search("Rust is a multiplayer survival game", 5, ids.clone())
+    .await
+    .unwrap();
+  assert_eq!(docs.len(), 1);
+
+  let docs = chat
+    .search(
+      "chemical process of rust formation on metal",
+      5,
+      ids.clone(),
+    )
+    .await
+    .unwrap();
+  assert_eq!(docs.len(), 1);
 
   let stream = chat
     .stream_question("Rust is a multiplayer survival game", Default::default())
@@ -97,7 +120,17 @@ async fn local_ollama_test_chat_context_retrieve() {
   dbg!(&sources);
 
   assert!(!answer.is_empty());
-  assert_eq!(sources.len(), 1);
+  assert!(!sources.is_empty());
+  assert!(sources[0].get(SOURCE_ID).unwrap().as_str().is_some());
+  assert!(sources[0].get(SOURCE).unwrap().as_str().is_some());
+  assert!(sources[0].get(SOURCE_NAME).unwrap().as_str().is_some());
+
+  let stream = chat
+    .stream_question("Japan ski resort", Default::default())
+    .await
+    .unwrap();
+  let (answer, _) = collect_stream(stream).await;
+  dbg!(&answer);
 }
 
 #[tokio::test]
@@ -116,7 +149,7 @@ async fn local_ollama_test_chat_format() {
   assert!(!answer.is_empty());
 }
 
-async fn collect_stream(stream: StreamAnswer) -> (String, Vec<String>) {
+async fn collect_stream(stream: StreamAnswer) -> (String, Vec<Value>) {
   let mut result = String::new();
   let mut sources = vec![];
   let mut stream = stream;
@@ -127,7 +160,8 @@ async fn collect_stream(stream: StreamAnswer) -> (String, Vec<String>) {
           result.push_str(&value);
         },
         QuestionStreamValue::Metadata { value } => {
-          sources.push(value.get(SOURCE_ID).unwrap().to_string());
+          dbg!("metadata", &value);
+          sources.push(value);
         },
         QuestionStreamValue::KeepAlive => {},
       },
diff --git a/frontend/rust-lib/flowy-core/src/app_life_cycle.rs b/frontend/rust-lib/flowy-core/src/app_life_cycle.rs
index bddca3af9adaa..1242de3bdce4e 100644
--- a/frontend/rust-lib/flowy-core/src/app_life_cycle.rs
+++ b/frontend/rust-lib/flowy-core/src/app_life_cycle.rs
@@ -3,12 +3,12 @@ use client_api::entity::billing_dto::SubscriptionPlan;
 use std::sync::{Arc, Weak};
 use tracing::{error, event, info, instrument};
 
-use crate::full_indexed_data_provider::FullIndexedDataProvider;
+use crate::full_indexed_data_provider::FullIndexedDataWriter;
 use crate::indexed_data_consumer::{close_document_tantivy_state, get_document_tantivy_state};
 use crate::server_layer::ServerProvider;
 use collab_entity::CollabType;
 use collab_integrate::collab_builder::AppFlowyCollabBuilder;
-use collab_integrate::instant_indexed_data_provider::InstantIndexedDataProvider;
+use collab_integrate::instant_indexed_data_provider::InstantIndexedDataWriter;
 use collab_plugins::local_storage::kv::doc::CollabKVAction;
 use collab_plugins::local_storage::kv::KVTransactionDB;
 use flowy_ai::ai_manager::AIManager;
@@ -39,8 +39,8 @@ pub(crate) struct AppLifeCycleImpl {
   pub(crate) storage_manager: Weak<StorageManager>,
   pub(crate) search_manager: Weak<SearchManager>,
   pub(crate) ai_manager: Weak<AIManager>,
-  pub(crate) instant_indexed_data_provider: Option<Arc<InstantIndexedDataProvider>>,
-  pub(crate) full_indexed_data_provider: Weak<RwLock<Option<FullIndexedDataProvider>>>,
+  pub(crate) instant_indexed_data_writer: Option<Arc<InstantIndexedDataWriter>>,
+  pub(crate) full_indexed_data_writer: Weak<RwLock<Option<FullIndexedDataWriter>>>,
   pub(crate) logged_user: Arc<dyn LoggedUser>,
   // By default, all callback will run on the caller thread. If you don't want to block the caller
   // thread, you can use runtime to spawn a new task.
diff --git a/frontend/rust-lib/flowy-core/src/deps_resolve/chat_deps.rs b/frontend/rust-lib/flowy-core/src/deps_resolve/chat_deps.rs
index 9509f271a6f68..895527013d24a 100644
--- a/frontend/rust-lib/flowy-core/src/deps_resolve/chat_deps.rs
+++ b/frontend/rust-lib/flowy-core/src/deps_resolve/chat_deps.rs
@@ -108,7 +108,7 @@ impl AIExternalService for ChatQueryServiceImpl {
           )?;
 
           if !is_change_since_sv(&collab, &prev_sv) {
-            info!("[Chat] no change since sv: {}", rag_id);
+            info!("[Embedding] skip full sync {}, no changes", rag_id);
             continue;
           }
         }
@@ -121,6 +121,7 @@ impl AIExternalService for ChatQueryServiceImpl {
         encoded_collab: query_collab.encoded_collab.clone(),
       };
 
+      info!("[Embedding] full sync rag document: {}", params.object_id);
       if let Err(err) = self
         .folder_cloud_service
         .full_sync_collab_object(workspace_id, params)
@@ -128,7 +129,6 @@ impl AIExternalService for ChatQueryServiceImpl {
       {
         error!("Failed to sync rag document: {} error: {}", rag_id, err);
       } else {
-        info!("[Chat] full sync rag document: {}", rag_id);
         result.push(AFCollabMetadata {
           object_id: rag_id.to_string(),
           updated_at: timestamp(),
diff --git a/frontend/rust-lib/flowy-core/src/deps_resolve/cloud_service_impl.rs b/frontend/rust-lib/flowy-core/src/deps_resolve/cloud_service_impl.rs
index 6aa61e59f4aa3..386dcda694991 100644
--- a/frontend/rust-lib/flowy-core/src/deps_resolve/cloud_service_impl.rs
+++ b/frontend/rust-lib/flowy-core/src/deps_resolve/cloud_service_impl.rs
@@ -688,7 +688,7 @@ impl ChatCloudService for ServerProvider {
     chat_id: &Uuid,
     question_id: i64,
     format: ResponseFormat,
-    ai_model: Option<AIModel>,
+    ai_model: AIModel,
   ) -> Result<StreamAnswer, FlowyError> {
     let server = self.get_server()?;
     server
diff --git a/frontend/rust-lib/flowy-core/src/deps_resolve/folder_deps/mod.rs b/frontend/rust-lib/flowy-core/src/deps_resolve/folder_deps/mod.rs
index 1be60c5a45db7..62a02e177d9a0 100644
--- a/frontend/rust-lib/flowy-core/src/deps_resolve/folder_deps/mod.rs
+++ b/frontend/rust-lib/flowy-core/src/deps_resolve/folder_deps/mod.rs
@@ -25,6 +25,7 @@ use crate::deps_resolve::folder_deps::folder_deps_doc_impl::DocumentFolderOperat
 use collab_plugins::local_storage::kv::KVTransactionDB;
 use flowy_folder_pub::query::{FolderQueryService, FolderService, FolderViewEdit, QueryCollab};
 use lib_infra::async_trait::async_trait;
+use tracing::trace;
 use uuid::Uuid;
 
 pub struct FolderDepsResolver();
@@ -126,10 +127,10 @@ impl FolderServiceImpl {
 #[async_trait]
 impl FolderViewEdit for FolderServiceImpl {
   async fn set_view_title_if_empty(&self, view_id: &Uuid, title: &str) -> FlowyResult<()> {
+    trace!("Set view title: view_id: {}, title: {}", view_id, title);
     if title.is_empty() {
       return Ok(());
     }
-
     if let Some(folder_manager) = self.folder_manager.upgrade() {
       if let Ok(view) = folder_manager.get_view(view_id.to_string().as_str()).await {
         if view.name.is_empty() {
diff --git a/frontend/rust-lib/flowy-core/src/folder_view_observer.rs b/frontend/rust-lib/flowy-core/src/folder_view_observer.rs
index 093e5552d1156..fe3d4c0aecc88 100644
--- a/frontend/rust-lib/flowy-core/src/folder_view_observer.rs
+++ b/frontend/rust-lib/flowy-core/src/folder_view_observer.rs
@@ -36,7 +36,7 @@ impl FolderViewObserver for FolderViewObserverImpl {
             Ok(view) => {
               let _ = state.write().await.add_document_metadata(
                 &view.id,
-                view.name.clone(),
+                Some(view.name.clone()),
                 view.icon.clone(),
               );
             },
@@ -46,7 +46,7 @@ impl FolderViewObserver for FolderViewObserverImpl {
             Ok(view) => {
               let _ = state.write().await.add_document_metadata(
                 &view.id,
-                view.name.clone(),
+                Some(view.name.clone()),
                 view.icon.clone(),
               );
             },
diff --git a/frontend/rust-lib/flowy-core/src/full_indexed_data_provider.rs b/frontend/rust-lib/flowy-core/src/full_indexed_data_provider.rs
index 16de1faf4d46e..bf0d0ffb6e054 100644
--- a/frontend/rust-lib/flowy-core/src/full_indexed_data_provider.rs
+++ b/frontend/rust-lib/flowy-core/src/full_indexed_data_provider.rs
@@ -1,8 +1,8 @@
 use client_api::entity::workspace_dto::ViewIcon;
 use collab::preclude::Collab;
-use collab_document::document::DocumentBody;
 use collab_entity::CollabType;
 use collab_folder::{View, ViewLayout};
+use collab_integrate::instant_indexed_data_provider::unindexed_data_form_collab;
 use collab_plugins::local_storage::kv::doc::CollabKVAction;
 use collab_plugins::local_storage::kv::KVTransactionDB;
 use flowy_ai_pub::entities::{UnindexedCollab, UnindexedCollabMetadata, UnindexedData};
@@ -28,7 +28,7 @@ pub trait FullIndexedDataConsumer: Send + Sync {
 }
 
 #[derive(Clone)]
-pub struct FullIndexedDataProvider {
+pub struct FullIndexedDataWriter {
   workspace_id: Uuid,
   folder_manager: Weak<FolderManager>,
   logged_user: Weak<dyn LoggedUser>,
@@ -36,7 +36,7 @@ pub struct FullIndexedDataProvider {
   consumers: Arc<RwLock<Vec<Box<dyn FullIndexedDataConsumer>>>>,
 }
 
-impl FullIndexedDataProvider {
+impl FullIndexedDataWriter {
   pub fn new(
     workspace_id: Uuid,
     folder_manager: Weak<FolderManager>,
@@ -249,7 +249,7 @@ impl FullIndexedDataProvider {
 
         // Create metadata once for reuse
         let metadata = UnindexedCollabMetadata {
-          name: view.name.clone(),
+          name: Some(view.name.clone()),
           icon: view.icon.as_ref().map(|icon| ViewIcon {
             ty: IconType::from(icon.ty.clone() as u8),
             value: icon.value.clone(),
@@ -268,26 +268,16 @@ impl FullIndexedDataProvider {
             };
 
             if load_success {
-              if let Some(document) = DocumentBody::from_collab(&collab) {
-                let paragraphs = document.paragraphs(collab.transact());
+              if let Some(data) = unindexed_data_form_collab(&collab, &collab_type) {
                 results.push(UnindexedCollab {
                   workspace_id,
                   object_id,
-                  collab_type: CollabType::Document,
-                  data: UnindexedData::Paragraphs(paragraphs),
+                  collab_type,
+                  data,
                   metadata,
                 });
-                continue;
               }
             }
-            // When load fails or document extraction fails, use empty text
-            results.push(UnindexedCollab {
-              workspace_id,
-              object_id,
-              collab_type: CollabType::Document,
-              data: UnindexedData::Text(String::new()),
-              metadata,
-            });
           },
           CollabType::Database => {
             results.push(UnindexedCollab {
diff --git a/frontend/rust-lib/flowy-core/src/indexed_data_consumer.rs b/frontend/rust-lib/flowy-core/src/indexed_data_consumer.rs
index d387c6660b026..67eb17760268d 100644
--- a/frontend/rust-lib/flowy-core/src/indexed_data_consumer.rs
+++ b/frontend/rust-lib/flowy-core/src/indexed_data_consumer.rs
@@ -1,6 +1,6 @@
 use crate::folder_view_observer::FolderViewObserverImpl;
 use crate::full_indexed_data_provider::FullIndexedDataConsumer;
-use collab_entity::{CollabObject, CollabType};
+use collab_entity::CollabType;
 use collab_folder::folder_diff::FolderViewChange;
 use collab_folder::{IconType, ViewIcon};
 use collab_integrate::instant_indexed_data_provider::InstantIndexedDataConsumer;
@@ -45,7 +45,7 @@ impl FullIndexedDataConsumer for EmbeddingFullIndexConsumer {
 }
 
 pub struct EmbeddingsInstantConsumerImpl {
-  consume_history: DashMap<String, String>,
+  consume_history: DashMap<Uuid, String>,
 }
 
 impl EmbeddingsInstantConsumerImpl {
@@ -64,41 +64,39 @@ impl InstantIndexedDataConsumer for EmbeddingsInstantConsumerImpl {
 
   async fn consume_collab(
     &self,
-    collab_object: &CollabObject,
+    workspace_id: &Uuid,
     data: UnindexedData,
+    object_id: &Uuid,
+    collab_type: CollabType,
   ) -> Result<bool, FlowyError> {
     if data.is_empty() {
       return Ok(false);
     }
 
     let content_hash = data.content_hash();
-    if let Some(entry) = self.consume_history.get(&collab_object.object_id) {
+    if let Some(entry) = self.consume_history.get(object_id) {
       if entry.value() == &content_hash {
         trace!(
-          "[Indexing] {} instant embedding already indexed, skipping",
-          collab_object.object_id
+          "[Indexing] {} instant embedding already indexed, hash:{}, skipping",
+          object_id,
+          content_hash,
         );
         return Ok(false);
       }
     }
 
-    self
-      .consume_history
-      .insert(collab_object.object_id.clone(), content_hash);
+    self.consume_history.insert(*object_id, content_hash);
 
     #[cfg(any(target_os = "windows", target_os = "macos", target_os = "linux"))]
     {
       use flowy_ai::embeddings::context::EmbedContext;
       if let Ok(scheduler) = EmbedContext::shared().get_scheduler() {
         let unindex_collab = UnindexedCollab {
-          workspace_id: Uuid::parse_str(&collab_object.workspace_id)?,
-          object_id: Uuid::parse_str(&collab_object.object_id)?,
-          collab_type: collab_object.collab_type,
+          workspace_id: *workspace_id,
+          object_id: *object_id,
+          collab_type,
           data,
-          metadata: UnindexedCollabMetadata {
-            name: "".to_string(),
-            icon: None,
-          },
+          metadata: UnindexedCollabMetadata::default(),
         };
 
         if let Err(err) = scheduler.index_collab(unindex_collab).await {
@@ -225,7 +223,7 @@ impl FullIndexedDataConsumer for SearchFullIndexConsumer {
 pub struct SearchInstantIndexImpl {
   workspace_id: Uuid,
   state: Weak<RwLock<DocumentTantivyState>>,
-  consume_history: DashMap<String, String>,
+  consume_history: DashMap<Uuid, String>,
   folder_manager: Weak<FolderManager>,
   #[allow(dead_code)]
   // bind the folder_observer lifetime to the SearchInstantIndexImpl
@@ -290,7 +288,7 @@ impl SearchInstantIndexImpl {
                 if let Some(view) = views_map.get(&view_id) {
                   let _ = state.write().await.add_document_metadata(
                     &view.id,
-                    view.name.clone(),
+                    Some(view.name.clone()),
                     view.icon.clone().map(|v| ViewIcon {
                       ty: IconType::from(v.ty as u8),
                       value: v.value,
@@ -324,10 +322,12 @@ impl InstantIndexedDataConsumer for SearchInstantIndexImpl {
 
   async fn consume_collab(
     &self,
-    collab_object: &CollabObject,
+    workspace_id: &Uuid,
     data: UnindexedData,
+    object_id: &Uuid,
+    _collab_type: CollabType,
   ) -> Result<bool, FlowyError> {
-    if self.workspace_id.to_string() != collab_object.workspace_id {
+    if self.workspace_id != *workspace_id {
       return Ok(false);
     }
 
@@ -340,7 +340,7 @@ impl InstantIndexedDataConsumer for SearchInstantIndexImpl {
       .folder_manager
       .upgrade()
       .ok_or_else(FlowyError::ref_drop)?;
-    let view = folder_manager.get_view(&collab_object.object_id).await?;
+    let view = folder_manager.get_view(&object_id.to_string()).await?;
 
     // Create a combined hash that includes content + view name + icon
     let content_hash = data.content_hash();
@@ -351,20 +351,26 @@ impl InstantIndexedDataConsumer for SearchInstantIndexImpl {
       name_hash
     };
 
-    if let Some(entry) = self.consume_history.get(&collab_object.object_id) {
+    if let Some(entry) = self.consume_history.get(object_id) {
+      if entry.value() == &content_hash {
+        trace!(
+          "[Indexing] {} instant search already indexed, hash:{}, skipping",
+          object_id,
+          content_hash,
+        );
+        return Ok(false);
+      }
+
       if entry.value() == &combined_hash {
         return Ok(false);
       }
     }
 
-    self
-      .consume_history
-      .insert(collab_object.object_id.clone(), combined_hash);
-
+    self.consume_history.insert(*object_id, combined_hash);
     state.write().await.add_document(
-      &collab_object.object_id,
+      &object_id.to_string(),
       data.into_string(),
-      view.name.clone(),
+      Some(view.name.clone()),
       view.icon.clone().map(|v| ViewIcon {
         ty: IconType::from(v.ty as u8),
         value: v.value,
diff --git a/frontend/rust-lib/flowy-core/src/indexing_data_runner.rs b/frontend/rust-lib/flowy-core/src/indexing_data_runner.rs
index 5b33dc3be8b49..4b8d659ef0606 100644
--- a/frontend/rust-lib/flowy-core/src/indexing_data_runner.rs
+++ b/frontend/rust-lib/flowy-core/src/indexing_data_runner.rs
@@ -1,5 +1,5 @@
 use crate::app_life_cycle::AppLifeCycleImpl;
-use crate::full_indexed_data_provider::FullIndexedDataProvider;
+use crate::full_indexed_data_provider::FullIndexedDataWriter;
 use crate::indexed_data_consumer::{
   get_or_init_document_tantivy_state, EmbeddingsInstantConsumerImpl, SearchFullIndexConsumer,
   SearchInstantIndexImpl,
@@ -56,7 +56,7 @@ impl AppLifeCycleImpl {
     _user_config: &UserConfig,
     user_paths: &UserPaths,
   ) {
-    let instant_indexed_data_provider = self.instant_indexed_data_provider.clone();
+    let instant_indexed_data_provider = self.instant_indexed_data_writer.clone();
     let runtime = self.runtime.clone();
     let workspace_id_cloned = *workspace_id;
     let workspace_type_cloned = *workspace_type;
@@ -150,7 +150,7 @@ impl AppLifeCycleImpl {
   ) {
     let folder_manager = self.folder_manager.clone();
     let logged_user = self.logged_user.clone();
-    let full_indexed_data_provider = self.full_indexed_data_provider.clone();
+    let full_indexed_data_writer = self.full_indexed_data_writer.clone();
     let workspace_id_cloned = *workspace_id;
     let workspace_type_cloned = *workspace_type;
     let user_paths = user_paths.clone();
@@ -167,7 +167,7 @@ impl AppLifeCycleImpl {
         workspace_id_cloned
       );
 
-      let new_provider = FullIndexedDataProvider::new(
+      let new_full_indexed_data_writer = FullIndexedDataWriter::new(
         workspace_id_cloned,
         folder_manager,
         Arc::downgrade(&logged_user),
@@ -175,7 +175,7 @@ impl AppLifeCycleImpl {
       #[cfg(any(target_os = "linux", target_os = "macos", target_os = "windows"))]
       {
         if workspace_type_cloned.is_local() {
-          new_provider
+          new_full_indexed_data_writer
             .register_full_indexed_consumer(Box::new(
               crate::indexed_data_consumer::EmbeddingFullIndexConsumer,
             ))
@@ -185,7 +185,7 @@ impl AppLifeCycleImpl {
 
       match SearchFullIndexConsumer::new(&workspace_id_cloned, user_paths.tanvity_index_path(uid)) {
         Ok(consumer) => {
-          new_provider
+          new_full_indexed_data_writer
             .register_full_indexed_consumer(Box::new(consumer))
             .await;
         },
@@ -195,13 +195,16 @@ impl AppLifeCycleImpl {
         ),
       }
 
-      if new_provider.num_consumers().await > 0 {
+      if new_full_indexed_data_writer.num_consumers().await > 0 {
         info!(
           "[Indexing] full indexed data provider with {} consumers for workspace: {:?}",
-          new_provider.num_consumers().await,
+          new_full_indexed_data_writer.num_consumers().await,
           workspace_id_cloned
         );
-        match new_provider.full_index_unindexed_documents().await {
+        match new_full_indexed_data_writer
+          .full_index_unindexed_documents()
+          .await
+        {
           Ok(()) => {
             info!("[Indexing] full index succeeded");
           },
@@ -213,8 +216,8 @@ impl AppLifeCycleImpl {
         info!("[Indexing] full indexed data provider stopped");
       }
 
-      if let Some(provider) = full_indexed_data_provider.upgrade() {
-        let old = provider.write().await.replace(new_provider);
+      if let Some(writer) = full_indexed_data_writer.upgrade() {
+        let old = writer.write().await.replace(new_full_indexed_data_writer);
         if let Some(old) = old {
           old.cancel_indexing();
         }
diff --git a/frontend/rust-lib/flowy-core/src/lib.rs b/frontend/rust-lib/flowy-core/src/lib.rs
index 19c5868a457aa..0ac8ff6e1542e 100644
--- a/frontend/rust-lib/flowy-core/src/lib.rs
+++ b/frontend/rust-lib/flowy-core/src/lib.rs
@@ -1,7 +1,7 @@
 #![allow(unused_doc_comments)]
 
 use collab_integrate::collab_builder::AppFlowyCollabBuilder;
-use collab_integrate::instant_indexed_data_provider::InstantIndexedDataProvider;
+use collab_integrate::instant_indexed_data_provider::InstantIndexedDataWriter;
 use collab_plugins::CollabKVDB;
 use flowy_ai::ai_manager::AIManager;
 use flowy_database2::DatabaseManager;
@@ -33,7 +33,7 @@ use module::make_plugins;
 use crate::config::AppFlowyCoreConfig;
 use crate::deps_resolve::file_storage_deps::FileStorageResolver;
 use crate::deps_resolve::*;
-use crate::full_indexed_data_provider::FullIndexedDataProvider;
+use crate::full_indexed_data_provider::FullIndexedDataWriter;
 use crate::log_filter::init_log;
 use crate::server_layer::ServerProvider;
 use app_life_cycle::AppLifeCycleImpl;
@@ -72,7 +72,7 @@ pub struct AppFlowyCore {
   pub ai_manager: Arc<AIManager>,
   pub storage_manager: Arc<StorageManager>,
   pub collab_builder: Arc<AppFlowyCollabBuilder>,
-  pub indexed_data_provider: Arc<RwLock<Option<FullIndexedDataProvider>>>,
+  pub full_indexed_data_writer: Arc<RwLock<Option<FullIndexedDataWriter>>>,
 }
 
 impl Drop for AppFlowyCore {
@@ -149,11 +149,17 @@ impl AppFlowyCore {
     ));
 
     debug!("🔥runtime:{}", runtime);
+    let instant_indexed_data_writer = if get_operating_system().is_desktop() {
+      Some(Arc::new(InstantIndexedDataWriter::new()))
+    } else {
+      None
+    };
 
     let server_provider = Arc::new(ServerProvider::new(
       config.clone(),
       Arc::downgrade(&store_preference),
       ServerUserImpl(Arc::downgrade(&authenticate_user)),
+      instant_indexed_data_writer.as_ref().map(Arc::downgrade),
     ));
 
     event!(tracing::Level::DEBUG, "Init managers",);
@@ -167,7 +173,7 @@ impl AppFlowyCore {
       search_manager,
       ai_manager,
       storage_manager,
-      instant_indexed_data_provider,
+      instant_indexed_data_writer,
     ) = async {
       let storage_manager = FileStorageResolver::resolve(
         Arc::downgrade(&authenticate_user),
@@ -175,17 +181,12 @@ impl AppFlowyCore {
         &user_config.storage_path,
       );
 
-      let instant_indexed_data_provider = if get_operating_system().is_desktop() {
-        Some(Arc::new(InstantIndexedDataProvider::new()))
-      } else {
-        None
-      };
       /// The shared collab builder is used to build the [Collab] instance. The plugins will be loaded
       /// on demand based on the [CollabPluginConfig].
       let collab_builder = Arc::new(AppFlowyCollabBuilder::new(
         server_provider.clone(),
         WorkspaceCollabIntegrateImpl(Arc::downgrade(&authenticate_user)),
-        instant_indexed_data_provider.as_ref().map(Arc::downgrade),
+        instant_indexed_data_writer.as_ref().map(Arc::downgrade),
       ));
 
       collab_builder
@@ -262,12 +263,12 @@ impl AppFlowyCore {
         search_manager,
         ai_manager,
         storage_manager,
-        instant_indexed_data_provider,
+        instant_indexed_data_writer,
       )
     }
     .await;
 
-    let indexed_data_provider = Arc::new(RwLock::new(None));
+    let full_indexed_data_writer = Arc::new(RwLock::new(None));
     let (full_indexed_finish_sender, _) = tokio::sync::watch::channel(false);
     let app_life_cycle = AppLifeCycleImpl {
       user_manager: Arc::downgrade(&user_manager),
@@ -279,8 +280,8 @@ impl AppFlowyCore {
       storage_manager: Arc::downgrade(&storage_manager),
       ai_manager: Arc::downgrade(&ai_manager),
       search_manager: Arc::downgrade(&search_manager),
-      instant_indexed_data_provider,
-      full_indexed_data_provider: Arc::downgrade(&indexed_data_provider),
+      instant_indexed_data_writer,
+      full_indexed_data_writer: Arc::downgrade(&full_indexed_data_writer),
       logged_user: Arc::new(ServerUserImpl(Arc::downgrade(&authenticate_user))),
       runtime: runtime.clone(),
       full_indexed_finish_sender,
@@ -325,7 +326,7 @@ impl AppFlowyCore {
       ai_manager,
       storage_manager,
       collab_builder,
-      indexed_data_provider,
+      full_indexed_data_writer,
     }
   }
 
diff --git a/frontend/rust-lib/flowy-core/src/log_filter.rs b/frontend/rust-lib/flowy-core/src/log_filter.rs
index 6704ad0507e30..863b3e0c3561d 100644
--- a/frontend/rust-lib/flowy-core/src/log_filter.rs
+++ b/frontend/rust-lib/flowy-core/src/log_filter.rs
@@ -60,7 +60,9 @@ pub fn create_log_filter(
   filters.push(format!("af_local_ai={}", level));
   filters.push(format!("af_plugin={}", level));
   filters.push(format!("flowy_ai={}", level));
+  filters.push(format!("flowy_ai_pub={}", level));
   filters.push(format!("flowy_storage={}", level));
+  filters.push(format!("flowy_sqlite_vec={}", level));
   // Enable the frontend logs. DO NOT DISABLE.
   // These logs are essential for debugging and verifying frontend behavior.
   filters.push(format!("dart_ffi={}", level));
diff --git a/frontend/rust-lib/flowy-core/src/server_layer.rs b/frontend/rust-lib/flowy-core/src/server_layer.rs
index f23d504c56625..f8c1d26fc591f 100644
--- a/frontend/rust-lib/flowy-core/src/server_layer.rs
+++ b/frontend/rust-lib/flowy-core/src/server_layer.rs
@@ -1,20 +1,26 @@
 use crate::AppFlowyCoreConfig;
 use arc_swap::{ArcSwap, ArcSwapOption};
+use collab::entity::EncodedCollab;
+use collab_entity::CollabType;
+use collab_integrate::instant_indexed_data_provider::InstantIndexedDataWriter;
 use dashmap::mapref::one::Ref;
 use dashmap::DashMap;
 use flowy_ai::local_ai::controller::LocalAIController;
+use flowy_ai_pub::entities::UnindexedCollab;
 use flowy_error::{FlowyError, FlowyResult};
 use flowy_server::af_cloud::define::AIUserServiceImpl;
 use flowy_server::af_cloud::{define::LoggedUser, AppFlowyCloudServer};
 use flowy_server::local_server::LocalServer;
-use flowy_server::{AppFlowyEncryption, AppFlowyServer, EncryptionImpl};
+use flowy_server::{AppFlowyEncryption, AppFlowyServer, EmbeddingWriter, EncryptionImpl};
 use flowy_server_pub::AuthenticatorType;
 use flowy_sqlite::kv::KVStorePreferences;
 use flowy_user_pub::entities::*;
+use lib_infra::async_trait::async_trait;
 use std::ops::Deref;
 use std::sync::atomic::{AtomicBool, Ordering};
 use std::sync::{Arc, Weak};
 use tracing::info;
+use uuid::Uuid;
 
 pub struct ServerProvider {
   config: AppFlowyCoreConfig,
@@ -25,6 +31,7 @@ pub struct ServerProvider {
   pub uid: Arc<ArcSwapOption<i64>>,
   pub user_enable_sync: Arc<AtomicBool>,
   pub encryption: Arc<dyn AppFlowyEncryption>,
+  pub indexed_data_writer: Option<Weak<InstantIndexedDataWriter>>,
 }
 
 // Our little guard wrapper:
@@ -53,6 +60,7 @@ impl ServerProvider {
     config: AppFlowyCoreConfig,
     store_preferences: Weak<KVStorePreferences>,
     user_service: impl LoggedUser + 'static,
+    indexed_data_writer: Option<Weak<InstantIndexedDataWriter>>,
   ) -> Self {
     let initial_auth = current_server_type();
     let logged_user = Arc::new(user_service) as Arc<dyn LoggedUser>;
@@ -70,6 +78,7 @@ impl ServerProvider {
       logged_user,
       uid: Default::default(),
       local_ai,
+      indexed_data_writer,
     }
   }
 
@@ -107,10 +116,18 @@ impl ServerProvider {
     }
 
     let server: Arc<dyn AppFlowyServer> = match auth_type {
-      AuthType::Local => Arc::new(LocalServer::new(
-        self.logged_user.clone(),
-        self.local_ai.clone(),
-      )),
+      AuthType::Local => {
+        let embedding_writer = self.indexed_data_writer.clone().map(|w| {
+          Arc::new(EmbeddingWriterImpl {
+            indexed_data_writer: w,
+          }) as Arc<dyn EmbeddingWriter>
+        });
+        Arc::new(LocalServer::new(
+          self.logged_user.clone(),
+          self.local_ai.clone(),
+          embedding_writer,
+        ))
+      },
       AuthType::AppFlowyCloud => {
         let cfg = self
           .config
@@ -134,3 +151,34 @@ impl ServerProvider {
     Ok(ServerHandle(guard))
   }
 }
+
+struct EmbeddingWriterImpl {
+  indexed_data_writer: Weak<InstantIndexedDataWriter>,
+}
+
+#[async_trait]
+impl EmbeddingWriter for EmbeddingWriterImpl {
+  async fn index_encoded_collab(
+    &self,
+    workspace_id: Uuid,
+    object_id: Uuid,
+    data: EncodedCollab,
+    collab_type: CollabType,
+  ) -> FlowyResult<()> {
+    let indexed_data_writer = self.indexed_data_writer.upgrade().ok_or_else(|| {
+      FlowyError::internal().with_context("Failed to upgrade InstantIndexedDataWriter")
+    })?;
+    indexed_data_writer
+      .index_encoded_collab(workspace_id, object_id, data, collab_type)
+      .await?;
+    Ok(())
+  }
+
+  async fn index_unindexed_collab(&self, data: UnindexedCollab) -> FlowyResult<()> {
+    let indexed_data_writer = self.indexed_data_writer.upgrade().ok_or_else(|| {
+      FlowyError::internal().with_context("Failed to upgrade InstantIndexedDataWriter")
+    })?;
+    indexed_data_writer.index_unindexed_collab(data).await?;
+    Ok(())
+  }
+}
diff --git a/frontend/rust-lib/flowy-search/src/document/cloud_search_handler.rs b/frontend/rust-lib/flowy-search/src/document/cloud_search_handler.rs
index ec9a4e1d09fe0..358940225f4c6 100644
--- a/frontend/rust-lib/flowy-search/src/document/cloud_search_handler.rs
+++ b/frontend/rust-lib/flowy-search/src/document/cloud_search_handler.rs
@@ -74,7 +74,7 @@ impl SearchHandler for DocumentCloudSearchHandler {
           return;
         }
       };
-      trace!("[Search] search result: {:?}", result_items);
+      trace!("[Search] ai search result: {:?}", result_items);
 
       // Prepare input for search summary generation.
       let summary_input: Vec<SearchResult> = result_items
diff --git a/frontend/rust-lib/flowy-search/src/document/local_search_handler.rs b/frontend/rust-lib/flowy-search/src/document/local_search_handler.rs
index c485d5c909b5a..be8e6dcb5557b 100644
--- a/frontend/rust-lib/flowy-search/src/document/local_search_handler.rs
+++ b/frontend/rust-lib/flowy-search/src/document/local_search_handler.rs
@@ -7,7 +7,7 @@ use std::pin::Pin;
 use std::sync::Weak;
 use tantivy::directory::MmapDirectory;
 use tantivy::schema::Value;
-use tantivy::{doc, Index, IndexReader, IndexWriter, TantivyDocument, Term};
+use tantivy::{Index, IndexReader, IndexWriter, TantivyDocument, Term};
 use tokio::sync::RwLock;
 use tracing::{error, trace, warn};
 use uuid::Uuid;
@@ -162,27 +162,86 @@ impl DocumentTantivyState {
     &mut self,
     id: &str,
     content: String,
-    name: String,
+    name: Option<String>,
     icon: Option<ViewIcon>,
   ) -> FlowyResult<()> {
-    trace!("[Tantivy] Adding document with id:{}, name:{}", id, name);
+    trace!("[Tantivy] Adding document with id:{}, name:{:?}", id, name);
     let term = Term::from_field_text(self.field_object_id, id);
+    let searcher = self.reader.searcher();
+    let query =
+      tantivy::query::TermQuery::new(term.clone(), tantivy::schema::IndexRecordOption::Basic);
+    let top_docs = searcher.search(&query, &tantivy::collector::TopDocs::with_limit(1))?;
+
+    let (existing_name, existing_icon) = if let Some((_score, doc_address)) = top_docs.first() {
+      let retrieved: TantivyDocument = searcher.doc(*doc_address)?;
+
+      // Get existing name if needed
+      let existing_name = if name.is_none() {
+        retrieved
+          .get_first(self.field_name)
+          .and_then(|v| v.as_str())
+          .map(|s| s.to_string())
+      } else {
+        None
+      };
+
+      // Get existing icon if needed
+      let existing_icon = if icon.is_none() {
+        let icon_value = retrieved
+          .get_first(self.field_icon)
+          .and_then(|v| v.as_str())
+          .map(|s| s.to_string());
+
+        if let Some(icon_value) = icon_value {
+          let icon_type_str = retrieved
+            .get_first(self.field_icon_type)
+            .and_then(|v| v.as_str())
+            .unwrap_or_default();
+
+          let icon_type = icon_type_str.parse::<u8>().unwrap_or_default();
+
+          // Recreate the ViewIcon from stored values
+          Some(ViewIcon {
+            value: icon_value,
+            ty: icon_type.into(),
+          })
+        } else {
+          None
+        }
+      } else {
+        None
+      };
+
+      (existing_name, existing_icon)
+    } else {
+      (None, None)
+    };
+
     // Delete existing document with same ID
     self.writer.delete_term(term);
 
-    let (icon, icon_type) = icon.map(|v| (v.value, v.ty as u8)).unwrap_or_default();
+    // Use existing values if new ones not provided
+    let final_name = name.or(existing_name);
+    let final_icon = icon.or(existing_icon);
+
+    // Ensure we have a name
+    let document_name = final_name.unwrap_or_else(|| String::from("Untitled"));
 
-    // Create document with cached fields
-    let tantivy_doc = doc!(
+    // Create base document with required fields
+    let mut doc_builder = tantivy::doc!(
         self.field_workspace_id => self.workspace_id.to_string(),
         self.field_object_id => id,
         self.field_content => content,
-        self.field_name => name,
-        self.field_icon => icon,
-        self.field_icon_type => icon_type.to_string()
+        self.field_name => document_name
     );
 
-    self.writer.add_document(tantivy_doc)?;
+    // Only add icon fields if icon is present
+    if let Some(view_icon) = final_icon {
+      doc_builder.add_text(self.field_icon, view_icon.value);
+      doc_builder.add_text(self.field_icon_type, (view_icon.ty as u8).to_string());
+    }
+
+    self.writer.add_document(doc_builder)?;
     self.writer.commit()?;
 
     Ok(())
@@ -191,7 +250,7 @@ impl DocumentTantivyState {
   pub fn add_document_metadata(
     &mut self,
     id: &str,
-    name: String,
+    name: Option<String>,
     icon: Option<ViewIcon>,
   ) -> FlowyResult<()> {
     let term = Term::from_field_text(self.field_object_id, id);
@@ -201,17 +260,86 @@ impl DocumentTantivyState {
 
     // Search for the document
     let top_docs = searcher.search(&query, &tantivy::collector::TopDocs::with_limit(1))?;
-    let content = if let Some((_score, doc_address)) = top_docs.first() {
-      let retrieved: TantivyDocument = searcher.doc(*doc_address)?;
-      retrieved
-        .get_first(self.field_content)
-        .and_then(|v| v.as_str())
-        .unwrap_or_default()
-        .to_string()
-    } else {
-      String::new()
-    };
-    self.add_document(id, content, name, icon)?;
+
+    let (existing_content, existing_name, existing_icon) =
+      if let Some((_score, doc_address)) = top_docs.first() {
+        let retrieved: TantivyDocument = searcher.doc(*doc_address)?;
+
+        // Get existing content
+        let content = retrieved
+          .get_first(self.field_content)
+          .and_then(|v| v.as_str())
+          .unwrap_or_default()
+          .to_string();
+
+        // Get existing name if needed
+        let existing_name = if name.is_none() {
+          retrieved
+            .get_first(self.field_name)
+            .and_then(|v| v.as_str())
+            .map(|s| s.to_string())
+        } else {
+          None
+        };
+
+        // Get existing icon if needed
+        let existing_icon = if icon.is_none() {
+          let icon_value = retrieved
+            .get_first(self.field_icon)
+            .and_then(|v| v.as_str())
+            .map(|s| s.to_string());
+
+          if let Some(icon_value) = icon_value {
+            let icon_type_str = retrieved
+              .get_first(self.field_icon_type)
+              .and_then(|v| v.as_str())
+              .unwrap_or_default();
+
+            let icon_type = icon_type_str.parse::<u8>().unwrap_or_default();
+
+            Some(ViewIcon {
+              value: icon_value,
+              ty: icon_type.into(),
+            })
+          } else {
+            None
+          }
+        } else {
+          None
+        };
+
+        (content, existing_name, existing_icon)
+      } else {
+        (String::new(), None, None)
+      };
+
+    // Use existing values if new ones not provided
+    let final_name = name.or(existing_name);
+    let final_icon = icon.or(existing_icon);
+
+    // Ensure we have a name
+    let document_name = final_name.unwrap_or_else(|| String::from("Untitled"));
+
+    // Delete existing document
+    self.writer.delete_term(term);
+
+    // Create base document with required fields
+    let mut doc_builder = tantivy::doc!(
+        self.field_workspace_id => self.workspace_id.to_string(),
+        self.field_object_id => id,
+        self.field_content => existing_content,
+        self.field_name => document_name
+    );
+
+    // Only add icon fields if icon is present
+    if let Some(view_icon) = final_icon {
+      doc_builder.add_text(self.field_icon, view_icon.value);
+      doc_builder.add_text(self.field_icon_type, (view_icon.ty as u8).to_string());
+    }
+
+    self.writer.add_document(doc_builder)?;
+    self.writer.commit()?;
+
     Ok(())
   }
 
@@ -314,23 +442,24 @@ impl DocumentTantivyState {
           .unwrap_or_default()
           .to_string();
 
-        let icon_type_str = retrieved
-          .get_first(self.field_icon_type)
-          .and_then(|v| v.as_str())
-          .unwrap_or_default();
+        // Only proceed with creating an icon if we have an actual value
+        if !icon_value.is_empty() {
+          let icon_type_str = retrieved
+            .get_first(self.field_icon_type)
+            .and_then(|v| v.as_str())
+            .unwrap_or_default();
 
-        let icon_type: ResultIconTypePB = match icon_type_str.parse::<i64>() {
-          Ok(val) => val.into(),
-          Err(_) => ResultIconTypePB::default(),
-        };
+          let icon_type: ResultIconTypePB = match icon_type_str.parse::<i64>() {
+            Ok(val) => val.into(),
+            Err(_) => ResultIconTypePB::default(),
+          };
 
-        if icon_value.is_empty() {
-          None
-        } else {
           Some(ResultIconPB {
             ty: icon_type,
             value: icon_value,
           })
+        } else {
+          None
         }
       };
 
diff --git a/frontend/rust-lib/flowy-server/src/af_cloud/impls/chat.rs b/frontend/rust-lib/flowy-server/src/af_cloud/impls/chat.rs
index 89355e2ee4ab6..0faf3ee2c98d2 100644
--- a/frontend/rust-lib/flowy-server/src/af_cloud/impls/chat.rs
+++ b/frontend/rust-lib/flowy-server/src/af_cloud/impls/chat.rs
@@ -99,9 +99,9 @@ where
     &self,
     workspace_id: &Uuid,
     chat_id: &Uuid,
-    message_id: i64,
+    question_id: i64,
     format: ResponseFormat,
-    ai_model: Option<AIModel>,
+    ai_model: AIModel,
   ) -> Result<StreamAnswer, FlowyError> {
     trace!(
       "stream_answer: workspace_id={}, chat_id={}, format={:?}, model: {:?}",
@@ -116,10 +116,10 @@ where
         workspace_id,
         ChatQuestionQuery {
           chat_id: chat_id.to_string(),
-          question_id: message_id,
+          question_id,
           format,
         },
-        ai_model.map(|v| v.name),
+        Some(ai_model.name),
       )
       .await;
 
diff --git a/frontend/rust-lib/flowy-server/src/local_server/impls/chat.rs b/frontend/rust-lib/flowy-server/src/local_server/impls/chat.rs
index 8476799be99cc..da5e603c9ab7c 100644
--- a/frontend/rust-lib/flowy-server/src/local_server/impls/chat.rs
+++ b/frontend/rust-lib/flowy-server/src/local_server/impls/chat.rs
@@ -1,4 +1,5 @@
 use crate::af_cloud::define::LoggedUser;
+use crate::local_server::uid::IDGenerator;
 use chrono::{TimeZone, Utc};
 use client_api::entity::ai_dto::RepeatedRelatedQuestion;
 use flowy_ai::local_ai::controller::LocalAIController;
@@ -15,15 +16,21 @@ use flowy_ai_pub::persistence::{
   ChatMessageTable, ChatTable, ChatTableChangeset,
 };
 use flowy_error::{FlowyError, FlowyResult};
+use lazy_static::lazy_static;
 use lib_infra::async_trait::async_trait;
 use lib_infra::util::timestamp;
 use serde_json::{json, Value};
 use std::collections::HashMap;
 use std::path::Path;
 use std::sync::Arc;
+use tokio::sync::Mutex;
 use tracing::trace;
 use uuid::Uuid;
 
+lazy_static! {
+  static ref ID_GEN: Mutex<IDGenerator> = Mutex::new(IDGenerator::new(2));
+}
+
 pub struct LocalChatServiceImpl {
   pub logged_user: Arc<dyn LoggedUser>,
   pub local_ai: Arc<LocalAIController>,
@@ -73,9 +80,10 @@ impl ChatCloudService for LocalChatServiceImpl {
     message: &str,
     message_type: ChatMessageType,
   ) -> Result<ChatMessage, FlowyError> {
+    let message_id = ID_GEN.lock().await.next_id();
     let message = match message_type {
-      ChatMessageType::System => ChatMessage::new_system(timestamp(), message.to_string()),
-      ChatMessageType::User => ChatMessage::new_human(timestamp(), message.to_string(), None),
+      ChatMessageType::System => ChatMessage::new_system(message_id, message.to_string()),
+      ChatMessageType::User => ChatMessage::new_human(message_id, message.to_string(), None),
     };
 
     self.upsert_message(chat_id, message.clone()).await?;
@@ -102,15 +110,15 @@ impl ChatCloudService for LocalChatServiceImpl {
     &self,
     _workspace_id: &Uuid,
     chat_id: &Uuid,
-    message_id: i64,
+    question_id: i64,
     format: ResponseFormat,
-    _ai_model: Option<AIModel>,
+    ai_model: AIModel,
   ) -> Result<StreamAnswer, FlowyError> {
     if self.local_ai.is_ready().await {
-      let content = self.get_message_content(message_id)?;
+      let content = self.get_message_content(question_id)?;
       self
         .local_ai
-        .stream_question(chat_id, &content, format)
+        .stream_question(chat_id, &content, format, &ai_model.name)
         .await
     } else {
       Err(FlowyError::local_ai_disabled())
diff --git a/frontend/rust-lib/flowy-server/src/local_server/impls/folder.rs b/frontend/rust-lib/flowy-server/src/local_server/impls/folder.rs
index 79b1d4be123d7..fcb7e929cb8ea 100644
--- a/frontend/rust-lib/flowy-server/src/local_server/impls/folder.rs
+++ b/frontend/rust-lib/flowy-server/src/local_server/impls/folder.rs
@@ -2,6 +2,7 @@
 
 use crate::af_cloud::define::LoggedUser;
 use crate::local_server::util::default_encode_collab_for_collab_type;
+use crate::EmbeddingWriter;
 use client_api::entity::workspace_dto::PublishInfoView;
 use client_api::entity::PublishInfo;
 use collab::core::origin::CollabOrigin;
@@ -21,6 +22,7 @@ use uuid::Uuid;
 pub(crate) struct LocalServerFolderCloudServiceImpl {
   #[allow(dead_code)]
   pub logged_user: Arc<dyn LoggedUser>,
+  pub embedding_writer: Option<Arc<dyn EmbeddingWriter>>,
 }
 
 #[async_trait]
@@ -68,6 +70,17 @@ impl FolderCloudService for LocalServerFolderCloudServiceImpl {
     workspace_id: &Uuid,
     params: FullSyncCollabParams,
   ) -> Result<(), FlowyError> {
+    if let Some(embedding_writer) = self.embedding_writer.as_ref() {
+      embedding_writer
+        .index_encoded_collab(
+          *workspace_id,
+          params.object_id,
+          params.encoded_collab,
+          params.collab_type,
+        )
+        .await?;
+    }
+
     Ok(())
   }
 
diff --git a/frontend/rust-lib/flowy-server/src/local_server/impls/user.rs b/frontend/rust-lib/flowy-server/src/local_server/impls/user.rs
index a461e1dc608e1..bf10fe2e0c51a 100644
--- a/frontend/rust-lib/flowy-server/src/local_server/impls/user.rs
+++ b/frontend/rust-lib/flowy-server/src/local_server/impls/user.rs
@@ -4,7 +4,7 @@ use crate::af_cloud::define::LoggedUser;
 use crate::local_server::template::create_workspace::{
   create_workspace_for_user, CreateWorkspaceCollab,
 };
-use crate::local_server::uid::UserIDGenerator;
+use crate::local_server::uid::IDGenerator;
 use anyhow::Context;
 use client_api::entity::GotrueTokenResponse;
 use collab::core::origin::CollabOrigin;
@@ -35,7 +35,7 @@ use tokio::sync::Mutex;
 use uuid::Uuid;
 
 lazy_static! {
-  static ref ID_GEN: Mutex<UserIDGenerator> = Mutex::new(UserIDGenerator::new(1));
+  static ref ID_GEN: Mutex<IDGenerator> = Mutex::new(IDGenerator::new(1));
 }
 
 pub(crate) struct LocalServerUserServiceImpl {
diff --git a/frontend/rust-lib/flowy-server/src/local_server/server.rs b/frontend/rust-lib/flowy-server/src/local_server/server.rs
index 4e1103daf5c98..f80064c08b1cd 100644
--- a/frontend/rust-lib/flowy-server/src/local_server/server.rs
+++ b/frontend/rust-lib/flowy-server/src/local_server/server.rs
@@ -4,7 +4,7 @@ use crate::local_server::impls::{
   LocalServerDocumentCloudServiceImpl, LocalServerFolderCloudServiceImpl,
   LocalServerUserServiceImpl,
 };
-use crate::AppFlowyServer;
+use crate::{AppFlowyServer, EmbeddingWriter};
 use anyhow::Error;
 use flowy_ai::local_ai::controller::LocalAIController;
 use flowy_ai_pub::cloud::ChatCloudService;
@@ -21,14 +21,20 @@ pub struct LocalServer {
   logged_user: Arc<dyn LoggedUser>,
   local_ai: Arc<LocalAIController>,
   stop_tx: Option<mpsc::Sender<()>>,
+  embedding_writer: Option<Arc<dyn EmbeddingWriter>>,
 }
 
 impl LocalServer {
-  pub fn new(logged_user: Arc<dyn LoggedUser>, local_ai: Arc<LocalAIController>) -> Self {
+  pub fn new(
+    logged_user: Arc<dyn LoggedUser>,
+    local_ai: Arc<LocalAIController>,
+    embedding_writer: Option<Arc<dyn EmbeddingWriter>>,
+  ) -> Self {
     Self {
       logged_user,
       local_ai,
       stop_tx: Default::default(),
+      embedding_writer,
     }
   }
 
@@ -54,6 +60,7 @@ impl AppFlowyServer for LocalServer {
   fn folder_service(&self) -> Arc<dyn FolderCloudService> {
     Arc::new(LocalServerFolderCloudServiceImpl {
       logged_user: self.logged_user.clone(),
+      embedding_writer: self.embedding_writer.clone(),
     })
   }
 
diff --git a/frontend/rust-lib/flowy-server/src/local_server/uid.rs b/frontend/rust-lib/flowy-server/src/local_server/uid.rs
index a39252e379885..ea45582cc1054 100644
--- a/frontend/rust-lib/flowy-server/src/local_server/uid.rs
+++ b/frontend/rust-lib/flowy-server/src/local_server/uid.rs
@@ -7,15 +7,15 @@ const NODE_ID_SHIFT: u64 = SEQUENCE_BITS;
 const TIMESTAMP_SHIFT: u64 = NODE_ID_BITS + SEQUENCE_BITS;
 const SEQUENCE_MASK: u64 = (1 << SEQUENCE_BITS) - 1;
 
-pub struct UserIDGenerator {
+pub struct IDGenerator {
   node_id: u64,
   sequence: u64,
   last_timestamp: u64,
 }
 
-impl UserIDGenerator {
-  pub fn new(node_id: u64) -> UserIDGenerator {
-    UserIDGenerator {
+impl IDGenerator {
+  pub fn new(node_id: u64) -> IDGenerator {
+    IDGenerator {
       node_id,
       sequence: 0,
       last_timestamp: 0,
diff --git a/frontend/rust-lib/flowy-server/src/server.rs b/frontend/rust-lib/flowy-server/src/server.rs
index ed65971c95d32..ece06a17cf46c 100644
--- a/frontend/rust-lib/flowy-server/src/server.rs
+++ b/frontend/rust-lib/flowy-server/src/server.rs
@@ -7,15 +7,33 @@ use std::sync::Arc;
 use anyhow::Error;
 use arc_swap::ArcSwapOption;
 use client_api::collab_sync::ServerCollabMessage;
+use collab::entity::EncodedCollab;
+use collab_entity::CollabType;
 use flowy_ai_pub::cloud::ChatCloudService;
-use tokio_stream::wrappers::WatchStream;
-
+use flowy_ai_pub::entities::UnindexedCollab;
 use flowy_database_pub::cloud::{DatabaseAIService, DatabaseCloudService};
 use flowy_document_pub::cloud::DocumentCloudService;
+use flowy_error::FlowyResult;
 use flowy_folder_pub::cloud::FolderCloudService;
 use flowy_storage_pub::cloud::StorageCloudService;
 use flowy_user_pub::cloud::UserCloudService;
 use flowy_user_pub::entities::UserTokenState;
+use lib_infra::async_trait::async_trait;
+use tokio_stream::wrappers::WatchStream;
+use uuid::Uuid;
+
+#[async_trait]
+pub trait EmbeddingWriter: Send + Sync + 'static {
+  async fn index_encoded_collab(
+    &self,
+    workspace_id: Uuid,
+    object_id: Uuid,
+    data: EncodedCollab,
+    collab_type: CollabType,
+  ) -> FlowyResult<()>;
+
+  async fn index_unindexed_collab(&self, data: UnindexedCollab) -> FlowyResult<()>;
+}
 
 pub trait AppFlowyEncryption: Send + Sync + 'static {
   fn get_secret(&self) -> Option<String>;
diff --git a/frontend/rust-lib/flowy-sqlite-vec/src/db.rs b/frontend/rust-lib/flowy-sqlite-vec/src/db.rs
index 2a94f8c8b8c65..a3b9e5d4b8a99 100644
--- a/frontend/rust-lib/flowy-sqlite-vec/src/db.rs
+++ b/frontend/rust-lib/flowy-sqlite-vec/src/db.rs
@@ -1,4 +1,4 @@
-use crate::entities::PendingIndexedCollab;
+use crate::entities::{PendingIndexedCollab, SqliteEmbeddedDocument, SqliteEmbeddedFragment};
 use crate::init_sqlite_vector_extension;
 use crate::migration::init_sqlite_with_migrations;
 use anyhow::{Context, Result};
@@ -9,6 +9,7 @@ use rusqlite::{params, ToSql};
 use serde_json::Value;
 use std::collections::{HashMap, HashSet};
 use std::path::PathBuf;
+use tracing::{trace, warn};
 use uuid::Uuid;
 use zerocopy::IntoBytes;
 
@@ -108,6 +109,91 @@ impl VectorSqliteDB {
     Ok(())
   }
 
+  pub async fn select_all_embedded_documents(
+    &self,
+    workspace_id: &str,
+    rag_ids: &[String],
+  ) -> Result<Vec<SqliteEmbeddedDocument>> {
+    let conn = self
+      .pool
+      .get()
+      .context("Failed to get connection from pool")?;
+
+    // Build SQL query based on whether rag_ids are provided
+    let (sql, params) = if rag_ids.is_empty() {
+      // No rag_ids provided, select all documents for workspace
+      let sql = "SELECT object_id, content, embedding 
+                FROM af_collab_embeddings 
+                WHERE workspace_id = ?";
+      let params: Vec<&dyn ToSql> = vec![&workspace_id];
+      (sql.to_string(), params)
+    } else {
+      // Filter by provided rag_ids
+      let placeholders = std::iter::repeat("?")
+        .take(rag_ids.len())
+        .collect::<Vec<_>>()
+        .join(", ");
+
+      let sql = format!(
+        "SELECT object_id, content, embedding 
+         FROM af_collab_embeddings 
+         WHERE workspace_id = ? AND object_id IN ({})",
+        placeholders
+      );
+
+      let mut params: Vec<&dyn ToSql> = vec![&workspace_id];
+      params.extend(rag_ids.iter().map(|id| id as &dyn ToSql));
+      (sql, params)
+    };
+
+    let mut stmt = conn.prepare(&sql)?;
+    let mut rows = stmt.query(params.as_slice())?;
+
+    // Group results by object_id
+    let mut documents_map: HashMap<String, Vec<SqliteEmbeddedFragment>> = HashMap::new();
+
+    while let Some(row) = rows.next()? {
+      let object_id: String = row.get(0)?;
+      let content: String = row.get(1)?;
+
+      // Convert embedding blob to Vec<f32>
+      let embedding_blob: Vec<u8> = row.get(2)?;
+      let embeddings = if !embedding_blob.is_empty() {
+        // Convert bytes to Vec<f32> - each f32 is 4 bytes
+        let mut vec = Vec::with_capacity(embedding_blob.len() / 4);
+        for chunk in embedding_blob.chunks_exact(4) {
+          if let Ok(array) = chunk.try_into() {
+            vec.push(f32::from_le_bytes(array));
+          }
+        }
+        vec
+      } else {
+        Vec::new()
+      };
+
+      // Add fragment to the corresponding object_id
+      documents_map
+        .entry(object_id)
+        .or_default()
+        .push(SqliteEmbeddedFragment {
+          content,
+          embeddings,
+        });
+    }
+
+    // Convert the map to the required Vec<SqliteEmbeddedDocument>
+    let documents = documents_map
+      .into_iter()
+      .map(|(object_id, fragments)| SqliteEmbeddedDocument {
+        workspace_id: workspace_id.to_string(),
+        object_id,
+        fragments,
+      })
+      .collect();
+
+    Ok(documents)
+  }
+
   /// Inserts or replaces all of `fragments` for the given (workspace_id, object_id),
   /// deleting anything else in that scope first, and storing the new vector blobs.
   pub async fn upsert_collabs_embeddings(
@@ -120,6 +206,13 @@ impl VectorSqliteDB {
       return Ok(());
     }
 
+    trace!(
+      "[VectorStore] workspace:{} upserting {} fragments for {}",
+      workspace_id,
+      fragments.len(),
+      object_id
+    );
+
     let mut conn = self
       .pool
       .get()
@@ -139,6 +232,7 @@ impl VectorSqliteDB {
     let existing_ids: HashSet<String> = stmt
       .query_map(params![workspace_id, object_id], |row| row.get(0))?
       .collect::<Result<_, _>>()?;
+    drop(stmt);
 
     // 3) Compute which to delete (existing − new)
     let to_delete: Vec<&str> = existing_ids
@@ -154,6 +248,11 @@ impl VectorSqliteDB {
 
     // 4) Delete stale fragments (if any)
     if !to_delete.is_empty() {
+      trace!(
+        "[VectorStore] Deleting {} {} stale fragments",
+        object_id,
+        to_delete.len()
+      );
       let placeholders = std::iter::repeat("?")
         .take(to_delete.len())
         .collect::<Vec<_>>()
@@ -180,6 +279,15 @@ impl VectorSqliteDB {
       .collect();
 
     if !to_insert.is_empty() {
+      trace!(
+        "[VectorStore] Inserting {} {} new fragments. ids: {:?}",
+        object_id,
+        to_insert.len(),
+        to_insert
+          .iter()
+          .map(|f| f.fragment_id.as_str())
+          .collect::<Vec<_>>()
+      );
       let mut insert = tx.prepare(
         "INSERT INTO af_collab_embeddings
                (workspace_id, object_id, fragment_id,
@@ -211,9 +319,7 @@ impl VectorSqliteDB {
           ])
           .context("Inserting new fragment")?;
       }
-      drop(insert);
     }
-    drop(stmt);
 
     tx.commit().context("Committing transaction")?;
     Ok(())
@@ -222,7 +328,7 @@ impl VectorSqliteDB {
   pub async fn search(
     &self,
     workspace_id: &str,
-    object_ids: Vec<String>,
+    object_ids: &[String],
     query: &[f32],
     top_k: i32,
   ) -> Result<Vec<SearchResult>> {
@@ -234,7 +340,7 @@ impl VectorSqliteDB {
   pub async fn search_with_score(
     &self,
     workspace_id: &str,
-    object_ids: Vec<String>,
+    object_ids: &[String],
     query: &[f32],
     top_k: i32,
     min_score: f32,
@@ -259,6 +365,11 @@ impl VectorSqliteDB {
     top_k: i32,
     min_score: f32,
   ) -> Result<Vec<SearchResult>> {
+    trace!(
+      "[VectorStore] Searching workspace:{} score:{}",
+      workspace_id,
+      min_score
+    );
     // distance = 1 - score, so we only want distance <= max_distance
     let max_distance = 1.0 - min_score;
     let query_blob = query.as_bytes();
@@ -298,11 +409,17 @@ impl VectorSqliteDB {
   async fn search_with_object_ids(
     &self,
     workspace_id: &str,
-    object_ids: Vec<String>,
+    object_ids: &[String],
     query: &[f32],
     top_k: i32,
     min_score: f32,
   ) -> Result<Vec<SearchResult>> {
+    trace!(
+      "[VectorStore] Searching workspace:{} with object_ids: {:?}, score:{}",
+      workspace_id,
+      object_ids,
+      min_score
+    );
     // distance = 1 - score, so we only want distance <= max_distance
     let max_distance = 1.0 - min_score;
     let query_blob = query.as_bytes();
@@ -343,12 +460,7 @@ impl VectorSqliteDB {
     query_params.push(&query_blob as &dyn ToSql);
     query_params.push(&top_k as &dyn ToSql);
     query_params.push(&workspace_id as &dyn ToSql);
-
-    // Add each object_id as an individual parameter
-    for oid in &object_ids {
-      query_params.push(oid as &dyn ToSql);
-    }
-
+    query_params.extend(object_ids.iter().map(|oid| oid as &dyn ToSql));
     query_params.push(&max_distance as &dyn ToSql);
     let mut rows = stmt.query(query_params.as_slice())?;
     self.process_search_results(&mut rows)
@@ -361,14 +473,21 @@ impl VectorSqliteDB {
       let oid_str: String = row.get(0)?;
       let oid = match Uuid::parse_str(&oid_str) {
         Ok(u) => u,
-        Err(_) => continue,
+        Err(err) => {
+          warn!("[VectorStore] Invalid UUID `{}` in DB: {}", oid_str, err);
+          continue;
+        },
       };
       let content: String = row.get(1)?;
       let metadata = row
         .get::<_, Option<String>>(2)?
         .and_then(|s| serde_json::from_str::<Value>(&s).ok());
       let score: f32 = row.get(4)?;
-
+      trace!(
+        "[VectorStore] Found {} embedding record, score: {}",
+        oid,
+        score
+      );
       results.push(SearchResult {
         oid,
         content,
diff --git a/frontend/rust-lib/flowy-sqlite-vec/src/entities.rs b/frontend/rust-lib/flowy-sqlite-vec/src/entities.rs
index bbd39d342b117..170ab711a251c 100644
--- a/frontend/rust-lib/flowy-sqlite-vec/src/entities.rs
+++ b/frontend/rust-lib/flowy-sqlite-vec/src/entities.rs
@@ -5,3 +5,16 @@ pub struct PendingIndexedCollab {
   pub content: String,
   pub collab_type: i16,
 }
+
+#[derive(Clone, Debug)]
+pub struct SqliteEmbeddedDocument {
+  pub workspace_id: String,
+  pub object_id: String,
+  pub fragments: Vec<SqliteEmbeddedFragment>,
+}
+
+#[derive(Clone, Debug)]
+pub struct SqliteEmbeddedFragment {
+  pub content: String,
+  pub embeddings: Vec<f32>,
+}
diff --git a/frontend/rust-lib/flowy-sqlite-vec/tests/main.rs b/frontend/rust-lib/flowy-sqlite-vec/tests/main.rs
index 272040064b4ec..194dd77024d36 100644
--- a/frontend/rust-lib/flowy-sqlite-vec/tests/main.rs
+++ b/frontend/rust-lib/flowy-sqlite-vec/tests/main.rs
@@ -73,7 +73,7 @@ async fn test_upsert_and_remove_fragments() -> Result<()> {
   let result = db
     .search(
       &workspace_id.to_string(),
-      vec![],
+      &[],
       &generate_embedding_with_size(768, 0.1),
       1,
     )
@@ -170,7 +170,7 @@ async fn test_search_no_hits() -> Result<()> {
 
   // Query with a very different vector should return empty
   let query = generate_embedding_with_size(768, -1.0);
-  let results = db.search(&workspace_id, vec![], &query, 1).await?;
+  let results = db.search(&workspace_id, &[], &query, 1).await?;
   assert!(
     results.is_empty(),
     "Expected no near neighbors for orthogonal vector"
@@ -197,14 +197,14 @@ async fn test_multi_workspace_isolation() -> Result<()> {
 
   // Searching in ws1 should not return ws2's fragment
   let res1 = db
-    .search(&ws1, vec![], &generate_embedding_with_size(768, 0.9), 1)
+    .search(&ws1, &[], &generate_embedding_with_size(768, 0.9), 1)
     .await?;
   assert_eq!(res1.len(), 1);
   assert_eq!(res1[0].oid, Uuid::parse_str(&oid)?);
 
   // Searching in ws2 should not return ws1's fragment
   let res2 = db
-    .search(&ws2, vec![], &generate_embedding_with_size(768, -0.9), 1)
+    .search(&ws2, &[], &generate_embedding_with_size(768, -0.9), 1)
     .await?;
   assert_eq!(res2.len(), 1);
   assert_eq!(res2[0].oid, Uuid::parse_str(&oid)?);
@@ -344,7 +344,7 @@ async fn test_object_ids_handling() -> Result<()> {
   let search_results = db
     .search(
       &workspace_id,
-      vec![],
+      &[],
       &generate_embedding_with_size(768, 0.2),
       10,
     )
