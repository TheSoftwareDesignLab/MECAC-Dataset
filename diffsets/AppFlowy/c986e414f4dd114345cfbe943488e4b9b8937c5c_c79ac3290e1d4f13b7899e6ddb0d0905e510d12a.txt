diff --git a/frontend/appflowy_flutter/lib/ai/service/ai_entities.dart b/frontend/appflowy_flutter/lib/ai/service/ai_entities.dart
index 6a2d1d4f4f67d..8b3198cbd789c 100644
--- a/frontend/appflowy_flutter/lib/ai/service/ai_entities.dart
+++ b/frontend/appflowy_flutter/lib/ai/service/ai_entities.dart
@@ -15,11 +15,12 @@ class AIStreamEventPrefix {
   static const start = 'start:';
   static const finish = 'finish:';
   static const comment = 'comment:';
-  static const aiResponseLimit = 'AI_RESPONSE_LIMIT';
-  static const aiImageResponseLimit = 'AI_IMAGE_RESPONSE_LIMIT';
-  static const aiMaxRequired = 'AI_MAX_REQUIRED:';
-  static const localAINotReady = 'LOCAL_AI_NOT_READY';
-  static const localAIDisabled = 'LOCAL_AI_DISABLED';
+  static const aiResponseLimit = 'ai_response_limit:';
+  static const aiImageResponseLimit = 'ai_image_response_limit:';
+  static const aiMaxRequired = 'ai_max_required:';
+  static const localAINotReady = 'local_ai_not_ready:';
+  static const localAIDisabled = 'local_ai_disabled:';
+  static const aiFollowUp = 'ai_follow_up:';
 }
 
 enum AiType {
diff --git a/frontend/appflowy_flutter/lib/plugins/ai_chat/application/chat_ai_message_bloc.dart b/frontend/appflowy_flutter/lib/plugins/ai_chat/application/chat_ai_message_bloc.dart
index 47c1668a2c986..bb39832b1604a 100644
--- a/frontend/appflowy_flutter/lib/plugins/ai_chat/application/chat_ai_message_bloc.dart
+++ b/frontend/appflowy_flutter/lib/plugins/ai_chat/application/chat_ai_message_bloc.dart
@@ -117,6 +117,14 @@ class ChatAIMessageBloc extends Bloc<ChatAIMessageEvent, ChatAIMessageState> {
         ),
       );
     });
+
+    on<_OnAIFollowUp>((event, emit) {
+      emit(
+        state.copyWith(
+          messageState: MessageState.aiFollowUp(event.followUpData),
+        ),
+      );
+    });
   }
 
   void _initializeStreamListener() {
@@ -137,6 +145,9 @@ class ChatAIMessageBloc extends Bloc<ChatAIMessageEvent, ChatAIMessageState> {
         },
         onLocalAIInitializing: () =>
             _safeAdd(const ChatAIMessageEvent.onLocalAIInitializing()),
+        onAIFollowUp: (data) {
+          _safeAdd(ChatAIMessageEvent.onAIFollowUp(data));
+        },
       );
     }
   }
@@ -174,6 +185,9 @@ class ChatAIMessageEvent with _$ChatAIMessageEvent {
   const factory ChatAIMessageEvent.receiveMetadata(
     MetadataCollection metadata,
   ) = _ReceiveMetadata;
+  const factory ChatAIMessageEvent.onAIFollowUp(
+    AIFollowUpData followUpData,
+  ) = _OnAIFollowUp;
 }
 
 @freezed
@@ -209,4 +223,6 @@ class MessageState with _$MessageState {
   const factory MessageState.onInitializingLocalAI() = _LocalAIInitializing;
   const factory MessageState.ready() = _Ready;
   const factory MessageState.loading() = _Loading;
+  const factory MessageState.aiFollowUp(AIFollowUpData followUpData) =
+      _AIFollowUp;
 }
diff --git a/frontend/appflowy_flutter/lib/plugins/ai_chat/application/chat_bloc.dart b/frontend/appflowy_flutter/lib/plugins/ai_chat/application/chat_bloc.dart
index 588d075fd6bbc..d6a7028263efb 100644
--- a/frontend/appflowy_flutter/lib/plugins/ai_chat/application/chat_bloc.dart
+++ b/frontend/appflowy_flutter/lib/plugins/ai_chat/application/chat_bloc.dart
@@ -24,6 +24,11 @@ import 'chat_message_stream.dart';
 
 part 'chat_bloc.freezed.dart';
 
+/// Returns current Unix timestamp (seconds since epoch)
+int timestamp() {
+  return DateTime.now().millisecondsSinceEpoch ~/ 1000;
+}
+
 class ChatBloc extends Bloc<ChatEvent, ChatState> {
   ChatBloc({
     required this.chatId,
@@ -277,6 +282,10 @@ class ChatBloc extends Bloc<ChatEvent, ChatState> {
           deleteMessage: (mesesage) async {
             await chatController.remove(mesesage);
           },
+          onAIFollowUp: (followUpData) {
+            shouldFetchRelatedQuestions =
+                followUpData.shouldGenerateRelatedQuestion;
+          },
         );
       },
     );
@@ -560,8 +569,7 @@ class ChatBloc extends Bloc<ChatEvent, ChatState> {
     Map<String, dynamic>? sentMetadata,
   ) {
     final now = DateTime.now();
-
-    questionStreamMessageId = (now.millisecondsSinceEpoch ~/ 1000).toString();
+    questionStreamMessageId = timestamp().toString();
 
     return TextMessage(
       author: User(id: userId),
@@ -674,6 +682,9 @@ class ChatEvent with _$ChatEvent {
   ) = _DidReceiveRelatedQueston;
 
   const factory ChatEvent.deleteMessage(Message message) = _DeleteMessage;
+
+  const factory ChatEvent.onAIFollowUp(AIFollowUpData followUpData) =
+      _OnAIFollowUp;
 }
 
 @freezed
diff --git a/frontend/appflowy_flutter/lib/plugins/ai_chat/application/chat_message_stream.dart b/frontend/appflowy_flutter/lib/plugins/ai_chat/application/chat_message_stream.dart
index c22559f21b4da..4f48617f4ffab 100644
--- a/frontend/appflowy_flutter/lib/plugins/ai_chat/application/chat_message_stream.dart
+++ b/frontend/appflowy_flutter/lib/plugins/ai_chat/application/chat_message_stream.dart
@@ -1,9 +1,14 @@
 import 'dart:async';
+import 'dart:convert';
 import 'dart:ffi';
 import 'dart:isolate';
 
 import 'package:appflowy/ai/service/ai_entities.dart';
 import 'package:appflowy/plugins/ai_chat/application/chat_message_service.dart';
+import 'package:appflowy_backend/log.dart';
+import 'package:json_annotation/json_annotation.dart';
+
+part 'chat_message_stream.g.dart';
 
 /// A stream that receives answer events from an isolate or external process.
 /// It caches events that might occur before a listener is attached.
@@ -37,7 +42,7 @@ class AnswerStream {
   void Function()? _onAIImageResponseLimit;
   void Function(String message)? _onAIMaxRequired;
   void Function(MetadataCollection metadata)? _onMetadata;
-
+  void Function(AIFollowUpData)? _onAIFollowUp;
   // Caches for events that occur before listen() is called.
   final List<String> _pendingAIMaxRequiredEvents = [];
   bool _pendingLocalAINotReady = false;
@@ -88,6 +93,15 @@ class AnswerStream {
       } else {
         _pendingLocalAINotReady = true;
       }
+    } else if (event.startsWith(AIStreamEventPrefix.aiFollowUp)) {
+      final s = event.substring(AIStreamEventPrefix.aiFollowUp.length);
+      try {
+        final dynamic jsonData = jsonDecode(s);
+        final data = AIFollowUpData.fromJson(jsonData);
+        _onAIFollowUp?.call(data);
+      } catch (e) {
+        Log.error('Error deserializing AIFollowUp data: $e\nRaw JSON: $s');
+      }
     }
   }
 
@@ -114,6 +128,7 @@ class AnswerStream {
     void Function(String message)? onAIMaxRequired,
     void Function(MetadataCollection metadata)? onMetadata,
     void Function()? onLocalAIInitializing,
+    void Function(AIFollowUpData)? onAIFollowUp,
   }) {
     _onData = onData;
     _onStart = onStart;
@@ -124,7 +139,7 @@ class AnswerStream {
     _onAIMaxRequired = onAIMaxRequired;
     _onMetadata = onMetadata;
     _onLocalAIInitializing = onLocalAIInitializing;
-
+    _onAIFollowUp = onAIFollowUp;
     // Flush pending AI_MAX_REQUIRED events.
     if (_onAIMaxRequired != null && _pendingAIMaxRequiredEvents.isNotEmpty) {
       for (final msg in _pendingAIMaxRequiredEvents) {
@@ -205,7 +220,6 @@ class QuestionStream {
   void Function()? _onIndexStart;
   void Function()? _onIndexEnd;
   void Function()? _onDone;
-
   int get nativePort => _port.sendPort.nativePort;
   bool get hasStarted => _hasStarted;
   String? get error => _error;
@@ -241,3 +255,18 @@ class QuestionStream {
     _onDone = onDone;
   }
 }
+
+@JsonSerializable()
+class AIFollowUpData {
+  AIFollowUpData({
+    required this.shouldGenerateRelatedQuestion,
+  });
+
+  factory AIFollowUpData.fromJson(Map<String, dynamic> json) =>
+      _$AIFollowUpDataFromJson(json);
+
+  @JsonKey(name: 'should_generate_related_question')
+  final bool shouldGenerateRelatedQuestion;
+
+  Map<String, dynamic> toJson() => _$AIFollowUpDataToJson(this);
+}
diff --git a/frontend/appflowy_flutter/lib/plugins/ai_chat/presentation/animated_chat_list.dart b/frontend/appflowy_flutter/lib/plugins/ai_chat/presentation/animated_chat_list.dart
index 359907ce2527c..edddc128a782e 100644
--- a/frontend/appflowy_flutter/lib/plugins/ai_chat/presentation/animated_chat_list.dart
+++ b/frontend/appflowy_flutter/lib/plugins/ai_chat/presentation/animated_chat_list.dart
@@ -152,6 +152,8 @@ class ChatAnimatedListState extends State<ChatAnimatedList>
     itemPositionsListener.itemPositions.addListener(() {
       _handleLoadPreviousMessages();
     });
+
+    // A trick to avoid the first message being scrolled to the top
   }
 
   @override
@@ -167,8 +169,8 @@ class ChatAnimatedListState extends State<ChatAnimatedList>
   Widget build(BuildContext context) {
     final builders = context.watch<Builders>();
     final height = MediaQuery.of(context).size.height;
-
     // A trick to avoid the first message being scrolled to the top
+
     initialScrollIndex = messages.length;
     initialAlignment = 1.0;
     if (messages.length <= 2) {
diff --git a/frontend/appflowy_flutter/lib/plugins/ai_chat/presentation/chat_page/text_message_widget.dart b/frontend/appflowy_flutter/lib/plugins/ai_chat/presentation/chat_page/text_message_widget.dart
index 26995b7861d8b..925177c517059 100644
--- a/frontend/appflowy_flutter/lib/plugins/ai_chat/presentation/chat_page/text_message_widget.dart
+++ b/frontend/appflowy_flutter/lib/plugins/ai_chat/presentation/chat_page/text_message_widget.dart
@@ -79,6 +79,8 @@ class TextMessageWidget extends StatelessWidget {
       selector: (state) => state.isSelectingMessages,
       builder: (context, isSelectingMessages) {
         return BlocBuilder<ChatBloc, ChatState>(
+          buildWhen: (previous, current) =>
+              previous.promptResponseState != current.promptResponseState,
           builder: (context, state) {
             final chatController = context.read<ChatBloc>().chatController;
             final messages = chatController.messages
diff --git a/frontend/appflowy_flutter/lib/plugins/ai_chat/presentation/message/ai_text_message.dart b/frontend/appflowy_flutter/lib/plugins/ai_chat/presentation/message/ai_text_message.dart
index 8779e875d4626..561cb0176bb1b 100644
--- a/frontend/appflowy_flutter/lib/plugins/ai_chat/presentation/message/ai_text_message.dart
+++ b/frontend/appflowy_flutter/lib/plugins/ai_chat/presentation/message/ai_text_message.dart
@@ -71,93 +71,106 @@ class ChatAIMessageWidget extends StatelessWidget {
         chatId: chatId,
         questionId: questionId,
       ),
-      child: BlocBuilder<ChatAIMessageBloc, ChatAIMessageState>(
-        builder: (context, state) {
-          final loadingText =
-              state.progress?.step ?? LocaleKeys.chat_generatingResponse.tr();
-
-          return BlocListener<ChatBloc, ChatState>(
-            listenWhen: (previous, current) =>
-                previous.clearErrorMessages != current.clearErrorMessages,
-            listener: (context, chatState) {
-              if (state.stream?.error?.isEmpty != false) {
-                return;
-              }
-              context.read<ChatBloc>().add(ChatEvent.deleteMessage(message));
-            },
-            child: Padding(
-              padding: AIChatUILayout.messageMargin,
-              child: state.messageState.when(
-                loading: () => ChatAIMessageBubble(
-                  message: message,
-                  showActions: false,
-                  child: Padding(
-                    padding: const EdgeInsets.only(top: 8.0),
-                    child: AILoadingIndicator(text: loadingText),
-                  ),
+      child: BlocConsumer<ChatAIMessageBloc, ChatAIMessageState>(
+        listenWhen: (previous, current) =>
+            previous.messageState != current.messageState,
+        listener: (context, state) => _handleMessageState(state, context),
+        builder: (context, blocState) {
+          final loadingText = blocState.progress?.step ??
+              LocaleKeys.chat_generatingResponse.tr();
+
+          return Padding(
+            padding: AIChatUILayout.messageMargin,
+            child: blocState.messageState.when(
+              loading: () => ChatAIMessageBubble(
+                message: message,
+                showActions: false,
+                child: Padding(
+                  padding: const EdgeInsets.only(top: 8.0),
+                  child: AILoadingIndicator(text: loadingText),
                 ),
-                ready: () {
-                  return state.text.isEmpty
-                      ? _LoadingMessage(
-                          message: message,
-                          loadingText: loadingText,
-                        )
-                      : _NonEmptyMessage(
-                          user: user,
-                          messageUserId: messageUserId,
-                          message: message,
-                          stream: stream,
-                          questionId: questionId,
-                          chatId: chatId,
-                          refSourceJsonString: refSourceJsonString,
-                          onStopStream: onStopStream,
-                          onSelectedMetadata: onSelectedMetadata,
-                          onRegenerate: onRegenerate,
-                          onChangeFormat: onChangeFormat,
-                          onChangeModel: onChangeModel,
-                          isLastMessage: isLastMessage,
-                          isStreaming: isStreaming,
-                          isSelectingMessages: isSelectingMessages,
-                          enableAnimation: enableAnimation,
-                        );
-                },
-                onError: (error) {
-                  return ChatErrorMessageWidget(
-                    errorMessage: LocaleKeys.chat_aiServerUnavailable.tr(),
-                  );
-                },
-                onAIResponseLimit: () {
-                  return ChatErrorMessageWidget(
-                    errorMessage:
-                        LocaleKeys.sideBar_askOwnerToUpgradeToAIMax.tr(),
-                  );
-                },
-                onAIImageResponseLimit: () {
-                  return ChatErrorMessageWidget(
-                    errorMessage: LocaleKeys.sideBar_purchaseAIMax.tr(),
-                  );
-                },
-                onAIMaxRequired: (message) {
-                  return ChatErrorMessageWidget(
-                    errorMessage: message,
-                  );
-                },
-                onInitializingLocalAI: () {
-                  onStopStream();
-
-                  return ChatErrorMessageWidget(
-                    errorMessage: LocaleKeys
-                        .settings_aiPage_keys_localAIInitializing
-                        .tr(),
-                  );
-                },
               ),
+              ready: () {
+                return blocState.text.isEmpty
+                    ? _LoadingMessage(
+                        message: message,
+                        loadingText: loadingText,
+                      )
+                    : _NonEmptyMessage(
+                        user: user,
+                        messageUserId: messageUserId,
+                        message: message,
+                        stream: stream,
+                        questionId: questionId,
+                        chatId: chatId,
+                        refSourceJsonString: refSourceJsonString,
+                        onStopStream: onStopStream,
+                        onSelectedMetadata: onSelectedMetadata,
+                        onRegenerate: onRegenerate,
+                        onChangeFormat: onChangeFormat,
+                        onChangeModel: onChangeModel,
+                        isLastMessage: isLastMessage,
+                        isStreaming: isStreaming,
+                        isSelectingMessages: isSelectingMessages,
+                        enableAnimation: enableAnimation,
+                      );
+              },
+              onError: (error) {
+                return ChatErrorMessageWidget(
+                  errorMessage: LocaleKeys.chat_aiServerUnavailable.tr(),
+                );
+              },
+              onAIResponseLimit: () {
+                return ChatErrorMessageWidget(
+                  errorMessage:
+                      LocaleKeys.sideBar_askOwnerToUpgradeToAIMax.tr(),
+                );
+              },
+              onAIImageResponseLimit: () {
+                return ChatErrorMessageWidget(
+                  errorMessage: LocaleKeys.sideBar_purchaseAIMax.tr(),
+                );
+              },
+              onAIMaxRequired: (message) {
+                return ChatErrorMessageWidget(
+                  errorMessage: message,
+                );
+              },
+              onInitializingLocalAI: () {
+                onStopStream();
+
+                return ChatErrorMessageWidget(
+                  errorMessage:
+                      LocaleKeys.settings_aiPage_keys_localAIInitializing.tr(),
+                );
+              },
+              aiFollowUp: (followUpData) {
+                return const SizedBox.shrink();
+              },
             ),
           );
         },
       ),
     );
   }
+
+  void _handleMessageState(ChatAIMessageState state, BuildContext context) {
+    if (state.stream?.error?.isEmpty != false) {
+      state.messageState.maybeMap(
+        aiFollowUp: (messageState) {
+          context
+              .read<ChatBloc>()
+              .add(ChatEvent.onAIFollowUp(messageState.followUpData));
+        },
+        orElse: () {
+          // do nothing
+        },
+      );
+
+      return;
+    }
+    context.read<ChatBloc>().add(ChatEvent.deleteMessage(message));
+  }
 }
 
 class _LoadingMessage extends StatelessWidget {
diff --git a/frontend/appflowy_flutter/macos/Podfile.lock b/frontend/appflowy_flutter/macos/Podfile.lock
index 879f010c46ef8..9709711e3b42c 100644
--- a/frontend/appflowy_flutter/macos/Podfile.lock
+++ b/frontend/appflowy_flutter/macos/Podfile.lock
@@ -135,32 +135,32 @@ EXTERNAL SOURCES:
     :path: Flutter/ephemeral/.symlinks/plugins/window_manager/macos
 
 SPEC CHECKSUMS:
-  app_links: 9028728e32c83a0831d9db8cf91c526d16cc5468
-  appflowy_backend: 464aeb3e5c6966a41641a2111e5ead72ce2695f7
-  auto_updater_macos: 3a42f1a06be6981f1a18be37e6e7bf86aa732118
-  bitsdojo_window_macos: 7959fb0ca65a3ccda30095c181ecb856fae48ea9
-  connectivity_plus: e74b9f74717d2d99d45751750e266e55912baeb5
-  desktop_drop: e0b672a7d84c0a6cbc378595e82cdb15f2970a43
-  device_info_plus: 4fb280989f669696856f8b129e4a5e3cd6c48f76
-  file_selector_macos: 6280b52b459ae6c590af5d78fc35c7267a3c4b31
-  flowy_infra_ui: 8760ff42a789de40bf5007a5f176b454722a341e
+  app_links: 10e0a0ab602ffaf34d142cd4862f29d34b303b2a
+  appflowy_backend: 865496343de667fc8c600e04b9fd05234e130cf9
+  auto_updater_macos: 3e3462c418fe4e731917eacd8d28eef7af84086d
+  bitsdojo_window_macos: 44e3b8fe3dd463820e0321f6256c5b1c16bb6a00
+  connectivity_plus: 18d3c32514c886e046de60e9c13895109866c747
+  desktop_drop: 69eeff437544aa619c8db7f4481b3a65f7696898
+  device_info_plus: 1b14eed9bf95428983aed283a8d51cce3d8c4215
+  file_selector_macos: cc3858c981fe6889f364731200d6232dac1d812d
+  flowy_infra_ui: 03301a39ad118771adbf051a664265c61c507f38
   FlutterMacOS: 8f6f14fa908a6fb3fba0cd85dbd81ec4b251fb24
   HotKey: 400beb7caa29054ea8d864c96f5ba7e5b4852277
-  hotkey_manager: b443f35f4d772162937aa73fd8995e579f8ac4e2
-  irondash_engine_context: 893c7d96d20ce361d7e996f39d360c4c2f9869ba
-  local_notifier: ebf072651e35ae5e47280ad52e2707375cb2ae4e
-  package_info_plus: f0052d280d17aa382b932f399edf32507174e870
-  path_provider_foundation: 080d55be775b7414fd5a5ef3ac137b97b097e564
+  hotkey_manager: c32bf0bfe8f934b7bc17ab4ad5c4c142960b023c
+  irondash_engine_context: da62996ee25616d2f01bbeb85dc115d813359478
+  local_notifier: e9506bc66fc70311e8bc7291fb70f743c081e4ff
+  package_info_plus: 12f1c5c2cfe8727ca46cbd0b26677728972d9a5b
+  path_provider_foundation: 2b6b4c569c0fb62ec74538f866245ac84301af46
   ReachabilitySwift: 32793e867593cfc1177f5d16491e3a197d2fccda
-  screen_retriever_macos: 452e51764a9e1cdb74b3c541238795849f21557f
-  share_plus: 510bf0af1a42cd602274b4629920c9649c52f4cc
-  shared_preferences_foundation: 9e1978ff2562383bd5676f64ec4e9aa8fa06a6f7
+  screen_retriever_macos: 776e0fa5d42c6163d2bf772d22478df4b302b161
+  share_plus: 1fa619de8392a4398bfaf176d441853922614e89
+  shared_preferences_foundation: fcdcbc04712aee1108ac7fda236f363274528f78
   Sparkle: 9c328bdcfbcaf8f030c4b678eadfd0fcb03822d8
-  sqflite_darwin: 20b2a3a3b70e43edae938624ce550a3cbf66a3d0
-  super_native_extensions: c2795d6d9aedf4a79fae25cb6160b71b50549189
-  url_launcher_macos: 0fba8ddabfc33ce0a9afe7c5fef5aab3d8d2d673
-  webview_flutter_wkwebview: 44d4dee7d7056d5ad185d25b38404436d56c547c
-  window_manager: e8d0b1431ab6c454f2b5c9ae26004bbfa43469aa
+  sqflite_darwin: 5a7236e3b501866c1c9befc6771dfd73ffb8702d
+  super_native_extensions: 85efee3a7495b46b04befcfc86ed12069264ebf3
+  url_launcher_macos: c82c93949963e55b228a30115bd219499a6fe404
+  webview_flutter_wkwebview: 0982481e3d9c78fd5c6f62a002fcd24fc791f1e4
+  window_manager: 990c8e348c4da2a93b81da638245d40554ec9436
 
 PODFILE CHECKSUM: 0532f3f001ca3110b8be345d6491fff690e95823
 
diff --git a/frontend/rust-lib/Cargo.lock b/frontend/rust-lib/Cargo.lock
index 96e64c41f7874..7d2e6e3e88f82 100644
--- a/frontend/rust-lib/Cargo.lock
+++ b/frontend/rust-lib/Cargo.lock
@@ -493,7 +493,7 @@ checksum = "dcfed56ad506cb2c684a14971b8861fdc3baaaae314b9e5f9bb532cbe3ba7a4f"
 [[package]]
 name = "app-error"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "anyhow",
  "bincode",
@@ -513,7 +513,7 @@ dependencies = [
 [[package]]
 name = "appflowy-ai-client"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "anyhow",
  "bytes",
@@ -1233,7 +1233,7 @@ dependencies = [
 [[package]]
 name = "client-api"
 version = "0.2.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "again",
  "anyhow",
@@ -1288,7 +1288,7 @@ dependencies = [
 [[package]]
 name = "client-api-entity"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "collab-entity",
  "collab-rt-entity",
@@ -1301,7 +1301,7 @@ dependencies = [
 [[package]]
 name = "client-websocket"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "futures-channel",
  "futures-util",
@@ -1572,7 +1572,7 @@ dependencies = [
 [[package]]
 name = "collab-rt-entity"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "anyhow",
  "bincode",
@@ -1594,7 +1594,7 @@ dependencies = [
 [[package]]
 name = "collab-rt-protocol"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "anyhow",
  "async-trait",
@@ -2073,7 +2073,7 @@ checksum = "c2e66c9d817f1720209181c316d28635c050fa304f9c79e47a520882661b7308"
 [[package]]
 name = "database-entity"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "bincode",
  "bytes",
@@ -2717,6 +2717,7 @@ dependencies = [
  "client-api",
  "flowy-error",
  "flowy-sqlite",
+ "flowy-user-pub",
  "futures",
  "lib-infra",
  "serde",
@@ -3195,6 +3196,7 @@ dependencies = [
  "r2d2_sqlite",
  "rusqlite",
  "rusqlite_migration",
+ "serde",
  "serde_json",
  "sqlite-vec",
  "tempfile",
@@ -3648,7 +3650,7 @@ dependencies = [
 [[package]]
 name = "gotrue"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "anyhow",
  "getrandom 0.2.10",
@@ -3663,7 +3665,7 @@ dependencies = [
 [[package]]
 name = "gotrue-entity"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "app-error",
  "jsonwebtoken",
@@ -4376,7 +4378,7 @@ dependencies = [
 [[package]]
 name = "infra"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "anyhow",
  "bytes",
@@ -7519,7 +7521,7 @@ dependencies = [
 [[package]]
 name = "shared-entity"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "anyhow",
  "app-error",
@@ -9665,7 +9667,7 @@ dependencies = [
 [[package]]
 name = "workspace-template"
 version = "0.1.0"
-source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=9e16ff714903823ed68a001e78607da5c7f5c93a#9e16ff714903823ed68a001e78607da5c7f5c93a"
+source = "git+https://github.com/AppFlowy-IO/AppFlowy-Cloud?rev=f840cd362cc6176079303adde973e0bd50cee749#f840cd362cc6176079303adde973e0bd50cee749"
 dependencies = [
  "anyhow",
  "async-trait",
diff --git a/frontend/rust-lib/Cargo.toml b/frontend/rust-lib/Cargo.toml
index 197aa4481dec9..e27d99183083b 100644
--- a/frontend/rust-lib/Cargo.toml
+++ b/frontend/rust-lib/Cargo.toml
@@ -111,9 +111,9 @@ flowy-sqlite-vec = { path = "flowy-sqlite-vec" }
 # Run the script.add_workspace_members:
 # scripts/tool/update_client_api_rev.sh  new_rev_id
 # ⚠️⚠️⚠️️
-client-api = { git = "https://github.com/AppFlowy-IO/AppFlowy-Cloud", rev = "9e16ff714903823ed68a001e78607da5c7f5c93a" }
-client-api-entity = { git = "https://github.com/AppFlowy-IO/AppFlowy-Cloud", rev = "9e16ff714903823ed68a001e78607da5c7f5c93a" }
-workspace-template = { git = "https://github.com/AppFlowy-IO/AppFlowy-Cloud", rev = "9e16ff714903823ed68a001e78607da5c7f5c93a" }
+client-api = { git = "https://github.com/AppFlowy-IO/AppFlowy-Cloud", rev = "f840cd362cc6176079303adde973e0bd50cee749" }
+client-api-entity = { git = "https://github.com/AppFlowy-IO/AppFlowy-Cloud", rev = "f840cd362cc6176079303adde973e0bd50cee749" }
+workspace-template = { git = "https://github.com/AppFlowy-IO/AppFlowy-Cloud", rev = "f840cd362cc6176079303adde973e0bd50cee749" }
 
 [profile.dev]
 opt-level = 0
diff --git a/frontend/rust-lib/event-integration-test/src/chat_event.rs b/frontend/rust-lib/event-integration-test/src/chat_event.rs
index 8336dfd476455..c08e899a71180 100644
--- a/frontend/rust-lib/event-integration-test/src/chat_event.rs
+++ b/frontend/rust-lib/event-integration-test/src/chat_event.rs
@@ -1,8 +1,8 @@
 use crate::event_builder::EventBuilder;
 use crate::EventIntegrationTest;
 use flowy_ai::entities::{
-  ChatMessageListPB, ChatMessageTypePB, LoadNextChatMessagePB, LoadPrevChatMessagePB,
-  SendChatPayloadPB,
+  ChatId, ChatMessageListPB, ChatMessageTypePB, LoadNextChatMessagePB, LoadPrevChatMessagePB,
+  StreamChatPayloadPB, UpdateChatSettingsPB,
 };
 use flowy_ai::event_map::AIEvent;
 use flowy_folder::entities::{CreateViewPayloadPB, ViewLayoutPB, ViewPB};
@@ -31,16 +31,33 @@ impl EventIntegrationTest {
       .parse::<ViewPB>()
   }
 
+  pub async fn set_chat_rag_ids(&self, chat_id: &str, rag_ids: Vec<String>) {
+    let payload = UpdateChatSettingsPB {
+      chat_id: ChatId {
+        value: chat_id.to_string(),
+      },
+      rag_ids,
+    };
+    EventBuilder::new(self.clone())
+      .event(AIEvent::UpdateChatSettings)
+      .payload(payload)
+      .async_send()
+      .await;
+  }
+
   pub async fn send_message(
     &self,
     chat_id: &str,
     message: impl ToString,
     message_type: ChatMessageTypePB,
   ) {
-    let payload = SendChatPayloadPB {
+    let payload = StreamChatPayloadPB {
       chat_id: chat_id.to_string(),
       message: message.to_string(),
       message_type,
+      answer_stream_port: 0,
+      question_stream_port: 0,
+      format: None,
     };
 
     EventBuilder::new(self.clone())
diff --git a/frontend/rust-lib/event-integration-test/tests/asset/japan_trip.md b/frontend/rust-lib/event-integration-test/tests/asset/japan_trip.md
new file mode 100644
index 0000000000000..9e10acad1b66d
--- /dev/null
+++ b/frontend/rust-lib/event-integration-test/tests/asset/japan_trip.md
@@ -0,0 +1,23 @@
+Our trip begins with a flight from America to Tokyo on January 7th.
+In Tokyo, we’ll spend three days, from February 7th to 10th, exploring
+the city’s tech scene and snowboarding gear shops. We’ll visit popular
+spots like Shibuya, Shinjuku, and Odaiba before heading to our next
+destination.
+
+From Tokyo, we fly to Sendai and then travel to Zao Onsen for a 3-day
+stay from February 10th to 14th. Zao Onsen is famous for its beautiful
+snow and the iconic ice trees, which will make for a unique snowboarding
+experience.
+
+After Zao Onsen, we fly from Sendai to Chitose, then head to Sapporo for
+a 2-day visit, exploring the city’s vibrant atmosphere and winter
+attractions. On the next day, we’ll spend time at Sapporo Tein, a ski
+resort that offers great runs and stunning views of the city and the sea.
+
+Then we head to Rusutsu for 5 days, one of the top ski resorts in Japan,
+known for its deep powder snow and extensive runs. Finally, we’ll fly
+back to Singapore after experiencing some of the best snowboarding Japan
+has to offer.
+
+Ski resorts to visit include Niseko (二世谷), Rusutsu (留寿都), Sapporo Tein
+(札幌和海景), and Zao Onsen Ski Resort (冰树).
\ No newline at end of file
diff --git a/frontend/rust-lib/event-integration-test/tests/chat/local_chat_test.rs b/frontend/rust-lib/event-integration-test/tests/chat/local_chat_test.rs
index 8b137891791fe..a1ce201027c6b 100644
--- a/frontend/rust-lib/event-integration-test/tests/chat/local_chat_test.rs
+++ b/frontend/rust-lib/event-integration-test/tests/chat/local_chat_test.rs
@@ -1 +1,43 @@
+use crate::util::load_text_file_content;
+use event_integration_test::user_event::use_localhost_af_cloud;
+use event_integration_test::EventIntegrationTest;
+use flowy_user_pub::entities::WorkspaceType;
 
+#[tokio::test]
+async fn local_ollama_test_create_chat_with_selected_sources() {
+  use_localhost_af_cloud().await;
+  let test = EventIntegrationTest::new().await;
+  test.af_cloud_sign_up().await;
+  test.toggle_local_ai().await;
+
+  let local_workspace = test
+    .create_workspace("my workspace", WorkspaceType::Local)
+    .await;
+
+  // create a chat document
+  test
+    .open_workspace(
+      &local_workspace.workspace_id,
+      local_workspace.workspace_type,
+    )
+    .await;
+  let doc = test
+    .create_and_open_document(
+      &local_workspace.workspace_id,
+      "japan trip".to_string(),
+      vec![],
+    )
+    .await;
+  let content = load_text_file_content("japan_trip.md");
+  test.insert_document_text(&doc.id, &content, 0).await;
+
+  //chat with the document
+  let chat = test.create_chat(&local_workspace.workspace_id).await;
+  test
+    .set_chat_rag_ids(&chat.id, vec![doc.id.to_string()])
+    .await;
+
+  // test
+  //   .send_message(&chat.id, "why use rust?", ChatMessageTypePB::User)
+  //   .await;
+}
diff --git a/frontend/rust-lib/event-integration-test/tests/util.rs b/frontend/rust-lib/event-integration-test/tests/util.rs
index 2e2c2d578ffc5..0401b4f01217b 100644
--- a/frontend/rust-lib/event-integration-test/tests/util.rs
+++ b/frontend/rust-lib/event-integration-test/tests/util.rs
@@ -173,3 +173,10 @@ pub fn gen_csv_import_data(file_name: &str, workspace_id: &str) -> ImportPayload
     }],
   }
 }
+
+pub fn load_text_file_content(name: &str) -> String {
+  let path = format!("tests/asset/{}", name);
+  std::fs::read_to_string(path).unwrap_or_else(|_| {
+    panic!("Failed to read asset file: {}", name);
+  })
+}
diff --git a/frontend/rust-lib/flowy-ai-pub/Cargo.toml b/frontend/rust-lib/flowy-ai-pub/Cargo.toml
index 016a7910b663f..9f543ef2ef158 100644
--- a/frontend/rust-lib/flowy-ai-pub/Cargo.toml
+++ b/frontend/rust-lib/flowy-ai-pub/Cargo.toml
@@ -16,4 +16,5 @@ uuid.workspace = true
 flowy-sqlite = { workspace = true }
 chrono.workspace = true
 twox-hash = { version = "2.1.0", features = ["xxhash64"] }
+flowy-user-pub = { workspace = true }
 tracing.workspace = true
diff --git a/frontend/rust-lib/flowy-ai-pub/src/entities.rs b/frontend/rust-lib/flowy-ai-pub/src/entities.rs
index 0cd20bdd58571..c728eea63efe9 100644
--- a/frontend/rust-lib/flowy-ai-pub/src/entities.rs
+++ b/frontend/rust-lib/flowy-ai-pub/src/entities.rs
@@ -4,6 +4,7 @@ use twox_hash::xxhash64::Hasher;
 use uuid::Uuid;
 pub const RAG_IDS: &str = "rag_ids";
 pub const SOURCE_ID: &str = "id";
+pub const WORKSPACE_ID: &str = "workspace_id";
 pub const SOURCE: &str = "source";
 pub const SOURCE_NAME: &str = "name";
 pub struct EmbeddingRecord {
diff --git a/frontend/rust-lib/flowy-ai-pub/src/user_service.rs b/frontend/rust-lib/flowy-ai-pub/src/user_service.rs
index e227c977fe6a7..459c7d575691a 100644
--- a/frontend/rust-lib/flowy-ai-pub/src/user_service.rs
+++ b/frontend/rust-lib/flowy-ai-pub/src/user_service.rs
@@ -1,5 +1,6 @@
 use flowy_error::{FlowyError, FlowyResult};
 use flowy_sqlite::DBConnection;
+use flowy_user_pub::entities::WorkspaceType;
 use lib_infra::async_trait::async_trait;
 use std::path::PathBuf;
 use uuid::Uuid;
@@ -9,6 +10,9 @@ pub trait AIUserService: Send + Sync + 'static {
   fn user_id(&self) -> Result<i64, FlowyError>;
   async fn is_local_model(&self) -> FlowyResult<bool>;
   fn workspace_id(&self) -> Result<Uuid, FlowyError>;
+
+  fn workspace_type(&self) -> FlowyResult<WorkspaceType>;
+
   fn sqlite_connection(&self, uid: i64) -> Result<DBConnection, FlowyError>;
   fn application_root_dir(&self) -> Result<PathBuf, FlowyError>;
 }
diff --git a/frontend/rust-lib/flowy-ai/Cargo.toml b/frontend/rust-lib/flowy-ai/Cargo.toml
index ea76bdbae6dbb..1aaafaa6478e6 100644
--- a/frontend/rust-lib/flowy-ai/Cargo.toml
+++ b/frontend/rust-lib/flowy-ai/Cargo.toml
@@ -50,12 +50,12 @@ async-trait.workspace = true
 async-stream = "0.3.6"
 flowy-database-pub = { workspace = true }
 langchain-rust = { version = "4.6.0", features = ["ollama"] }
+text-splitter = { version = "0.25.1" }
+flowy-sqlite-vec.workspace = true
 
 [target.'cfg(any(target_os = "macos", target_os = "linux", target_os = "windows"))'.dependencies]
 notify = "6.1.1"
 af-mcp = { version = "0.1.0" }
-text-splitter = { version = "0.25.1" }
-flowy-sqlite-vec.workspace = true
 lopdf = { version = "0.36.0", optional = true }
 pulldown-cmark = { version = "0.13.0", optional = true }
 
diff --git a/frontend/rust-lib/flowy-ai/src/ai_manager.rs b/frontend/rust-lib/flowy-ai/src/ai_manager.rs
index bbda3e75adbe1..7d8b48e11a509 100644
--- a/frontend/rust-lib/flowy-ai/src/ai_manager.rs
+++ b/frontend/rust-lib/flowy-ai/src/ai_manager.rs
@@ -152,16 +152,20 @@ impl AIManager {
     }
   }
 
-  async fn prepare_local_ai(&self, workspace_id: &Uuid) {
+  async fn prepare_local_ai(&self, workspace_id: &Uuid, is_enabled: bool) {
     self
       .local_ai
       .reload_ollama_client(&workspace_id.to_string())
       .await;
-    self
-      .model_control
-      .lock()
-      .await
-      .add_source(Box::new(LocalAiSource::new(self.local_ai.clone())));
+    if is_enabled {
+      self
+        .model_control
+        .lock()
+        .await
+        .add_source(Box::new(LocalAiSource::new(self.local_ai.clone())));
+    } else {
+      self.model_control.lock().await.remove_local_source();
+    }
   }
 
   #[instrument(skip_all, err)]
@@ -170,13 +174,8 @@ impl AIManager {
       .local_ai
       .is_enabled_on_workspace(&workspace_id.to_string());
 
-    info!("local is enabled: {}", is_enabled);
-    if is_enabled {
-      self.prepare_local_ai(workspace_id).await;
-    } else {
-      self.model_control.lock().await.remove_local_source();
-    }
-
+    info!("{} local ai is enabled: {}", workspace_id, is_enabled);
+    self.prepare_local_ai(workspace_id, is_enabled).await;
     self.reload_with_workspace_id(workspace_id).await;
     Ok(())
   }
@@ -241,7 +240,7 @@ impl AIManager {
       .await
       {
         Ok(settings) => {
-          local_ai.set_rag_ids(&chat_id, &settings.rag_ids).await;
+          local_ai.set_rag_ids(&chat_id, &settings.rag_ids);
           let rag_ids = settings
             .rag_ids
             .into_iter()
@@ -434,7 +433,7 @@ impl AIManager {
     let enabled = self.local_ai.toggle_local_ai().await?;
     let workspace_id = self.user_service.workspace_id()?;
     if enabled {
-      self.prepare_local_ai(&workspace_id).await;
+      self.prepare_local_ai(&workspace_id, enabled).await;
 
       if let Some(name) = self.local_ai.get_local_chat_model() {
         let model = AIModel::local(name, "".to_string());
@@ -713,7 +712,7 @@ impl AIManager {
 
     let user_service = self.user_service.clone();
     let external_service = self.external_service.clone();
-    self.local_ai.set_rag_ids(chat_id, &rag_ids).await;
+    self.local_ai.set_rag_ids(chat_id, &rag_ids);
 
     let rag_ids = rag_ids
       .into_iter()
diff --git a/frontend/rust-lib/flowy-ai/src/chat.rs b/frontend/rust-lib/flowy-ai/src/chat.rs
index c81cdf09f515c..9f8af289e10e7 100644
--- a/frontend/rust-lib/flowy-ai/src/chat.rs
+++ b/frontend/rust-lib/flowy-ai/src/chat.rs
@@ -4,7 +4,7 @@ use crate::entities::{
 };
 use crate::middleware::chat_service_mw::ChatServiceMiddleware;
 use crate::notification::{chat_notification_builder, ChatNotification};
-use crate::stream_message::StreamMessage;
+use crate::stream_message::{AIFollowUpData, StreamMessage};
 use allo_isolate::Isolate;
 use flowy_ai_pub::cloud::{
   AIModel, ChatCloudService, ChatMessage, MessageCursor, QuestionStreamValue, ResponseFormat,
@@ -94,9 +94,6 @@ impl Chat {
     let uid = self.user_service.user_id()?;
     let workspace_id = self.user_service.workspace_id()?;
 
-    let _ = question_sink
-      .send(StreamMessage::Text(params.message.to_string()).to_string())
-      .await;
     let question = self
       .chat_service
       .create_question(
@@ -209,15 +206,26 @@ impl Chat {
                   },
                   QuestionStreamValue::Metadata { value } => {
                     if let Ok(s) = serde_json::to_string(&value) {
-                      // trace!("[Chat] stream metadata: {}", s);
                       answer_stream_buffer.lock().await.set_metadata(value);
                       let _ = answer_sink
                         .send(StreamMessage::Metadata(s).to_string())
                         .await;
                     }
                   },
-                  QuestionStreamValue::KeepAlive => {
-                    // trace!("[Chat] stream keep alive");
+                  QuestionStreamValue::SuggestedQuestion {
+                    context_suggested_questions: _,
+                  } => {},
+                  QuestionStreamValue::FollowUp {
+                    should_generate_related_question,
+                  } => {
+                    let _ = answer_sink
+                      .send(
+                        StreamMessage::OnFollowUp(AIFollowUpData {
+                          should_generate_related_question,
+                        })
+                        .to_string(),
+                      )
+                      .await;
                   },
                 }
               },
diff --git a/frontend/rust-lib/flowy-ai/src/completion.rs b/frontend/rust-lib/flowy-ai/src/completion.rs
index f739e2ee9583f..6585561c4f145 100644
--- a/frontend/rust-lib/flowy-ai/src/completion.rs
+++ b/frontend/rust-lib/flowy-ai/src/completion.rs
@@ -189,15 +189,15 @@ impl CompletionTask {
 
 async fn handle_error(sink: &mut IsolateSink, err: FlowyError) {
   if err.is_ai_response_limit_exceeded() {
-    let _ = sink.send("AI_RESPONSE_LIMIT".to_string()).await;
+    let _ = sink.send("ai_response_limit:".to_string()).await;
   } else if err.is_ai_image_response_limit_exceeded() {
-    let _ = sink.send("AI_IMAGE_RESPONSE_LIMIT".to_string()).await;
+    let _ = sink.send("ai_image_response_limit:".to_string()).await;
   } else if err.is_ai_max_required() {
-    let _ = sink.send(format!("AI_MAX_REQUIRED:{}", err.msg)).await;
+    let _ = sink.send(format!("ai_max_required:{}", err.msg)).await;
   } else if err.is_local_ai_not_ready() {
-    let _ = sink.send(format!("LOCAL_AI_NOT_READY:{}", err.msg)).await;
+    let _ = sink.send(format!("local_ai_not_ready:{}", err.msg)).await;
   } else if err.is_local_ai_disabled() {
-    let _ = sink.send(format!("LOCAL_AI_DISABLED:{}", err.msg)).await;
+    let _ = sink.send(format!("local_ai_disabled:{}", err.msg)).await;
   } else {
     let _ = sink
       .send(StreamMessage::OnError(err.msg.clone()).to_string())
diff --git a/frontend/rust-lib/flowy-ai/src/embeddings/context.rs b/frontend/rust-lib/flowy-ai/src/embeddings/context.rs
index 2a1fb45cd487c..5605a1ea0b40c 100644
--- a/frontend/rust-lib/flowy-ai/src/embeddings/context.rs
+++ b/frontend/rust-lib/flowy-ai/src/embeddings/context.rs
@@ -52,6 +52,13 @@ impl EmbedContext {
 
   pub fn set_ollama(&self, ollama: Option<Arc<Ollama>>) {
     if let Some(ollama) = ollama {
+      if let Some(o) = self.ollama.load().as_ref() {
+        if o.uri() == ollama.uri() {
+          info!("[Embedding] Ollama does not change");
+          return;
+        }
+      }
+
       self.ollama.store(Some(ollama));
       self.try_create_scheduler();
     } else {
@@ -82,6 +89,9 @@ impl EmbedContext {
         },
         Err(err) => error!("[Embedding] Failed to create scheduler: {}", err),
       }
+    } else {
+      info!("[Embedding] Ollama or vector db is not initialized, remove embedding scheduler");
+      self.scheduler.store(None);
     }
   }
 }
diff --git a/frontend/rust-lib/flowy-ai/src/embeddings/scheduler.rs b/frontend/rust-lib/flowy-ai/src/embeddings/scheduler.rs
index 2a04132953bf1..fd6666fc68990 100644
--- a/frontend/rust-lib/flowy-ai/src/embeddings/scheduler.rs
+++ b/frontend/rust-lib/flowy-ai/src/embeddings/scheduler.rs
@@ -192,19 +192,20 @@ pub async fn spawn_write_embeddings(
   mut stop_rx: broadcast::Receiver<()>,
 ) {
   let mut buf = Vec::with_capacity(EMBEDDING_RECORD_BUFFER_SIZE);
+  info!("[Embedding] spawn embedding writer");
 
   loop {
     select! {
       // Shutdown signal arrives
       _ = stop_rx.recv() => {
-          info!("Received stop signal; shutting down embedding writer");
+          info!("[Embedding] Received stop signal; shutting down embedding writer");
           break;
       }
       // Next batch from the input channel
       n = rx.recv_many(&mut buf, EMBEDDING_RECORD_BUFFER_SIZE) => {
         // channel closed
         if n == 0 {
-          info!("Input channel closed; stopping write embeddings");
+          info!("[Embedding] Input channel closed; stopping write embeddings");
           break;
         }
 
@@ -212,7 +213,7 @@ pub async fn spawn_write_embeddings(
         let scheduler = match scheduler.upgrade() {
           Some(db) => db,
           None => {
-              error!("EmbeddingScheduler dropped; stopping write embeddings");
+              error!("[Embedding] EmbeddingScheduler dropped; stopping write embeddings");
               break;
           }
         };
@@ -243,10 +244,11 @@ async fn spawn_generate_embeddings(
   mut stop_rx: broadcast::Receiver<()>,
 ) {
   let mut buf = Vec::with_capacity(EMBEDDING_RECORD_BUFFER_SIZE);
+  info!("[Embedding] spawn embedding generator");
   loop {
     select! {
       _ = stop_rx.recv() => {
-        info!("Received stop signal; shutting down embedding writer");
+        info!("[Embedding] Received stop signal; shutting down embedding writer");
         break;
       }
       n = rx.recv_many(&mut buf, EMBEDDING_RECORD_BUFFER_SIZE) => {
@@ -302,7 +304,7 @@ async fn spawn_generate_embeddings(
 
                     if chunks.iter().all(|c| c.content.is_none()) {
                       trace!(
-                        "[Embedding] skip generating embeddings for collab: {}",
+                        "[Embedding] content doesn't change, skip generating embeddings for collab: {}",
                         record.object_id
                       );
                       continue;
diff --git a/frontend/rust-lib/flowy-ai/src/embeddings/store.rs b/frontend/rust-lib/flowy-ai/src/embeddings/store.rs
index f7afd249f5bf9..75054055a0dce 100644
--- a/frontend/rust-lib/flowy-ai/src/embeddings/store.rs
+++ b/frontend/rust-lib/flowy-ai/src/embeddings/store.rs
@@ -6,7 +6,7 @@ use flowy_ai_pub::cloud::CollabType;
 use flowy_ai_pub::entities::{RAG_IDS, SOURCE_ID};
 use flowy_error::{FlowyError, FlowyResult};
 use flowy_sqlite_vec::db::VectorSqliteDB;
-use flowy_sqlite_vec::entities::SqliteEmbeddedDocument;
+use flowy_sqlite_vec::entities::{EmbeddedContent, SqliteEmbeddedDocument};
 use futures::stream::{self, StreamExt};
 use langchain_rust::llm::client::OllamaClient;
 use langchain_rust::{
@@ -65,6 +65,25 @@ impl SqliteVectorStore {
         FlowyError::internal().with_context(format!("Failed to select embedded documents: {}", err))
       })
   }
+
+  pub async fn select_all_embedded_content(
+    &self,
+    workspace_id: &str,
+    rag_ids: &[String],
+    limit: usize,
+  ) -> FlowyResult<Vec<EmbeddedContent>> {
+    let vector_db = match self.vector_db.upgrade() {
+      Some(db) => db,
+      None => return Err(FlowyError::internal().with_context("Vector database not initialized")),
+    };
+
+    vector_db
+      .select_all_embedded_content(workspace_id, rag_ids, limit)
+      .await
+      .map_err(|err| {
+        FlowyError::internal().with_context(format!("Failed to select embedded content: {}", err))
+      })
+  }
 }
 
 #[async_trait]
diff --git a/frontend/rust-lib/flowy-ai/src/entities.rs b/frontend/rust-lib/flowy-ai/src/entities.rs
index 0364084e1501b..2254336cbc227 100644
--- a/frontend/rust-lib/flowy-ai/src/entities.rs
+++ b/frontend/rust-lib/flowy-ai/src/entities.rs
@@ -300,6 +300,8 @@ pub struct ChatMessagePB {
 
   #[pb(index = 7, one_of)]
   pub metadata: Option<String>,
+  // #[pb(index = 8)]
+  // pub should_fetch_related_question: bool,
 }
 
 #[derive(Debug, Clone, Default, ProtoBuf)]
@@ -625,9 +627,6 @@ pub struct UpdateChatSettingsPB {
 
   #[pb(index = 2)]
   pub rag_ids: Vec<String>,
-
-  #[pb(index = 3)]
-  pub chat_model: String,
 }
 
 #[derive(Debug, Default, Clone, ProtoBuf)]
diff --git a/frontend/rust-lib/flowy-ai/src/lib.rs b/frontend/rust-lib/flowy-ai/src/lib.rs
index f2ea33765d189..771f0375df064 100644
--- a/frontend/rust-lib/flowy-ai/src/lib.rs
+++ b/frontend/rust-lib/flowy-ai/src/lib.rs
@@ -12,51 +12,9 @@ pub mod local_ai;
 
 #[cfg(feature = "ai-tool")]
 mod ai_tool;
-#[cfg(any(target_os = "windows", target_os = "macos", target_os = "linux"))]
 pub mod embeddings;
-
-#[cfg(any(target_os = "windows", target_os = "macos", target_os = "linux"))]
 pub use embeddings::store::SqliteVectorStore;
 
-#[cfg(not(any(target_os = "windows", target_os = "macos", target_os = "linux")))]
-pub use mock::SqliteVectorStore;
-
-#[cfg(all(
-  not(target_os = "windows"),
-  not(target_os = "macos"),
-  not(target_os = "linux")
-))]
-#[cfg(not(any(target_os = "windows", target_os = "macos", target_os = "linux")))]
-mod mock {
-  use async_trait::async_trait;
-  use langchain_rust::schemas::Document;
-  use langchain_rust::vectorstore::{VecStoreOptions, VectorStore};
-  use serde_json::Value;
-  use std::error::Error;
-  #[derive(Clone)]
-  pub struct SqliteVectorStore;
-  #[async_trait]
-  impl VectorStore for SqliteVectorStore {
-    type Options = VecStoreOptions<Value>;
-    async fn add_documents(
-      &self,
-      docs: &[Document],
-      _opt: &Self::Options,
-    ) -> Result<Vec<String>, Box<dyn Error>> {
-      Ok(vec![])
-    }
-
-    async fn similarity_search(
-      &self,
-      query: &str,
-      limit: usize,
-      opt: &Self::Options,
-    ) -> Result<Vec<Document>, Box<dyn Error>> {
-      Ok(vec![])
-    }
-  }
-}
-
 mod middleware;
 mod model_select;
 #[cfg(test)]
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/chat/chains/context_question_chain.rs b/frontend/rust-lib/flowy-ai/src/local_ai/chat/chains/context_question_chain.rs
new file mode 100644
index 0000000000000..e98877a73c021
--- /dev/null
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/chat/chains/context_question_chain.rs
@@ -0,0 +1,123 @@
+use crate::local_ai::chat::llm::LLMOllama;
+use crate::SqliteVectorStore;
+use flowy_error::{FlowyError, FlowyResult};
+use langchain_rust::language_models::llm::LLM;
+use langchain_rust::prompt::TemplateFormat;
+use langchain_rust::prompt::{PromptFromatter, PromptTemplate};
+use langchain_rust::prompt_args;
+use langchain_rust::schemas::Message;
+use ollama_rs::generation::parameters::{FormatType, JsonStructure};
+use schemars::JsonSchema;
+use serde::{Deserialize, Serialize};
+use serde_json::json;
+use tracing::trace;
+use uuid::Uuid;
+
+const SYSTEM_PROMPT: &str = r#"
+Instruction:
+You are a precise question generator working with a context document that has a unique object_id. Generate exactly three questions that:
+1. Can be directly answered using ONLY explicit information stated in the document
+2. Reference specific facts, details, or statements found verbatim in the text
+3. Do not require inference, speculation, or outside knowledge to answer
+4. Cover different aspects of the information provided in the document
+
+##Context##
+{{context}}
+
+Output format:
+Return a valid JSON array of exactly three objects:
+[
+  {
+    "content": "Question that refers to explicitly stated information",
+    "object_id": "id_of_context_item"
+  },
+  {
+    "content": "Another question based only on factual content in the document",
+    "object_id": "id_of_context_item"
+  },
+  {
+    "content": "A third question referencing specific details from the text",
+    "object_id": "id_of_context_item"
+  }
+]
+
+IMPORTANT RULES:
+- Questions MUST be answerable using ONLY information explicitly stated in the document
+- Do not create questions about information that must be inferred or guessed
+- Do not ask about comparative elements unless the comparison is explicitly made in the text
+- Verify that the exact answer to each question appears in the document verbatim
+- Ensure questions reference different parts of the document for better coverage
+"#;
+
+#[derive(Debug, Deserialize, JsonSchema)]
+struct ContextQuestionsResponse {
+  questions: Vec<ContextQuestion>,
+}
+
+#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
+pub struct ContextQuestion {
+  pub content: String,
+  pub object_id: String,
+}
+
+pub struct RelatedQuestionChain {
+  workspace_id: Uuid,
+  llm: LLMOllama,
+  store: SqliteVectorStore,
+}
+
+impl RelatedQuestionChain {
+  pub fn new(workspace_id: Uuid, ollama: LLMOllama, store: SqliteVectorStore) -> Self {
+    let format = FormatType::StructuredJson(JsonStructure::new::<ContextQuestionsResponse>());
+    Self {
+      workspace_id,
+      llm: ollama.with_format(format),
+      store,
+    }
+  }
+
+  pub async fn generate_questions(&self, rag_ids: &[String]) -> FlowyResult<Vec<ContextQuestion>> {
+    trace!(
+      "[embedding] Generating context related questions for RAG IDs: {:?}",
+      rag_ids
+    );
+
+    let context = self
+      .store
+      .select_all_embedded_content(&self.workspace_id.to_string(), rag_ids, 3)
+      .await?;
+
+    trace!(
+      "[embedding] Generating related questions base on: {:?}",
+      context,
+    );
+
+    let context_str = json!(context).to_string();
+    let input_variables = prompt_args! {
+        "context" => context_str,
+    };
+
+    let template = PromptTemplate::new(
+      SYSTEM_PROMPT.to_string(),
+      vec!["context".to_string()],
+      TemplateFormat::Jinja2,
+    );
+
+    let formatted_prompt = template
+      .format(input_variables)
+      .map_err(|err| FlowyError::internal().with_context(format!("{}", err)))?;
+
+    let messages = vec![Message::new_system_message(formatted_prompt)];
+    let result = self.llm.generate(&messages).await.map_err(|err| {
+      FlowyError::internal().with_context(format!("Error generating related questions: {}", err))
+    })?;
+
+    let mut parsed_result = serde_json::from_str::<ContextQuestionsResponse>(&result.generation)?;
+    // filter out questions that are not in the rag_ids
+    parsed_result
+      .questions
+      .retain(|v| rag_ids.contains(&v.object_id));
+
+    Ok(parsed_result.questions)
+  }
+}
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/chat/conversation_chain.rs b/frontend/rust-lib/flowy-ai/src/local_ai/chat/chains/conversation_chain.rs
similarity index 57%
rename from frontend/rust-lib/flowy-ai/src/local_ai/chat/conversation_chain.rs
rename to frontend/rust-lib/flowy-ai/src/local_ai/chat/chains/conversation_chain.rs
index 9191f16fbaf5b..d48860d56359c 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/chat/conversation_chain.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/chat/chains/conversation_chain.rs
@@ -1,7 +1,11 @@
+use crate::local_ai::chat::chains::context_question_chain::RelatedQuestionChain;
 use crate::local_ai::chat::llm::LLMOllama;
+use crate::SqliteVectorStore;
 use async_stream::stream;
 use async_trait::async_trait;
+use flowy_ai_pub::cloud::{ContextSuggestedQuestion, QuestionStreamValue};
 use flowy_ai_pub::entities::{RAG_IDS, SOURCE_ID};
+use flowy_error::{FlowyError, FlowyResult};
 use futures::Stream;
 use futures_util::{pin_mut, StreamExt};
 use langchain_rust::chain::{
@@ -13,13 +17,17 @@ use langchain_rust::memory::SimpleMemory;
 use langchain_rust::prompt::{FormatPrompter, PromptArgs};
 use langchain_rust::schemas::{BaseMemory, Document, Message, Retriever, StreamData};
 use langchain_rust::vectorstore::{VecStoreOptions, VectorStore};
+use serde::{Deserialize, Serialize};
 use serde_json::{json, Value};
 use std::error::Error;
 use std::{collections::HashMap, pin::Pin, sync::Arc};
 use tokio::sync::Mutex;
 use tokio_util::either::Either;
-use tracing::trace;
+use tracing::{error, trace};
+use uuid::Uuid;
 
+pub const CAN_NOT_ANSWER_WITH_CONTEXT: &str = "I couldn't find any relevant information in the sources you selected. Please try asking a different question";
+pub const ANSWER_WITH_SUGGESTED_QUESTION: &str = "I couldn't find any relevant information in the sources you selected. Please try ask following questions";
 pub(crate) const DEFAULT_OUTPUT_KEY: &str = "output";
 pub(crate) const DEFAULT_RESULT_KEY: &str = "generate_result";
 
@@ -33,11 +41,24 @@ pub struct ConversationalRetrieverChain {
   pub memory: Arc<Mutex<dyn BaseMemory>>,
   pub(crate) combine_documents_chain: Box<dyn Chain>,
   pub(crate) condense_question_chain: Box<dyn Chain>,
+  pub(crate) context_question_chain: Option<RelatedQuestionChain>,
   pub(crate) rephrase_question: bool,
   pub(crate) return_source_documents: bool,
   pub(crate) input_key: String,
   pub(crate) output_key: String,
 }
+
+#[derive(Debug, Clone, Serialize, Deserialize)]
+enum StreamValue {
+  Answer {
+    value: String,
+  },
+  ContextSuggested {
+    value: String,
+    suggested_questions: Vec<ContextSuggestedQuestion>,
+  },
+}
+
 impl ConversationalRetrieverChain {
   async fn get_question(
     &self,
@@ -73,7 +94,7 @@ impl ConversationalRetrieverChain {
   async fn get_documents_or_result(
     &self,
     question: &str,
-  ) -> Result<Either<Vec<Document>, GenerateResult>, ChainError> {
+  ) -> Result<Either<Vec<Document>, StreamValue>, ChainError> {
     let rag_ids = self.retriever.get_rag_ids();
     if rag_ids.is_empty() {
       Ok(Either::Left(vec![]))
@@ -86,13 +107,44 @@ impl ConversationalRetrieverChain {
 
       if documents.is_empty() {
         trace!(
-          "[Embedding] No relevant documents found, but we have RAG IDs:{:?}. return I don't know",
+          "[Embedding] No relevant documents for given RAG IDs:{:?}. try generating suggested questions",
           rag_ids
         );
-        return Ok(Either::Right(GenerateResult {
-            tokens: None,
-            generation: "I couldn’t find any relevant information in the sources you selected. Please try asking a different question".to_string(),
-          }));
+
+        let mut suggested_questions = vec![];
+        if let Some(c) = self.context_question_chain.as_ref() {
+          let rag_ids = rag_ids.iter().map(|v| v.to_string()).collect::<Vec<_>>();
+          match c.generate_questions(&rag_ids).await {
+            Ok(questions) => {
+              trace!("[embedding]: context related questions: {:?}", questions);
+              suggested_questions = questions
+                .into_iter()
+                .map(|q| ContextSuggestedQuestion {
+                  content: q.content,
+                  object_id: q.object_id,
+                })
+                .collect::<Vec<_>>();
+            },
+            Err(err) => {
+              error!(
+                "[embedding] Error generating context related questions: {}",
+                err
+              );
+            },
+          }
+        }
+
+        return if suggested_questions.is_empty() {
+          Ok(Either::Right(StreamValue::ContextSuggested {
+            value: CAN_NOT_ANSWER_WITH_CONTEXT.to_string(),
+            suggested_questions,
+          }))
+        } else {
+          Ok(Either::Right(StreamValue::ContextSuggested {
+            value: ANSWER_WITH_SUGGESTED_QUESTION.to_string(),
+            suggested_questions,
+          }))
+        };
       }
 
       Ok(Either::Left(documents))
@@ -133,12 +185,19 @@ impl Chain for ConversationalRetrieverChain {
       Either::Right(result) => {
         let mut memory = self.memory.lock().await;
         memory.add_message(human_message).await;
-        memory
-          .add_message(Message::new_ai_message(&result.generation))
-          .await;
 
         let mut output = HashMap::new();
-        output.insert(self.output_key.clone(), json!(result.generation));
+        match &result {
+          StreamValue::Answer { value } => {
+            memory.add_message(Message::new_ai_message(value)).await;
+            output.insert(self.output_key.clone(), json!(value));
+          },
+          StreamValue::ContextSuggested { value, .. } => {
+            memory.add_message(Message::new_ai_message(value)).await;
+            output.insert(self.output_key.clone(), json!(value));
+          },
+        }
+
         output.insert(DEFAULT_RESULT_KEY.to_string(), json!(result));
         return Ok(output);
       },
@@ -203,25 +262,90 @@ impl Chain for ConversationalRetrieverChain {
       let memory = self.memory.lock().await;
       memory.messages().await
     };
-
     let (question, _) = self.get_question(&history, &human_message.content).await?;
-
     let documents = match self.get_documents_or_result(&question).await? {
       Either::Left(docs) => docs,
       Either::Right(result) => {
         let mut memory = self.memory.lock().await;
         memory.add_message(human_message).await;
-        memory
-          .add_message(Message::new_ai_message(&result.generation))
-          .await;
-
-        return Ok(Box::pin(stream! {
-          yield Ok(StreamData::new(
-            json!(result),
-            result.tokens,
-            result.generation,
-          ));
-        }));
+
+        return match result {
+          StreamValue::Answer { value } => {
+            memory.add_message(Message::new_ai_message(&value)).await;
+            Ok(Box::pin(stream! {
+              yield Ok(StreamData::new(
+                json!( QuestionStreamValue::Answer { value: value.clone() }),
+                None,
+                value.clone()
+              ));
+            }))
+          },
+          StreamValue::ContextSuggested {
+            value,
+            suggested_questions,
+          } => {
+            // Create final value for memory once
+            let final_value = if suggested_questions.is_empty() {
+              value.clone()
+            } else {
+              let formatted_questions = suggested_questions
+                .iter()
+                .enumerate()
+                .map(|(i, q)| format!("{}. {}", i + 1, q.content))
+                .collect::<Vec<_>>()
+                .join("\n");
+
+              format!("{}\n\n{}", value, formatted_questions)
+            };
+
+            memory
+              .add_message(Message::new_ai_message(&final_value))
+              .await;
+
+            Ok(Box::pin(stream! {
+              // Yield the initial message
+              yield Ok(StreamData::new(
+                json!(QuestionStreamValue::Answer { value: value.clone() }),
+                None,
+                value
+              ));
+
+              // If we have questions, add a newline separator before questions
+              if !suggested_questions.is_empty() {
+                yield Ok(StreamData::new(
+                  json!(QuestionStreamValue::Answer { value: "\n\n".to_string() }),
+                  None,
+                  "\n\n".to_string()
+                ));
+
+                yield Ok(StreamData::new(
+                  json!(QuestionStreamValue::SuggestedQuestion { context_suggested_questions: suggested_questions.clone() }),
+                  None,
+                  String::new(),
+                ));
+
+                // Yield each question separately with a newline
+                // simulate stream effect
+                for (i, question) in suggested_questions.iter().enumerate() {
+                  tokio::time::sleep(tokio::time::Duration::from_millis(200)).await;
+                  let formatted_question = format!("{}. {}\n", i + 1, question.content);
+                  yield Ok(StreamData::new(
+                    json!(QuestionStreamValue::Answer { value: formatted_question.clone() }),
+                    None,
+                    formatted_question
+                  ));
+                  tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
+                }
+
+                yield Ok(StreamData::new(
+                  json!(QuestionStreamValue::FollowUp { should_generate_related_question: false }),
+                  None,
+                  String::new(),
+                ));
+              }
+            }))
+          },
+        };
       },
     };
 
@@ -239,16 +363,19 @@ impl Chain for ConversationalRetrieverChain {
     let memory = self.memory.clone();
     let complete_ai_message = Arc::new(Mutex::new(String::new()));
     let complete_ai_message_clone = complete_ai_message.clone();
+
     let output_stream = stream! {
         pin_mut!(stream);
         while let Some(result) = stream.next().await {
             match result {
                 Ok(data) => {
-                    let mut complete_ai_message_clone =
-                        complete_ai_message_clone.lock().await;
-                    complete_ai_message_clone.push_str(&data.content);
-
-                    yield Ok(data);
+                    let mut ai_message = complete_ai_message_clone.lock().await;
+                    ai_message.push_str(&data.content);
+                    yield Ok(StreamData::new(
+                        json!(QuestionStreamValue::Answer { value: data.content.clone() }),
+                        data.tokens,
+                        data.content,
+                    ));
                 },
                 Err(e) => {
                     yield Err(e);
@@ -256,17 +383,20 @@ impl Chain for ConversationalRetrieverChain {
             }
         }
 
+        // Stream source metadata after content
         for source in sources {
-          yield Ok(StreamData::new(
-              json!({"source": source}),
-              None,
-              "".to_string(),
-          ));
+            yield Ok(StreamData::new(
+                json!(source),
+                None,
+                String::new(),
+            ));
         }
 
+        // Update memory with the conversation
         let mut memory = memory.lock().await;
         memory.add_message(human_message).await;
-        memory.add_message(Message::new_ai_message(&complete_ai_message.lock().await)).await;
+        let complete_message = complete_ai_message.lock().await;
+        memory.add_message(Message::new_ai_message(&complete_message)).await;
     };
 
     Ok(Box::pin(output_stream))
@@ -294,38 +424,38 @@ impl Chain for ConversationalRetrieverChain {
 }
 
 pub struct ConversationalRetrieverChainBuilder {
-  llm: Option<LLMOllama>,
-  retriever: Option<AFRetriever>,
+  workspace_id: Uuid,
+  llm: LLMOllama,
+  retriever: AFRetriever,
   memory: Option<Arc<Mutex<dyn BaseMemory>>>,
-  combine_documents_chain: Option<Box<dyn Chain>>,
-  condense_question_chain: Option<Box<dyn Chain>>,
   prompt: Option<Box<dyn FormatPrompter>>,
   rephrase_question: bool,
   return_source_documents: bool,
   input_key: String,
   output_key: String,
+  store: Option<SqliteVectorStore>,
 }
 impl ConversationalRetrieverChainBuilder {
-  pub fn new() -> Self {
+  pub fn new(
+    workspace_id: Uuid,
+    llm: LLMOllama,
+    retriever: AFRetriever,
+    store: Option<SqliteVectorStore>,
+  ) -> Self {
     ConversationalRetrieverChainBuilder {
-      llm: None,
-      retriever: None,
+      workspace_id,
+      llm,
+      retriever,
       memory: None,
-      combine_documents_chain: None,
-      condense_question_chain: None,
       prompt: None,
       rephrase_question: true,
       return_source_documents: true,
       input_key: CONVERSATIONAL_RETRIEVAL_QA_DEFAULT_INPUT_KEY.to_string(),
       output_key: DEFAULT_OUTPUT_KEY.to_string(),
+      store,
     }
   }
 
-  pub fn retriever(mut self, retriever: AFRetriever) -> Self {
-    self.retriever = Some(retriever);
-    self
-  }
-
   ///If you want to add a custom prompt,keep in mind which variables are obligatory.
   pub fn prompt<P: Into<Box<dyn FormatPrompter>>>(mut self, prompt: P) -> Self {
     self.prompt = Some(prompt.into());
@@ -337,31 +467,6 @@ impl ConversationalRetrieverChainBuilder {
     self
   }
 
-  pub fn llm(mut self, llm: LLMOllama) -> Self {
-    self.llm = Some(llm);
-    self
-  }
-
-  ///Chain designed to take the documents and the question and generate an output
-  #[allow(dead_code)]
-  pub fn combine_documents_chain<C: Into<Box<dyn Chain>>>(
-    mut self,
-    combine_documents_chain: C,
-  ) -> Self {
-    self.combine_documents_chain = Some(combine_documents_chain.into());
-    self
-  }
-
-  ///Chain designed to reformulate the question based on the cat history
-  #[allow(dead_code)]
-  pub fn condense_question_chain<C: Into<Box<dyn Chain>>>(
-    mut self,
-    condense_question_chain: C,
-  ) -> Self {
-    self.condense_question_chain = Some(condense_question_chain.into());
-    self
-  }
-
   pub fn rephrase_question(mut self, rephrase_question: bool) -> Self {
     self.rephrase_question = rephrase_question;
     self
@@ -373,41 +478,33 @@ impl ConversationalRetrieverChainBuilder {
     self
   }
 
-  pub fn build(mut self) -> Result<ConversationalRetrieverChain, ChainError> {
-    if let Some(llm) = self.llm.as_ref() {
-      let combine_documents_chain = {
-        let mut builder = StuffDocumentBuilder::new().llm(llm.clone());
-        if let Some(prompt) = self.prompt {
-          builder = builder.prompt(prompt);
-        }
-        builder.build()?
-      };
-      let condense_question_chain = CondenseQuestionGeneratorChain::new(llm.clone());
-      self.combine_documents_chain = Some(Box::new(combine_documents_chain));
-      self.condense_question_chain = Some(Box::new(condense_question_chain));
-    }
-
-    let retriever = self
-      .retriever
-      .ok_or_else(|| ChainError::MissingObject("Retriever must be set".into()))?;
+  pub fn build(self) -> FlowyResult<ConversationalRetrieverChain> {
+    let combine_documents_chain = {
+      let mut builder = StuffDocumentBuilder::new().llm(self.llm.clone());
+      if let Some(prompt) = self.prompt {
+        builder = builder.prompt(prompt);
+      }
+      builder
+        .build()
+        .map_err(|err| FlowyError::local_ai().with_context(err))?
+    };
 
+    let condense_question_chain = CondenseQuestionGeneratorChain::new(self.llm.clone());
     let memory = self
       .memory
       .unwrap_or_else(|| Arc::new(Mutex::new(SimpleMemory::new())));
 
-    let combine_documents_chain = self.combine_documents_chain.ok_or_else(|| {
-      ChainError::MissingObject("Combine documents chain must be set or llm must be set".into())
-    })?;
-    let condense_question_chain = self.condense_question_chain.ok_or_else(|| {
-      ChainError::MissingObject("Condense question chain must be set or llm must be set".into())
-    })?;
+    let context_question_chain = self
+      .store
+      .map(|store| RelatedQuestionChain::new(self.workspace_id, self.llm.clone(), store));
 
     Ok(ConversationalRetrieverChain {
-      ollama: self.llm.unwrap(),
-      retriever,
+      ollama: self.llm,
+      retriever: self.retriever,
       memory,
-      combine_documents_chain,
-      condense_question_chain,
+      combine_documents_chain: Box::new(combine_documents_chain),
+      condense_question_chain: Box::new(condense_question_chain),
+      context_question_chain,
       rephrase_question: self.rephrase_question,
       return_source_documents: self.return_source_documents,
       input_key: self.input_key,
@@ -419,18 +516,18 @@ impl ConversationalRetrieverChainBuilder {
 // Retriever is a retriever for vector stores.
 pub type RetrieverOption = VecStoreOptions<Value>;
 pub struct AFRetriever {
-  vector_store: Box<dyn VectorStore<Options = RetrieverOption>>,
+  vector_store: Option<Box<dyn VectorStore<Options = RetrieverOption>>>,
   num_docs: usize,
   options: RetrieverOption,
 }
 impl AFRetriever {
   pub fn new<V: Into<Box<dyn VectorStore<Options = RetrieverOption>>>>(
-    vector_store: V,
+    vector_store: Option<V>,
     num_docs: usize,
     options: RetrieverOption,
   ) -> Self {
     AFRetriever {
-      vector_store: vector_store.into(),
+      vector_store: vector_store.map(Into::into),
       num_docs,
       options,
     }
@@ -461,19 +558,29 @@ impl Retriever for AFRetriever {
       self.options.filters,
       query,
     );
-    self
-      .vector_store
-      .similarity_search(query, self.num_docs, &self.options)
-      .await
+
+    match self.vector_store.as_ref() {
+      None => Ok(vec![]),
+      Some(vector_store) => {
+        vector_store
+          .similarity_search(query, self.num_docs, &self.options)
+          .await
+      },
+    }
   }
 }
 
 /// Deduplicates metadata from a list of documents by merging metadata entries with the same keys
-fn deduplicate_metadata(documents: &[Document]) -> Vec<Value> {
-  let mut merged_metadata: HashMap<String, Value> = HashMap::new();
+fn deduplicate_metadata(documents: &[Document]) -> Vec<QuestionStreamValue> {
+  let mut merged_metadata: HashMap<String, QuestionStreamValue> = HashMap::new();
   for document in documents {
     if let Some(object_id) = document.metadata.get(SOURCE_ID).and_then(|s| s.as_str()) {
-      merged_metadata.insert(object_id.to_string(), json!(document.metadata.clone()));
+      merged_metadata.insert(
+        object_id.to_string(),
+        QuestionStreamValue::Metadata {
+          value: json!(document.metadata.clone()),
+        },
+      );
     }
   }
   merged_metadata.into_values().collect()
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/chat/chains/mod.rs b/frontend/rust-lib/flowy-ai/src/local_ai/chat/chains/mod.rs
new file mode 100644
index 0000000000000..15641a0db289c
--- /dev/null
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/chat/chains/mod.rs
@@ -0,0 +1,3 @@
+pub mod context_question_chain;
+pub mod conversation_chain;
+pub mod related_question_chain;
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/chat/related_question_chain.rs b/frontend/rust-lib/flowy-ai/src/local_ai/chat/chains/related_question_chain.rs
similarity index 100%
rename from frontend/rust-lib/flowy-ai/src/local_ai/chat/related_question_chain.rs
rename to frontend/rust-lib/flowy-ai/src/local_ai/chat/chains/related_question_chain.rs
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/chat/format_prompt.rs b/frontend/rust-lib/flowy-ai/src/local_ai/chat/format_prompt.rs
index fb29ea7b7766c..17683e9d08cc8 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/chat/format_prompt.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/chat/format_prompt.rs
@@ -8,7 +8,7 @@ use langchain_rust::schemas::{Message, PromptValue};
 use langchain_rust::template_jinja2;
 use std::sync::{Arc, RwLock};
 
-const DEFAULT_QA_TEMPLATE: &str = r#"
+const QA_CONTEXT_TEMPLATE: &str = r#"
 Only Use the context provided below to formulate your answer. Do not use any other information. If the context doesn't contain sufficient information to answer the question, respond with "I don't know".
 Do not reference external knowledge or information outside the context.
 
@@ -19,32 +19,55 @@ Question:{{question}}
 Answer:
 "#;
 
+const QA_TEMPLATE: &str = r#"
+Question:{{question}}
+Answer:
+"#;
+
+fn template_from_rag_ids(rag_ids: &[String]) -> HumanMessagePromptTemplate {
+  if rag_ids.is_empty() {
+    HumanMessagePromptTemplate::new(template_jinja2!(QA_TEMPLATE, "question"))
+  } else {
+    HumanMessagePromptTemplate::new(template_jinja2!(QA_CONTEXT_TEMPLATE, "context", "question"))
+  }
+}
+
 struct FormatState {
   format_msg: Arc<Message>,
   format: ResponseFormat,
 }
 
-pub struct AFMessageFormatter {
+pub struct AFContextPrompt {
+  rag_ids: Vec<String>,
   system_msg: Arc<Message>,
   state: Arc<RwLock<FormatState>>,
-  user_tmpl: Arc<HumanMessagePromptTemplate>,
+  user_tmpl: Arc<RwLock<HumanMessagePromptTemplate>>,
 }
 
-impl AFMessageFormatter {
-  pub fn new(system_msg: Message, fmt: &ResponseFormat) -> Self {
-    // Compile the Jinja template exactly once
-    let user_tmpl =
-      HumanMessagePromptTemplate::new(template_jinja2!(DEFAULT_QA_TEMPLATE, "context", "question"));
-
+impl AFContextPrompt {
+  pub fn new(system_msg: Message, fmt: &ResponseFormat, rag_ids: &[String]) -> Self {
+    let user_tmpl = template_from_rag_ids(rag_ids);
     let state = FormatState {
       format_msg: Arc::new(format_prompt(fmt)),
       format: fmt.clone(),
     };
 
     Self {
+      rag_ids: rag_ids.to_vec(),
       system_msg: Arc::new(system_msg),
       state: Arc::new(RwLock::new(state)),
-      user_tmpl: Arc::new(user_tmpl),
+      user_tmpl: Arc::new(RwLock::new(user_tmpl)),
+    }
+  }
+
+  pub fn set_rag_ids(&mut self, rag_ids: &[String]) {
+    if self.rag_ids == rag_ids {
+      return;
+    }
+    self.rag_ids = rag_ids.to_vec();
+    let template = template_from_rag_ids(rag_ids);
+    if let Ok(mut w) = self.user_tmpl.try_write() {
+      *w = template;
     }
   }
 
@@ -64,9 +87,10 @@ impl AFMessageFormatter {
   }
 }
 
-impl Clone for AFMessageFormatter {
+impl Clone for AFContextPrompt {
   fn clone(&self) -> Self {
     Self {
+      rag_ids: self.rag_ids.clone(),
       system_msg: Arc::clone(&self.system_msg),
       state: Arc::clone(&self.state),
       user_tmpl: Arc::clone(&self.user_tmpl),
@@ -74,7 +98,7 @@ impl Clone for AFMessageFormatter {
   }
 }
 
-impl MessageFormatter for AFMessageFormatter {
+impl MessageFormatter for AFContextPrompt {
   fn format_messages(&self, args: PromptArgs) -> Result<Vec<Message>, PromptError> {
     let mut out = Vec::with_capacity(3);
     out.push((*self.system_msg).clone());
@@ -83,7 +107,10 @@ impl MessageFormatter for AFMessageFormatter {
       out.push((*st.format_msg).clone());
     }
 
-    out.extend(self.user_tmpl.format_messages(args)?);
+    if let Ok(user_tmpl) = self.user_tmpl.try_read() {
+      out.extend(user_tmpl.format_messages(args)?);
+    }
+
     Ok(out)
   }
 
@@ -92,7 +119,7 @@ impl MessageFormatter for AFMessageFormatter {
   }
 }
 
-impl FormatPrompter for AFMessageFormatter {
+impl FormatPrompter for AFContextPrompt {
   fn format_prompt(&self, input_variables: PromptArgs) -> Result<PromptValue, PromptError> {
     let messages = self.format_messages(input_variables)?;
     Ok(PromptValue::from_messages(messages))
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/chat/llm_chat.rs b/frontend/rust-lib/flowy-ai/src/local_ai/chat/llm_chat.rs
index 542b62056777c..289e31d389bf8 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/chat/llm_chat.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/chat/llm_chat.rs
@@ -1,13 +1,14 @@
-use crate::local_ai::chat::conversation_chain::{
+use crate::local_ai::chat::chains::conversation_chain::{
   AFRetriever, ConversationalRetrieverChain, ConversationalRetrieverChainBuilder, RetrieverOption,
 };
-use crate::local_ai::chat::format_prompt::AFMessageFormatter;
+use crate::local_ai::chat::chains::related_question_chain::RelatedQuestionChain;
+use crate::local_ai::chat::format_prompt::AFContextPrompt;
 use crate::local_ai::chat::llm::LLMOllama;
 use crate::local_ai::chat::summary_memory::SummaryMemory;
-use crate::local_ai::chat::{LLMChatInfo, OllamaClientRef};
+use crate::local_ai::chat::LLMChatInfo;
 use crate::SqliteVectorStore;
 use flowy_ai_pub::cloud::{QuestionStreamValue, ResponseFormat, StreamAnswer};
-use flowy_ai_pub::entities::{RAG_IDS, SOURCE_ID};
+use flowy_ai_pub::entities::{RAG_IDS, SOURCE_ID, WORKSPACE_ID};
 use flowy_ai_pub::user_service::AIUserService;
 use flowy_error::{FlowyError, FlowyResult};
 use futures::StreamExt;
@@ -16,70 +17,74 @@ use langchain_rust::memory::SimpleMemory;
 use langchain_rust::prompt_args;
 use langchain_rust::schemas::{Document, Message};
 use langchain_rust::vectorstore::{VecStoreOptions, VectorStore};
+use ollama_rs::Ollama;
 use serde_json::json;
 use std::collections::HashMap;
-use std::sync::Weak;
-use tracing::{info, trace};
+use std::sync::{Arc, Weak};
+use tracing::trace;
 use uuid::Uuid;
 
 pub struct LLMChat {
   store: Option<SqliteVectorStore>,
   chain: ConversationalRetrieverChain,
-  #[allow(dead_code)]
-  client: OllamaClientRef,
-  formatter: AFMessageFormatter,
+  client: Arc<Ollama>,
+  prompt: AFContextPrompt,
   info: LLMChatInfo,
 }
 
 impl LLMChat {
-  pub async fn new(
+  pub fn new(
     info: LLMChatInfo,
-    client: OllamaClientRef,
+    client: Arc<Ollama>,
     store: Option<SqliteVectorStore>,
     user_service: Option<Weak<dyn AIUserService>>,
   ) -> FlowyResult<Self> {
     let response_format = ResponseFormat::default();
-    let formatter = create_formatter_prompt_with_format(&response_format);
-    let llm = create_llm(&client, &info.model).await?;
-    let summary_llm = create_llm(&client, &info.model).await?;
+    let formatter = create_formatter_prompt_with_format(&response_format, &info.rag_ids);
+    let llm = LLMOllama::new(&info.model, client.clone(), None, None);
+    let summary_llm = LLMOllama::new(&info.model, client.clone(), None, None);
     let memory = SummaryMemory::new(summary_llm, info.summary.clone(), user_service)
-      .await
       .map(|v| v.into())
       .unwrap_or(SimpleMemory::new().into());
 
-    let mut builder = ConversationalRetrieverChainBuilder::new()
-      .llm(llm)
-      .rephrase_question(false)
-      .memory(memory);
-
-    if let Some(store) = store.clone() {
-      let retriever = create_retriever(&info.workspace_id, info.rag_ids.clone(), store);
-      builder = builder.retriever(retriever);
-    }
-
-    let chain = builder
-      .prompt(formatter.clone())
-      .build()
-      .map_err(|err| FlowyError::local_ai().with_context(err))?;
+    let retriever = create_retriever(&info.workspace_id, info.rag_ids.clone(), store.clone());
+    let builder =
+      ConversationalRetrieverChainBuilder::new(info.workspace_id, llm, retriever, store.clone())
+        .rephrase_question(false)
+        .memory(memory);
 
+    let chain = builder.prompt(formatter.clone()).build()?;
     Ok(Self {
       store,
       chain,
       client,
-      formatter,
+      prompt: formatter,
       info,
     })
   }
 
+  pub async fn get_related_question(&self, user_message: String) -> FlowyResult<Vec<String>> {
+    let chain = RelatedQuestionChain::new(LLMOllama::new(
+      &self.info.model,
+      self.client.clone(),
+      None,
+      None,
+    ));
+    let questions = chain.related_question(&user_message).await?;
+    trace!(
+      "related questions: {:?} for message: {}",
+      questions,
+      user_message
+    );
+    Ok(questions)
+  }
+
   pub fn set_chat_model(&mut self, model: &str) {
     self.chain.ollama.set_model(model);
   }
 
-  pub async fn set_rag_ids(&mut self, rag_ids: Vec<String>) {
-    info!(
-      "[VectorStore]: {} set rag ids: {:?}",
-      self.info.chat_id, rag_ids
-    );
+  pub fn set_rag_ids(&mut self, rag_ids: Vec<String>) {
+    self.prompt.set_rag_ids(&rag_ids);
     self.chain.retriever.set_rag_ids(rag_ids);
   }
 
@@ -126,7 +131,7 @@ impl LLMChat {
     paragraphs: Vec<String>,
   ) -> FlowyResult<()> {
     let mut metadata = HashMap::new();
-    metadata.insert("workspace_id".to_string(), json!(self.info.workspace_id));
+    metadata.insert(WORKSPACE_ID.to_string(), json!(self.info.workspace_id));
     metadata.insert(SOURCE_ID.to_string(), json!(object_id));
     let document = Document::new(paragraphs.join("\n\n")).with_metadata(metadata);
     if let Some(store) = &self.store {
@@ -157,7 +162,8 @@ impl LLMChat {
     message: &str,
     format: ResponseFormat,
   ) -> Result<StreamAnswer, FlowyError> {
-    self.formatter.update_format(&format)?;
+    trace!("[chat]: {} stream question: {}", self.info.chat_id, message);
+    self.prompt.update_format(&format)?;
     let input_variables = prompt_args! {
         "question" => message,
     };
@@ -167,16 +173,11 @@ impl LLMChat {
     let transformed_stream = stream.map(|result| {
       result
         .map(|stream_data| {
-          if let Some(source) = stream_data.value.as_object().and_then(|v| v.get("source")) {
-            trace!("[VectorStore]: reference sources: {:?}", source);
-            QuestionStreamValue::Metadata {
-              value: source.clone(),
-            }
-          } else {
+          serde_json::from_value::<QuestionStreamValue>(stream_data.value).unwrap_or_else(|_| {
             QuestionStreamValue::Answer {
-              value: stream_data.content,
+              value: String::new(),
             }
-          }
+          })
         })
         .map_err(map_chain_error)
     });
@@ -184,17 +185,19 @@ impl LLMChat {
   }
 }
 
-fn create_formatter_prompt_with_format(format: &ResponseFormat) -> AFMessageFormatter {
+fn create_formatter_prompt_with_format(
+  format: &ResponseFormat,
+  rag_ids: &[String],
+) -> AFContextPrompt {
   let system_message =
     Message::new_system_message("You are an assistant for question-answering tasks");
-
-  AFMessageFormatter::new(system_message, format)
+  AFContextPrompt::new(system_message, format, rag_ids)
 }
 
 fn create_retriever(
   workspace_id: &Uuid,
   rag_ids: Vec<String>,
-  store: SqliteVectorStore,
+  store: Option<SqliteVectorStore>,
 ) -> AFRetriever {
   trace!(
     "[VectorStore]: {} create retriever with rag_ids: {:?}",
@@ -208,18 +211,6 @@ fn create_retriever(
   AFRetriever::new(store, 5, options)
 }
 
-async fn create_llm(client: &OllamaClientRef, model: &str) -> FlowyResult<LLMOllama> {
-  let read_guard = client.read().await;
-  let client = read_guard
-    .as_ref()
-    .ok_or_else(|| FlowyError::local_ai().with_context("Ollama client not initialized"))?
-    .upgrade()
-    .ok_or_else(|| FlowyError::local_ai().with_context("Ollama client has been dropped"))?
-    .clone();
-  let ollama = LLMOllama::new(model, client, None, None);
-  Ok(ollama)
-}
-
 fn map_chain_error(err: ChainError) -> FlowyError {
   match err {
     ChainError::MissingInputVariable(var) => {
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/chat/mod.rs b/frontend/rust-lib/flowy-ai/src/local_ai/chat/mod.rs
index a7844b980cc16..6dbca1155e521 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/chat/mod.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/chat/mod.rs
@@ -1,18 +1,17 @@
-mod conversation_chain;
+pub mod chains;
 mod format_prompt;
 pub mod llm;
 pub mod llm_chat;
-pub mod related_question_chain;
 mod summary_memory;
 
+use crate::local_ai::chat::chains::related_question_chain::RelatedQuestionChain;
 use crate::local_ai::chat::llm::LLMOllama;
 use crate::local_ai::chat::llm_chat::LLMChat;
-use crate::local_ai::chat::related_question_chain::RelatedQuestionChain;
 use crate::local_ai::completion::chain::CompletionChain;
 use crate::local_ai::database::summary::DatabaseSummaryChain;
 use crate::local_ai::database::translate::DatabaseTranslateChain;
 use crate::SqliteVectorStore;
-use dashmap::DashMap;
+use dashmap::{DashMap, Entry};
 use flowy_ai_pub::cloud::ai_dto::{TranslateRowData, TranslateRowResponse};
 use flowy_ai_pub::cloud::chat_dto::ChatAuthorType;
 use flowy_ai_pub::cloud::{
@@ -73,23 +72,34 @@ impl LLMChatController {
     *self.store.write().await = Some(store);
   }
 
-  pub async fn set_rag_ids(&self, chat_id: &Uuid, rag_ids: &[String]) {
+  pub fn set_rag_ids(&self, chat_id: &Uuid, rag_ids: &[String]) {
     if let Some(mut chat) = self.chat_by_id.get_mut(chat_id) {
-      chat.set_rag_ids(rag_ids.to_vec()).await;
+      chat.set_rag_ids(rag_ids.to_vec());
     }
   }
 
-  pub async fn open_chat(&self, info: LLMChatInfo) -> FlowyResult<()> {
+  async fn create_chat_if_not_exist(&self, info: LLMChatInfo) -> FlowyResult<()> {
     let store = self.store.read().await.clone();
-    let chat_id = info.chat_id;
-    let chat = LLMChat::new(
-      info,
-      self.client.clone(),
-      store,
-      Some(self.user_service.clone()),
-    )
-    .await?;
-    self.chat_by_id.insert(chat_id, chat);
+    let client = self
+      .client
+      .read()
+      .await
+      .as_ref()
+      .ok_or_else(|| FlowyError::local_ai().with_context("Ollama client not initialized"))?
+      .upgrade()
+      .ok_or_else(|| FlowyError::local_ai().with_context("Ollama client has been dropped"))?
+      .clone();
+    let entry = self.chat_by_id.entry(info.chat_id);
+
+    if let Entry::Vacant(e) = entry {
+      let chat = LLMChat::new(info, client, store, Some(self.user_service.clone()))?;
+      e.insert(chat);
+    }
+    Ok(())
+  }
+
+  pub async fn open_chat(&self, info: LLMChatInfo) -> FlowyResult<()> {
+    let _ = self.create_chat_if_not_exist(info).await;
     Ok(())
   }
 
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/chat/summary_memory.rs b/frontend/rust-lib/flowy-ai/src/local_ai/chat/summary_memory.rs
index b2d2a9403f9b9..de13b6f27fd1e 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/chat/summary_memory.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/chat/summary_memory.rs
@@ -17,7 +17,7 @@ pub struct SummaryMemory {
 }
 
 impl SummaryMemory {
-  pub async fn new(
+  pub fn new(
     llm: LLMOllama,
     summary: String,
     user_service: Option<Weak<dyn AIUserService>>,
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/controller.rs b/frontend/rust-lib/flowy-ai/src/local_ai/controller.rs
index 9aae9ae4c8825..1df2d68befe03 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/controller.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/controller.rs
@@ -26,7 +26,7 @@ use serde::{Deserialize, Serialize};
 use std::ops::Deref;
 use std::path::PathBuf;
 use std::sync::{Arc, Weak};
-use tracing::{debug, error, info, instrument};
+use tracing::{debug, error, info, instrument, trace};
 use uuid::Uuid;
 
 #[derive(Clone, Debug, Serialize, Deserialize)]
@@ -98,6 +98,13 @@ impl LocalAIController {
 
   pub async fn reload_ollama_client(&self, workspace_id: &str) {
     if !self.is_enabled_on_workspace(workspace_id) {
+      #[cfg(any(target_os = "windows", target_os = "macos", target_os = "linux"))]
+      {
+        trace!("[Local AI] local ai is disabled, clear ollama client",);
+        let shared = crate::embeddings::context::EmbedContext::shared();
+        shared.set_ollama(None);
+        self.ollama.store(None);
+      }
       return;
     }
 
@@ -182,7 +189,7 @@ impl LocalAIController {
       return;
     }
 
-    self.llm_controller.set_rag_ids(chat_id, rag_ids).await;
+    self.llm_controller.set_rag_ids(chat_id, rag_ids);
   }
 
   pub async fn open_chat(
diff --git a/frontend/rust-lib/flowy-ai/src/local_ai/resource.rs b/frontend/rust-lib/flowy-ai/src/local_ai/resource.rs
index e5cba112702fe..1e5a4369f05e4 100644
--- a/frontend/rust-lib/flowy-ai/src/local_ai/resource.rs
+++ b/frontend/rust-lib/flowy-ai/src/local_ai/resource.rs
@@ -1,21 +1,19 @@
 use crate::local_ai::controller::LocalAISetting;
-use flowy_error::{ErrorCode, FlowyError, FlowyResult};
+use flowy_error::{FlowyError, FlowyResult};
 use lib_infra::async_trait::async_trait;
 
 use crate::entities::LackOfAIResourcePB;
 use crate::notification::{
   chat_notification_builder, ChatNotification, APPFLOWY_AI_NOTIFICATION_KEY,
 };
-use af_local_ai::ollama_plugin::OllamaPluginConfig;
-use af_plugin::core::path::{is_plugin_ready, ollama_plugin_path};
 use flowy_ai_pub::user_service::AIUserService;
-use lib_infra::util::{get_operating_system, OperatingSystem};
+use lib_infra::util::get_operating_system;
 use reqwest::Client;
 use serde::Deserialize;
 use std::path::PathBuf;
 use std::sync::Arc;
 use std::time::Duration;
-use tracing::{error, info, instrument, trace};
+use tracing::{error, info, instrument};
 
 #[derive(Debug, Deserialize)]
 struct TagsResponse {
@@ -109,12 +107,6 @@ impl LocalAIResourceController {
   }
 
   pub async fn calculate_pending_resources(&self) -> FlowyResult<Option<PendingResource>> {
-    let app_path = ollama_plugin_path();
-    if !is_plugin_ready() {
-      trace!("[LLM Resource] offline app not found: {:?}", app_path);
-      return Ok(Some(PendingResource::PluginExecutableNotReady));
-    }
-
     let setting = self.get_llm_setting();
     let client = Client::builder().timeout(Duration::from_secs(5)).build()?;
     match client.get(&setting.ollama_server_url).send().await {
@@ -170,54 +162,6 @@ impl LocalAIResourceController {
     Ok(None)
   }
 
-  #[instrument(level = "info", skip_all)]
-  pub async fn get_plugin_config(&self, rag_enabled: bool) -> FlowyResult<OllamaPluginConfig> {
-    if !self.is_resource_ready().await {
-      return Err(FlowyError::new(
-        ErrorCode::AppFlowyLAINotReady,
-        "AppFlowyLAI not found",
-      ));
-    }
-
-    let llm_setting = self.get_llm_setting();
-    let bin_path = match get_operating_system() {
-      OperatingSystem::MacOS | OperatingSystem::Windows | OperatingSystem::Linux => {
-        ollama_plugin_path()
-      },
-      _ => {
-        return Err(
-          FlowyError::local_ai_unavailable()
-            .with_context("Local AI not available on current platform"),
-        );
-      },
-    };
-
-    let mut config = OllamaPluginConfig::new(
-      bin_path,
-      "af_ollama_plugin".to_string(),
-      llm_setting.chat_model_name.clone(),
-      llm_setting.embedding_model_name.clone(),
-      Some(llm_setting.ollama_server_url.clone()),
-    )?;
-
-    //config = config.with_log_level("debug".to_string());
-
-    if rag_enabled {
-      let resource_dir = self.resource_dir()?;
-      let persist_directory = resource_dir.join("vectorstore");
-      if !persist_directory.exists() {
-        std::fs::create_dir_all(&persist_directory)?;
-      }
-      config.set_rag_enabled(&persist_directory)?;
-    }
-
-    if cfg!(debug_assertions) {
-      config = config.with_verbose(true);
-    }
-    trace!("[AI Chat] config: {:?}", config);
-    Ok(config)
-  }
-
   pub(crate) fn user_model_folder(&self) -> FlowyResult<PathBuf> {
     self.resource_dir().map(|dir| dir.join(LLM_MODEL_DIR))
   }
diff --git a/frontend/rust-lib/flowy-ai/src/stream_message.rs b/frontend/rust-lib/flowy-ai/src/stream_message.rs
index 3f7b37bd3456c..8d3193dbd1317 100644
--- a/frontend/rust-lib/flowy-ai/src/stream_message.rs
+++ b/frontend/rust-lib/flowy-ai/src/stream_message.rs
@@ -1,3 +1,4 @@
+use serde::Serialize;
 use std::fmt::Display;
 
 #[allow(dead_code)]
@@ -5,8 +6,8 @@ pub enum StreamMessage {
   MessageId(i64),
   IndexStart,
   IndexEnd,
-  Text(String),
   OnData(String),
+  OnFollowUp(AIFollowUpData),
   OnError(String),
   Metadata(String),
   Done,
@@ -15,15 +16,17 @@ pub enum StreamMessage {
   IndexFileError { file_name: String },
 }
 
+#[derive(Debug, Clone, Serialize, Default)]
+pub struct AIFollowUpData {
+  pub should_generate_related_question: bool,
+}
+
 impl Display for StreamMessage {
   fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
     match self {
       StreamMessage::MessageId(message_id) => write!(f, "message_id:{}", message_id),
       StreamMessage::IndexStart => write!(f, "index_start:"),
       StreamMessage::IndexEnd => write!(f, "index_end"),
-      StreamMessage::Text(text) => {
-        write!(f, "data:{}", text)
-      },
       StreamMessage::OnData(message) => write!(f, "data:{message}"),
       StreamMessage::OnError(message) => write!(f, "error:{message}"),
       StreamMessage::Done => write!(f, "done:"),
@@ -37,6 +40,13 @@ impl Display for StreamMessage {
       StreamMessage::IndexFileError { file_name } => {
         write!(f, "index_file_error:{}", file_name)
       },
+      StreamMessage::OnFollowUp(data) => {
+        if let Ok(s) = serde_json::to_string(&data) {
+          write!(f, "ai_follow_up:{}", s)
+        } else {
+          write!(f, "ai_follow_up:",)
+        }
+      },
     }
   }
 }
diff --git a/frontend/rust-lib/flowy-ai/tests/asset/japan_trip.md b/frontend/rust-lib/flowy-ai/tests/asset/japan_trip.md
new file mode 100644
index 0000000000000..9e10acad1b66d
--- /dev/null
+++ b/frontend/rust-lib/flowy-ai/tests/asset/japan_trip.md
@@ -0,0 +1,23 @@
+Our trip begins with a flight from America to Tokyo on January 7th.
+In Tokyo, we’ll spend three days, from February 7th to 10th, exploring
+the city’s tech scene and snowboarding gear shops. We’ll visit popular
+spots like Shibuya, Shinjuku, and Odaiba before heading to our next
+destination.
+
+From Tokyo, we fly to Sendai and then travel to Zao Onsen for a 3-day
+stay from February 10th to 14th. Zao Onsen is famous for its beautiful
+snow and the iconic ice trees, which will make for a unique snowboarding
+experience.
+
+After Zao Onsen, we fly from Sendai to Chitose, then head to Sapporo for
+a 2-day visit, exploring the city’s vibrant atmosphere and winter
+attractions. On the next day, we’ll spend time at Sapporo Tein, a ski
+resort that offers great runs and stunning views of the city and the sea.
+
+Then we head to Rusutsu for 5 days, one of the top ski resorts in Japan,
+known for its deep powder snow and extensive runs. Finally, we’ll fly
+back to Singapore after experiencing some of the best snowboarding Japan
+has to offer.
+
+Ski resorts to visit include Niseko (二世谷), Rusutsu (留寿都), Sapporo Tein
+(札幌和海景), and Zao Onsen Ski Resort (冰树).
\ No newline at end of file
diff --git a/frontend/rust-lib/flowy-ai/tests/chat_test/mod.rs b/frontend/rust-lib/flowy-ai/tests/chat_test/mod.rs
index 5a21360781fc9..968f18804ecc6 100644
--- a/frontend/rust-lib/flowy-ai/tests/chat_test/mod.rs
+++ b/frontend/rust-lib/flowy-ai/tests/chat_test/mod.rs
@@ -1 +1,2 @@
 mod qa_test;
+mod related_question_test;
diff --git a/frontend/rust-lib/flowy-ai/tests/chat_test/qa_test.rs b/frontend/rust-lib/flowy-ai/tests/chat_test/qa_test.rs
index ec32237faf293..44db0a7d1b11d 100644
--- a/frontend/rust-lib/flowy-ai/tests/chat_test/qa_test.rs
+++ b/frontend/rust-lib/flowy-ai/tests/chat_test/qa_test.rs
@@ -1,10 +1,12 @@
-use crate::{setup_log, TestContext};
+use crate::{collect_stream, load_asset_content, setup_log, TestContext};
+use flowy_ai::local_ai::chat::chains::conversation_chain::{
+  ANSWER_WITH_SUGGESTED_QUESTION, CAN_NOT_ANSWER_WITH_CONTEXT,
+};
+use flowy_ai::local_ai::chat::chains::related_question_chain::RelatedQuestionChain;
 use flowy_ai::local_ai::chat::llm::LLMOllama;
-use flowy_ai::local_ai::chat::related_question_chain::RelatedQuestionChain;
-use flowy_ai_pub::cloud::{OutputLayout, QuestionStreamValue, ResponseFormat, StreamAnswer};
+use flowy_ai_pub::cloud::{OutputLayout, ResponseFormat};
 use flowy_ai_pub::entities::{SOURCE, SOURCE_ID, SOURCE_NAME};
-use serde_json::Value;
-use tokio_stream::StreamExt;
+use uuid::Uuid;
 
 #[tokio::test]
 async fn local_ollama_test_simple_question() {
@@ -16,6 +18,33 @@ async fn local_ollama_test_simple_question() {
     .unwrap();
   let result = collect_stream(stream).await;
   dbg!(result);
+
+  let doc_id = Uuid::new_v4();
+  // set rag_id but not rag document content, should return CAN_NOT_ANSWER_WITH_CONTEXT
+  chat.set_rag_ids(vec![doc_id.to_string()]);
+  let stream = chat
+    .stream_question("hello world", Default::default())
+    .await
+    .unwrap();
+  let result = collect_stream(stream).await;
+  dbg!(&result);
+  assert!(result.answer.starts_with(CAN_NOT_ANSWER_WITH_CONTEXT));
+  assert!(result.gen_related_question);
+
+  // Update the rag document content
+  let trip_docs = load_asset_content("japan_trip.md");
+  chat
+    .embed_paragraphs(&doc_id.to_string(), vec![trip_docs])
+    .await
+    .unwrap();
+  let stream = chat
+    .stream_question("hello world", Default::default())
+    .await
+    .unwrap();
+  let result = collect_stream(stream).await;
+  dbg!(&result);
+  assert!(result.answer.starts_with(ANSWER_WITH_SUGGESTED_QUESTION));
+  assert!(!result.gen_related_question);
 }
 
 #[tokio::test]
@@ -30,7 +59,7 @@ async fn local_ollama_test_chat_with_multiple_docs_retrieve() {
         ids.push(id.to_string());
         chat.embed_paragraphs(&id.to_string(), vec![doc.to_string()]).await.unwrap();
     }
-  chat.set_rag_ids(ids.clone()).await;
+  chat.set_rag_ids(ids.clone());
 
   let all_docs = chat.get_all_embedded_documents().await.unwrap();
   assert_eq!(all_docs.len(), 3);
@@ -58,22 +87,26 @@ async fn local_ollama_test_chat_with_multiple_docs_retrieve() {
     .stream_question("Rust is a multiplayer survival game", Default::default())
     .await
     .unwrap();
-  let (answer, sources) = collect_stream(stream).await;
-  dbg!(&answer);
-  dbg!(&sources);
-
-  assert!(!answer.is_empty());
-  assert!(!sources.is_empty());
-  assert!(sources[0].get(SOURCE_ID).unwrap().as_str().is_some());
-  assert!(sources[0].get(SOURCE).unwrap().as_str().is_some());
-  assert!(sources[0].get(SOURCE_NAME).unwrap().as_str().is_some());
+  let result = collect_stream(stream).await;
+  dbg!(&result);
+  dbg!(&result.sources);
+
+  assert!(!result.answer.is_empty());
+  assert!(!result.sources.is_empty());
+  assert!(result.sources[0].get(SOURCE_ID).unwrap().as_str().is_some());
+  assert!(result.sources[0].get(SOURCE).unwrap().as_str().is_some());
+  assert!(result.sources[0]
+    .get(SOURCE_NAME)
+    .unwrap()
+    .as_str()
+    .is_some());
 
   let stream = chat
     .stream_question("Japan ski resort", Default::default())
     .await
     .unwrap();
-  let (answer, _) = collect_stream(stream).await;
-  dbg!(&answer);
+  let result = collect_stream(stream).await;
+  dbg!(&result);
 }
 
 #[tokio::test]
@@ -87,34 +120,10 @@ async fn local_ollama_test_chat_format() {
     .stream_question("Compare rust and js", format)
     .await
     .unwrap();
-  let (answer, _) = collect_stream(stream).await;
-  dbg!(&answer);
-  assert!(!answer.is_empty());
-}
-
-async fn collect_stream(stream: StreamAnswer) -> (String, Vec<Value>) {
-  let mut result = String::new();
-  let mut sources = vec![];
-  let mut stream = stream;
-  while let Some(chunk) = stream.next().await {
-    match chunk {
-      Ok(value) => match value {
-        QuestionStreamValue::Answer { value } => {
-          result.push_str(&value);
-        },
-        QuestionStreamValue::Metadata { value } => {
-          dbg!("metadata", &value);
-          sources.push(value);
-        },
-        QuestionStreamValue::KeepAlive => {},
-      },
-      Err(e) => {
-        eprintln!("Error: {}", e);
-      },
-    }
-  }
-
-  (result, sources)
+  let result = collect_stream(stream).await;
+  dbg!(&result);
+  assert!(!result.answer.is_empty());
+  assert!(result.gen_related_question);
 }
 
 #[tokio::test]
diff --git a/frontend/rust-lib/flowy-ai/tests/chat_test/related_question_test.rs b/frontend/rust-lib/flowy-ai/tests/chat_test/related_question_test.rs
new file mode 100644
index 0000000000000..1b955f759ca82
--- /dev/null
+++ b/frontend/rust-lib/flowy-ai/tests/chat_test/related_question_test.rs
@@ -0,0 +1,48 @@
+use crate::{collect_stream, load_asset_content, TestContext};
+use uuid::Uuid;
+
+#[tokio::test]
+async fn local_ollama_test_context_related_questions() {
+  let context = TestContext::new().unwrap();
+  let mut chat = context.create_chat(vec![]).await;
+  let stream = chat
+    .stream_question("hello world", Default::default())
+    .await
+    .unwrap();
+  let result = collect_stream(stream).await;
+  assert!(!result.answer.is_empty());
+
+  let doc_id = Uuid::new_v4().to_string();
+  let trip_docs = load_asset_content("japan_trip.md");
+  chat.set_rag_ids(vec![doc_id.clone()]);
+  chat
+    .embed_paragraphs(&doc_id, vec![trip_docs])
+    .await
+    .unwrap();
+
+  let stream = chat
+    .stream_question("Compare rust with js", Default::default())
+    .await
+    .unwrap();
+  let result = collect_stream(stream).await;
+  dbg!(&result.suggested_questions);
+  assert_eq!(result.suggested_questions.len(), 3);
+  assert!(!result.gen_related_question);
+
+  // all suggested questions' object id should equal to doc_id
+  for question in result.suggested_questions.iter() {
+    assert_eq!(question.object_id, doc_id);
+  }
+
+  let stream = chat
+    .stream_question(
+      result.suggested_questions[0].content.as_str(),
+      Default::default(),
+    )
+    .await
+    .unwrap();
+  let result = collect_stream(stream).await;
+  dbg!(&result);
+  assert!(result.suggested_questions.is_empty());
+  assert!(result.gen_related_question);
+}
diff --git a/frontend/rust-lib/flowy-ai/tests/main.rs b/frontend/rust-lib/flowy-ai/tests/main.rs
index 565148a0dd240..e5a91d93ef23c 100644
--- a/frontend/rust-lib/flowy-ai/tests/main.rs
+++ b/frontend/rust-lib/flowy-ai/tests/main.rs
@@ -6,12 +6,14 @@ mod translate_test;
 use flowy_ai::local_ai::chat::llm_chat::LLMChat;
 use flowy_ai::local_ai::chat::LLMChatInfo;
 use flowy_ai::SqliteVectorStore;
+use flowy_ai_pub::cloud::{ContextSuggestedQuestion, QuestionStreamValue, StreamAnswer};
 use flowy_sqlite_vec::db::VectorSqliteDB;
 use langchain_rust::url::Url;
 use ollama_rs::Ollama;
+use serde_json::Value;
 use std::sync::{Arc, Once};
 use tempfile::tempdir;
-use tokio::sync::RwLock;
+use tokio_stream::StreamExt;
 use tracing_subscriber::fmt::Subscriber;
 use tracing_subscriber::util::SubscriberInitExt;
 use tracing_subscriber::EnvFilter;
@@ -66,7 +68,6 @@ impl TestContext {
     let workspace_id = Uuid::new_v4();
     let chat_id = Uuid::new_v4();
     let model = "llama3.1";
-    let ollama_client = Arc::new(RwLock::new(Some(Arc::downgrade(&self.ollama))));
     let info = LLMChatInfo {
       chat_id,
       workspace_id,
@@ -75,8 +76,63 @@ impl TestContext {
       summary: "".to_string(),
     };
 
-    LLMChat::new(info, ollama_client, Some(self.store.clone()), None)
-      .await
-      .unwrap()
+    LLMChat::new(info, self.ollama.clone(), Some(self.store.clone()), None).unwrap()
   }
 }
+
+#[derive(Debug)]
+pub struct StreamResult {
+  pub answer: String,
+  pub sources: Vec<Value>,
+  pub suggested_questions: Vec<ContextSuggestedQuestion>,
+  pub gen_related_question: bool,
+}
+
+pub async fn collect_stream(stream: StreamAnswer) -> StreamResult {
+  let mut result = String::new();
+  let mut sources = vec![];
+  let mut gen_related_question = true;
+  let mut suggested_questions = vec![];
+  let mut stream = stream;
+  while let Some(chunk) = stream.next().await {
+    match chunk {
+      Ok(value) => match value {
+        QuestionStreamValue::Answer { value } => {
+          result.push_str(&value);
+        },
+        QuestionStreamValue::Metadata { value } => {
+          dbg!("metadata", &value);
+          sources.push(value);
+        },
+
+        QuestionStreamValue::SuggestedQuestion {
+          context_suggested_questions,
+        } => {
+          suggested_questions = context_suggested_questions;
+        },
+        QuestionStreamValue::FollowUp {
+          should_generate_related_question,
+        } => {
+          gen_related_question = should_generate_related_question;
+        },
+      },
+      Err(e) => {
+        eprintln!("Error: {}", e);
+      },
+    }
+  }
+
+  StreamResult {
+    answer: result,
+    sources,
+    suggested_questions,
+    gen_related_question,
+  }
+}
+
+pub fn load_asset_content(name: &str) -> String {
+  let path = format!("tests/asset/{}", name);
+  std::fs::read_to_string(path).unwrap_or_else(|_| {
+    panic!("Failed to read asset file: {}", name);
+  })
+}
diff --git a/frontend/rust-lib/flowy-core/src/deps_resolve/chat_deps.rs b/frontend/rust-lib/flowy-core/src/deps_resolve/chat_deps.rs
index 895527013d24a..5a954504a2a83 100644
--- a/frontend/rust-lib/flowy-core/src/deps_resolve/chat_deps.rs
+++ b/frontend/rust-lib/flowy-core/src/deps_resolve/chat_deps.rs
@@ -17,12 +17,13 @@ use flowy_sqlite::kv::KVStorePreferences;
 use flowy_sqlite::DBConnection;
 use flowy_storage_pub::storage::StorageService;
 use flowy_user::services::authenticate_user::AuthenticateUser;
+use flowy_user_pub::entities::WorkspaceType;
 use lib_infra::async_trait::async_trait;
 use lib_infra::util::timestamp;
 use std::collections::HashMap;
 use std::path::PathBuf;
 use std::sync::{Arc, Weak};
-use tracing::{error, info};
+use tracing::{debug, error, info};
 use uuid::Uuid;
 
 pub struct ChatDepsResolver;
@@ -83,6 +84,7 @@ impl AIExternalService for ChatQueryServiceImpl {
   ) -> Result<Vec<AFCollabMetadata>, FlowyError> {
     let mut result = Vec::new();
 
+    info!("[Embedding] sync rag documents: {:?}", rag_ids);
     for rag_id in rag_ids {
       // Retrieve the collab object for the current rag_id
       let query_collab = match self
@@ -92,6 +94,10 @@ impl AIExternalService for ChatQueryServiceImpl {
       {
         Some(collab) => collab,
         None => {
+          debug!(
+            "[Embedding] can not find collab data, skip sync rag document: {}",
+            rag_id
+          );
           continue;
         },
       };
@@ -99,17 +105,20 @@ impl AIExternalService for ChatQueryServiceImpl {
       // Check if the state vector exists and detect changes
       if let Some(metadata) = rag_metadata_map.remove(&rag_id) {
         if let Ok(prev_sv) = StateVector::decode_v1(&metadata.prev_sync_state_vector) {
-          let collab = Collab::new_with_source(
+          if let Ok(collab) = Collab::new_with_source(
             CollabOrigin::Empty,
             &rag_id.to_string(),
             DataSource::DocStateV1(query_collab.encoded_collab.doc_state.to_vec()),
             vec![],
             false,
-          )?;
-
-          if !is_change_since_sv(&collab, &prev_sv) {
-            info!("[Embedding] skip full sync {}, no changes", rag_id);
-            continue;
+          ) {
+            if !is_change_since_sv(&collab, &prev_sv) {
+              info!(
+                "[Embedding] skip full sync rag document {}, no changes",
+                rag_id
+              );
+              continue;
+            }
           }
         }
       }
@@ -127,7 +136,10 @@ impl AIExternalService for ChatQueryServiceImpl {
         .full_sync_collab_object(workspace_id, params)
         .await
       {
-        error!("Failed to sync rag document: {} error: {}", rag_id, err);
+        error!(
+          "[Embedding] failed to sync rag document: {} error: {}",
+          rag_id, err
+        );
       } else {
         result.push(AFCollabMetadata {
           object_id: rag_id.to_string(),
@@ -179,6 +191,10 @@ impl AIUserService for ChatUserServiceImpl {
     self.upgrade_user()?.workspace_id()
   }
 
+  fn workspace_type(&self) -> FlowyResult<WorkspaceType> {
+    self.upgrade_user()?.workspace_type()
+  }
+
   fn sqlite_connection(&self, uid: i64) -> Result<DBConnection, FlowyError> {
     self.upgrade_user()?.get_sqlite_connection(uid)
   }
diff --git a/frontend/rust-lib/flowy-core/src/indexing_data_runner.rs b/frontend/rust-lib/flowy-core/src/indexing_data_runner.rs
index 7a5c7c68b5937..93edb7ec06ea8 100644
--- a/frontend/rust-lib/flowy-core/src/indexing_data_runner.rs
+++ b/frontend/rust-lib/flowy-core/src/indexing_data_runner.rs
@@ -216,7 +216,10 @@ impl AppLifeCycleImpl {
           },
         }
         full_indexed_finish_sender.send_replace(true);
-        info!("[Indexing] full indexed data provider stopped");
+        info!(
+          "[Indexing] {} full indexed data provider stopped",
+          workspace_id_cloned
+        );
       }
 
       if let Some(writer) = full_indexed_data_writer.upgrade() {
diff --git a/frontend/rust-lib/flowy-core/src/lib.rs b/frontend/rust-lib/flowy-core/src/lib.rs
index 0ac8ff6e1542e..799f78b39267f 100644
--- a/frontend/rust-lib/flowy-core/src/lib.rs
+++ b/frontend/rust-lib/flowy-core/src/lib.rs
@@ -39,6 +39,7 @@ use crate::server_layer::ServerProvider;
 use app_life_cycle::AppLifeCycleImpl;
 use deps_resolve::reminder_deps::CollabInteractImpl;
 use flowy_sqlite::DBConnection;
+use flowy_user_pub::entities::WorkspaceType;
 use lib_infra::async_trait::async_trait;
 
 pub(crate) mod app_life_cycle;
@@ -354,6 +355,10 @@ impl LoggedUser for ServerUserImpl {
     self.upgrade_user()?.workspace_id()
   }
 
+  fn workspace_type(&self) -> FlowyResult<WorkspaceType> {
+    self.upgrade_user()?.workspace_type()
+  }
+
   fn user_id(&self) -> FlowyResult<i64> {
     self.upgrade_user()?.user_id()
   }
diff --git a/frontend/rust-lib/flowy-error/Cargo.toml b/frontend/rust-lib/flowy-error/Cargo.toml
index de57c67e3fd47..423480190bbe2 100644
--- a/frontend/rust-lib/flowy-error/Cargo.toml
+++ b/frontend/rust-lib/flowy-error/Cargo.toml
@@ -35,8 +35,6 @@ collab-folder = { workspace = true, optional = true }
 client-api = { workspace = true, optional = true }
 tantivy = { workspace = true, optional = true }
 uuid.workspace = true
-
-[target.'cfg(any(target_os = "macos", target_os = "linux", target_os = "windows"))'.dependencies]
 ollama-rs.workspace = true
 
 
diff --git a/frontend/rust-lib/flowy-error/src/errors.rs b/frontend/rust-lib/flowy-error/src/errors.rs
index 36240cd08d517..722363b89d3da 100644
--- a/frontend/rust-lib/flowy-error/src/errors.rs
+++ b/frontend/rust-lib/flowy-error/src/errors.rs
@@ -265,7 +265,6 @@ impl From<uuid::Error> for FlowyError {
   }
 }
 
-#[cfg(any(target_os = "windows", target_os = "macos", target_os = "linux"))]
 impl From<ollama_rs::error::OllamaError> for FlowyError {
   fn from(value: ollama_rs::error::OllamaError) -> Self {
     FlowyError::local_ai().with_context(value)
diff --git a/frontend/rust-lib/flowy-server/src/af_cloud/define.rs b/frontend/rust-lib/flowy-server/src/af_cloud/define.rs
index 31114629ac216..7ab9e947a31fb 100644
--- a/frontend/rust-lib/flowy-server/src/af_cloud/define.rs
+++ b/frontend/rust-lib/flowy-server/src/af_cloud/define.rs
@@ -2,6 +2,7 @@ use collab_plugins::CollabKVDB;
 use flowy_ai_pub::user_service::AIUserService;
 use flowy_error::{FlowyError, FlowyResult};
 use flowy_sqlite::DBConnection;
+use flowy_user_pub::entities::WorkspaceType;
 use lib_infra::async_trait::async_trait;
 use std::path::PathBuf;
 use std::sync::{Arc, Weak};
@@ -17,6 +18,7 @@ pub const USER_DEVICE_ID: &str = "device_id";
 pub trait LoggedUser: Send + Sync {
   /// different user might return different workspace id.
   fn workspace_id(&self) -> FlowyResult<Uuid>;
+  fn workspace_type(&self) -> FlowyResult<WorkspaceType>;
 
   fn user_id(&self) -> FlowyResult<i64>;
   async fn is_local_mode(&self) -> FlowyResult<bool>;
@@ -54,6 +56,10 @@ impl AIUserService for AIUserServiceImpl {
     self.logged_user()?.workspace_id()
   }
 
+  fn workspace_type(&self) -> FlowyResult<WorkspaceType> {
+    self.logged_user()?.workspace_type()
+  }
+
   fn sqlite_connection(&self, uid: i64) -> Result<DBConnection, FlowyError> {
     self.logged_user()?.get_sqlite_db(uid)
   }
diff --git a/frontend/rust-lib/flowy-server/src/local_server/impls/chat.rs b/frontend/rust-lib/flowy-server/src/local_server/impls/chat.rs
index dcf4a3836b60f..6137dc8abefb8 100644
--- a/frontend/rust-lib/flowy-server/src/local_server/impls/chat.rs
+++ b/frontend/rust-lib/flowy-server/src/local_server/impls/chat.rs
@@ -1,5 +1,4 @@
 use crate::af_cloud::define::LoggedUser;
-use crate::local_server::uid::IDGenerator;
 use chrono::{TimeZone, Utc};
 use client_api::entity::ai_dto::RepeatedRelatedQuestion;
 use flowy_ai::local_ai::controller::LocalAIController;
@@ -28,7 +27,7 @@ use tracing::trace;
 use uuid::Uuid;
 
 lazy_static! {
-  static ref ID_GEN: Mutex<IDGenerator> = Mutex::new(IDGenerator::new(2));
+  static ref ID_GEN: Mutex<MessageIDGenerator> = Mutex::new(MessageIDGenerator::new());
 }
 
 pub struct LocalChatServiceImpl {
@@ -98,7 +97,8 @@ impl ChatCloudService for LocalChatServiceImpl {
     question_id: i64,
     metadata: Option<serde_json::Value>,
   ) -> Result<ChatMessage, FlowyError> {
-    let mut message = ChatMessage::new_ai(timestamp(), message.to_string(), Some(question_id));
+    let message_id = ID_GEN.lock().await.next_id();
+    let mut message = ChatMessage::new_ai(message_id, message.to_string(), Some(question_id));
     if let Some(metadata) = metadata {
       message.metadata = metadata;
     }
@@ -334,3 +334,31 @@ fn chat_message_from_row(row: ChatMessageTable) -> ChatMessage {
     reply_message_id: row.reply_message_id,
   }
 }
+
+pub struct MessageIDGenerator {
+  last_timestamp: i64,
+}
+
+impl MessageIDGenerator {
+  pub fn new() -> MessageIDGenerator {
+    MessageIDGenerator {
+      last_timestamp: timestamp(),
+    }
+  }
+
+  pub fn next_id(&mut self) -> i64 {
+    let mut current_timestamp = timestamp();
+    if current_timestamp < self.last_timestamp {
+      current_timestamp = self.last_timestamp;
+    }
+
+    if current_timestamp == self.last_timestamp {
+      self.last_timestamp += 1;
+      current_timestamp = self.last_timestamp;
+    } else {
+      self.last_timestamp = current_timestamp;
+    }
+
+    current_timestamp
+  }
+}
diff --git a/frontend/rust-lib/flowy-sqlite-vec/Cargo.toml b/frontend/rust-lib/flowy-sqlite-vec/Cargo.toml
index fe775a6a127d3..1b59d8af02304 100644
--- a/frontend/rust-lib/flowy-sqlite-vec/Cargo.toml
+++ b/frontend/rust-lib/flowy-sqlite-vec/Cargo.toml
@@ -18,6 +18,7 @@ serde_json.workspace = true
 r2d2 = { version = "0.8" }
 r2d2_sqlite = { version = "0.28" }
 chrono.workspace = true
+serde = { version = "1.0.219", features = ["derive"] }
 
 [dev-dependencies]
 tempfile = "3.8.0"
diff --git a/frontend/rust-lib/flowy-sqlite-vec/src/db.rs b/frontend/rust-lib/flowy-sqlite-vec/src/db.rs
index a3b9e5d4b8a99..4805347eb9750 100644
--- a/frontend/rust-lib/flowy-sqlite-vec/src/db.rs
+++ b/frontend/rust-lib/flowy-sqlite-vec/src/db.rs
@@ -1,4 +1,6 @@
-use crate::entities::{PendingIndexedCollab, SqliteEmbeddedDocument, SqliteEmbeddedFragment};
+use crate::entities::{
+  EmbeddedContent, PendingIndexedCollab, SqliteEmbeddedDocument, SqliteEmbeddedFragment,
+};
 use crate::init_sqlite_vector_extension;
 use crate::migration::init_sqlite_with_migrations;
 use anyhow::{Context, Result};
@@ -109,6 +111,55 @@ impl VectorSqliteDB {
     Ok(())
   }
 
+  pub async fn select_all_embedded_content(
+    &self,
+    workspace_id: &str,
+    rag_ids: &[String],
+    limit: usize,
+  ) -> Result<Vec<EmbeddedContent>> {
+    let conn = self
+      .pool
+      .get()
+      .context("Failed to get connection from pool")?;
+
+    // Build SQL query based on whether rag_ids are provided
+    let (sql, params) = if rag_ids.is_empty() {
+      // No rag_ids provided, select all content for workspace
+      let sql =
+        "SELECT object_id, content FROM af_collab_embeddings WHERE workspace_id = ? LIMIT ?";
+      let params: Vec<&dyn ToSql> = vec![&workspace_id, &limit];
+      (sql.to_string(), params)
+    } else {
+      // Filter by provided rag_ids
+      let placeholders = std::iter::repeat("?")
+        .take(rag_ids.len())
+        .collect::<Vec<_>>()
+        .join(", ");
+
+      let sql = format!(
+        "SELECT object_id, content FROM af_collab_embeddings WHERE workspace_id = ? AND object_id IN ({}) LIMIT ?",
+        placeholders
+      );
+
+      let mut params: Vec<&dyn ToSql> = vec![&workspace_id];
+      params.extend(rag_ids.iter().map(|id| id as &dyn ToSql));
+      params.push(&limit);
+      (sql, params)
+    };
+
+    let mut stmt = conn.prepare(&sql)?;
+    let mut rows = stmt.query(params.as_slice())?;
+
+    let mut contents = Vec::new();
+    while let Some(row) = rows.next()? {
+      let object_id: String = row.get(0)?;
+      let content: String = row.get(1)?;
+      contents.push(EmbeddedContent { content, object_id });
+    }
+
+    Ok(contents)
+  }
+
   pub async fn select_all_embedded_documents(
     &self,
     workspace_id: &str,
diff --git a/frontend/rust-lib/flowy-sqlite-vec/src/entities.rs b/frontend/rust-lib/flowy-sqlite-vec/src/entities.rs
index 170ab711a251c..d402b7bc220ab 100644
--- a/frontend/rust-lib/flowy-sqlite-vec/src/entities.rs
+++ b/frontend/rust-lib/flowy-sqlite-vec/src/entities.rs
@@ -1,3 +1,5 @@
+use serde::Serialize;
+
 #[derive(Clone, Debug)]
 pub struct PendingIndexedCollab {
   pub object_id: String,
@@ -18,3 +20,9 @@ pub struct SqliteEmbeddedFragment {
   pub content: String,
   pub embeddings: Vec<f32>,
 }
+
+#[derive(Clone, Debug, Serialize)]
+pub struct EmbeddedContent {
+  pub content: String,
+  pub object_id: String,
+}
diff --git a/frontend/rust-lib/flowy-user/src/services/authenticate_user.rs b/frontend/rust-lib/flowy-user/src/services/authenticate_user.rs
index 690d9fb344ef4..67ef4848b88a9 100644
--- a/frontend/rust-lib/flowy-user/src/services/authenticate_user.rs
+++ b/frontend/rust-lib/flowy-user/src/services/authenticate_user.rs
@@ -71,6 +71,13 @@ impl AuthenticateUser {
     Ok(workspace_uuid)
   }
 
+  pub fn workspace_type(&self) -> FlowyResult<WorkspaceType> {
+    let session = self.get_session()?;
+    let mut conn = self.get_sqlite_connection(session.user_id)?;
+    let workspace_type = select_user_workspace_type(&session.workspace_id, &mut conn)?;
+    Ok(workspace_type)
+  }
+
   pub fn workspace_database_object_id(&self) -> FlowyResult<Uuid> {
     let session = self.get_session()?;
     let mut conn = self.get_sqlite_connection(session.user_id)?;
diff --git a/frontend/rust-lib/lib-infra/src/util.rs b/frontend/rust-lib/lib-infra/src/util.rs
index 18ff068a73fac..819172f71593c 100644
--- a/frontend/rust-lib/lib-infra/src/util.rs
+++ b/frontend/rust-lib/lib-infra/src/util.rs
@@ -60,7 +60,6 @@ where
   }
 }
 
-#[allow(dead_code)]
 pub fn timestamp() -> i64 {
   chrono::Utc::now().timestamp()
 }
